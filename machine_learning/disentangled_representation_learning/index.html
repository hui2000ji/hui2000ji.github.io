
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="Personal website of Huiyu CAI.">
      
      
      
        <link rel="canonical" href="https://hui2000ji.github.io/machine_learning/disentangled_representation_learning/">
      
      
        <link rel="prev" href="../representation_learning_with_mutual_information_maximization/">
      
      
        <link rel="next" href="../equivariant_gnns/">
      
      
      <link rel="icon" href="../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.5.36">
    
    
      
        <title>Disentangled Representation Learning - Huiyu CAI</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.06209087.min.css">
      
        
        <link rel="stylesheet" href="../../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Nunito:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Nunito";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.6.0/styles/atom-one-light.min.css">
    
      <link rel="stylesheet" href="../../assets/css/style.css">
    
    <script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="indigo">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#disentangled-representation-learning" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../.." title="Huiyu CAI" class="md-header__button md-logo" aria-label="Huiyu CAI" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Huiyu CAI
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Disentangled Representation Learning
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="indigo"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 2a7 7 0 0 1 7 7c0 2.38-1.19 4.47-3 5.74V17a1 1 0 0 1-1 1H9a1 1 0 0 1-1-1v-2.26C6.19 13.47 5 11.38 5 9a7 7 0 0 1 7-7M9 21v-1h6v1a1 1 0 0 1-1 1h-4a1 1 0 0 1-1-1m3-17a5 5 0 0 0-5 5c0 2.05 1.23 3.81 3 4.58V16h4v-2.42c1.77-.77 3-2.53 3-4.58a5 5 0 0 0-5-5"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="slate" data-md-color-primary="deep-orange" data-md-color-accent="deep-orange"  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 2a7 7 0 0 0-7 7c0 2.38 1.19 4.47 3 5.74V17a1 1 0 0 0 1 1h6a1 1 0 0 0 1-1v-2.26c1.81-1.27 3-3.36 3-5.74a7 7 0 0 0-7-7M9 21a1 1 0 0 0 1 1h4a1 1 0 0 0 1-1v-1H9z"/></svg>
      </label>
    
  
</form>
      
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
      <div class="md-header__source">
        <a href="https://github.com/hui2000ji/hui2000ji.github.io" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81"/></svg>
  </div>
  <div class="md-source__repository">
    hui2000ji/hui2000ji.github.io
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
            
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
    <li class="md-tabs__item">
      <a href="../.." class="md-tabs__link">
        
  
    
  
  Home

      </a>
    </li>
  

      
        
  
  
  
    <li class="md-tabs__item">
      <a href="../../assets/documents/CV%20-%20Huiyu%20Cai.pdf" class="md-tabs__link">
        
  
    
  
  CV

      </a>
    </li>
  

      
        
  
  
    
  
  
    
    
      <li class="md-tabs__item md-tabs__item--active">
        <a href="../" class="md-tabs__link">
          
  
  Machine Learning

        </a>
      </li>
    
  

      
    </ul>
  </div>
</nav>
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="Huiyu CAI" class="md-nav__button md-logo" aria-label="Huiyu CAI" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    Huiyu CAI
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/hui2000ji/hui2000ji.github.io" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81"/></svg>
  </div>
  <div class="md-source__repository">
    hui2000ji/hui2000ji.github.io
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Home
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../assets/documents/CV%20-%20Huiyu%20Cai.pdf" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    CV
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
    
      
        
        
      
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" checked>
        
          
          <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="">
            
  
  <span class="md-ellipsis">
    Machine Learning
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            Machine Learning
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Index
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../graph_neural_networks/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Graph Neural Networks
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../representation_learning_with_mutual_information_maximization/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Representation Learning with Mutual Information Maximization
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  <span class="md-ellipsis">
    Disentangled Representation Learning
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  <span class="md-ellipsis">
    Disentangled Representation Learning
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#what-makes-a-good-representation" class="md-nav__link">
    <span class="md-ellipsis">
      What makes a good representation?
    </span>
  </a>
  
    <nav class="md-nav" aria-label="What makes a good representation?">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#merits-of-disentangled-representation" class="md-nav__link">
    <span class="md-ellipsis">
      Merits of Disentangled Representation
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#metrics-of-disentanglement" class="md-nav__link">
    <span class="md-ellipsis">
      Metrics of disentanglement
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Metrics of disentanglement">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#beta-vae-metric" class="md-nav__link">
    <span class="md-ellipsis">
      \(\beta\)-VAE Metric
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#factorvae-metric" class="md-nav__link">
    <span class="md-ellipsis">
      FactorVAE Metric
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mutual-information-gap-mig" class="md-nav__link">
    <span class="md-ellipsis">
      Mutual Information Gap (MIG)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#modularity-explicitness" class="md-nav__link">
    <span class="md-ellipsis">
      Modularity &amp; Explicitness
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#dci-disentanglement-score" class="md-nav__link">
    <span class="md-ellipsis">
      DCI Disentanglement Score
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sap-score" class="md-nav__link">
    <span class="md-ellipsis">
      SAP Score
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#metric-agreement" class="md-nav__link">
    <span class="md-ellipsis">
      Metric Agreement
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#unsupervised-disentanglement" class="md-nav__link">
    <span class="md-ellipsis">
      Unsupervised Disentanglement
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Unsupervised Disentanglement">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#constraining-the-vae-bottleneck-capacity" class="md-nav__link">
    <span class="md-ellipsis">
      Constraining the VAE Bottleneck Capacity
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Constraining the VAE Bottleneck Capacity">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#beta-vae" class="md-nav__link">
    <span class="md-ellipsis">
      \(\beta\)-VAE
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#annealedvae" class="md-nav__link">
    <span class="md-ellipsis">
      AnnealedVAE
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#penalizing-the-total-correlation" class="md-nav__link">
    <span class="md-ellipsis">
      Penalizing the total correlation
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Penalizing the total correlation">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#factorvae" class="md-nav__link">
    <span class="md-ellipsis">
      FactorVAE
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#beta-tcvae" class="md-nav__link">
    <span class="md-ellipsis">
      \(\beta\)-TCVAE
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#disentangling-the-inferred-prior-aggregated-posterior" class="md-nav__link">
    <span class="md-ellipsis">
      Disentangling the Inferred Prior (Aggregated Posterior)
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Disentangling the Inferred Prior (Aggregated Posterior)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#dip-vae" class="md-nav__link">
    <span class="md-ellipsis">
      DIP-VAE
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#aae" class="md-nav__link">
    <span class="md-ellipsis">
      AAE
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#infovae" class="md-nav__link">
    <span class="md-ellipsis">
      InfoVAE
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#maximizing-mi-bt-latent-code-and-gan-samples" class="md-nav__link">
    <span class="md-ellipsis">
      Maximizing MI b/t Latent Code and GAN Samples
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Maximizing MI b/t Latent Code and GAN Samples">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#infogan" class="md-nav__link">
    <span class="md-ellipsis">
      InfoGAN
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#infogan-cr" class="md-nav__link">
    <span class="md-ellipsis">
      InfoGAN-CR
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#elastic-infogan" class="md-nav__link">
    <span class="md-ellipsis">
      Elastic-InfoGAN
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#leveraging-the-hierarchy-of-latent-variables" class="md-nav__link">
    <span class="md-ellipsis">
      Leveraging the Hierarchy of Latent Variables
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Leveraging the Hierarchy of Latent Variables">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#stylegan" class="md-nav__link">
    <span class="md-ellipsis">
      StyleGAN
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#very-deep-vae" class="md-nav__link">
    <span class="md-ellipsis">
      Very Deep VAE
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#the-impossibility-of-unsupervised-disentanglement" class="md-nav__link">
    <span class="md-ellipsis">
      The Impossibility of Unsupervised Disentanglement
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#learning-from-partial-class-labels" class="md-nav__link">
    <span class="md-ellipsis">
      Learning from Partial Class Labels
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Learning from Partial Class Labels">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#latent-feature-discriminative-model-m1" class="md-nav__link">
    <span class="md-ellipsis">
      Latent Feature Discriminative Model (M1)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#semi-supervised-vae" class="md-nav__link">
    <span class="md-ellipsis">
      Semi-Supervised VAE
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Semi-Supervised VAE">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#m2" class="md-nav__link">
    <span class="md-ellipsis">
      M2
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#generalized-m2" class="md-nav__link">
    <span class="md-ellipsis">
      Generalized M2
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#adversarial-training" class="md-nav__link">
    <span class="md-ellipsis">
      Adversarial Training
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Adversarial Training">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#vae-gan" class="md-nav__link">
    <span class="md-ellipsis">
      VAE-GAN
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#drnet" class="md-nav__link">
    <span class="md-ellipsis">
      DrNet
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#latent-optimization" class="md-nav__link">
    <span class="md-ellipsis">
      Latent Optimization
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Latent Optimization">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lord" class="md-nav__link">
    <span class="md-ellipsis">
      LORD
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#learning-from-grouped-observations" class="md-nav__link">
    <span class="md-ellipsis">
      Learning from Grouped Observations
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Learning from Grouped Observations">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#ml-vae" class="md-nav__link">
    <span class="md-ellipsis">
      ML-VAE
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gvae" class="md-nav__link">
    <span class="md-ellipsis">
      GVAE
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#f-statistics" class="md-nav__link">
    <span class="md-ellipsis">
      F-Statistics
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#learning-from-paired-observations" class="md-nav__link">
    <span class="md-ellipsis">
      Learning from Paired Observations
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Learning from Paired Observations">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#ada-mlvae-and-ada-gvae" class="md-nav__link">
    <span class="md-ellipsis">
      Ada-MLVAE and Ada-GVAE
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../equivariant_gnns/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Equivariant GNNs
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#what-makes-a-good-representation" class="md-nav__link">
    <span class="md-ellipsis">
      What makes a good representation?
    </span>
  </a>
  
    <nav class="md-nav" aria-label="What makes a good representation?">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#merits-of-disentangled-representation" class="md-nav__link">
    <span class="md-ellipsis">
      Merits of Disentangled Representation
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#metrics-of-disentanglement" class="md-nav__link">
    <span class="md-ellipsis">
      Metrics of disentanglement
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Metrics of disentanglement">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#beta-vae-metric" class="md-nav__link">
    <span class="md-ellipsis">
      \(\beta\)-VAE Metric
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#factorvae-metric" class="md-nav__link">
    <span class="md-ellipsis">
      FactorVAE Metric
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mutual-information-gap-mig" class="md-nav__link">
    <span class="md-ellipsis">
      Mutual Information Gap (MIG)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#modularity-explicitness" class="md-nav__link">
    <span class="md-ellipsis">
      Modularity &amp; Explicitness
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#dci-disentanglement-score" class="md-nav__link">
    <span class="md-ellipsis">
      DCI Disentanglement Score
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sap-score" class="md-nav__link">
    <span class="md-ellipsis">
      SAP Score
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#metric-agreement" class="md-nav__link">
    <span class="md-ellipsis">
      Metric Agreement
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#unsupervised-disentanglement" class="md-nav__link">
    <span class="md-ellipsis">
      Unsupervised Disentanglement
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Unsupervised Disentanglement">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#constraining-the-vae-bottleneck-capacity" class="md-nav__link">
    <span class="md-ellipsis">
      Constraining the VAE Bottleneck Capacity
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Constraining the VAE Bottleneck Capacity">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#beta-vae" class="md-nav__link">
    <span class="md-ellipsis">
      \(\beta\)-VAE
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#annealedvae" class="md-nav__link">
    <span class="md-ellipsis">
      AnnealedVAE
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#penalizing-the-total-correlation" class="md-nav__link">
    <span class="md-ellipsis">
      Penalizing the total correlation
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Penalizing the total correlation">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#factorvae" class="md-nav__link">
    <span class="md-ellipsis">
      FactorVAE
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#beta-tcvae" class="md-nav__link">
    <span class="md-ellipsis">
      \(\beta\)-TCVAE
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#disentangling-the-inferred-prior-aggregated-posterior" class="md-nav__link">
    <span class="md-ellipsis">
      Disentangling the Inferred Prior (Aggregated Posterior)
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Disentangling the Inferred Prior (Aggregated Posterior)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#dip-vae" class="md-nav__link">
    <span class="md-ellipsis">
      DIP-VAE
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#aae" class="md-nav__link">
    <span class="md-ellipsis">
      AAE
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#infovae" class="md-nav__link">
    <span class="md-ellipsis">
      InfoVAE
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#maximizing-mi-bt-latent-code-and-gan-samples" class="md-nav__link">
    <span class="md-ellipsis">
      Maximizing MI b/t Latent Code and GAN Samples
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Maximizing MI b/t Latent Code and GAN Samples">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#infogan" class="md-nav__link">
    <span class="md-ellipsis">
      InfoGAN
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#infogan-cr" class="md-nav__link">
    <span class="md-ellipsis">
      InfoGAN-CR
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#elastic-infogan" class="md-nav__link">
    <span class="md-ellipsis">
      Elastic-InfoGAN
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#leveraging-the-hierarchy-of-latent-variables" class="md-nav__link">
    <span class="md-ellipsis">
      Leveraging the Hierarchy of Latent Variables
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Leveraging the Hierarchy of Latent Variables">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#stylegan" class="md-nav__link">
    <span class="md-ellipsis">
      StyleGAN
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#very-deep-vae" class="md-nav__link">
    <span class="md-ellipsis">
      Very Deep VAE
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#the-impossibility-of-unsupervised-disentanglement" class="md-nav__link">
    <span class="md-ellipsis">
      The Impossibility of Unsupervised Disentanglement
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#learning-from-partial-class-labels" class="md-nav__link">
    <span class="md-ellipsis">
      Learning from Partial Class Labels
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Learning from Partial Class Labels">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#latent-feature-discriminative-model-m1" class="md-nav__link">
    <span class="md-ellipsis">
      Latent Feature Discriminative Model (M1)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#semi-supervised-vae" class="md-nav__link">
    <span class="md-ellipsis">
      Semi-Supervised VAE
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Semi-Supervised VAE">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#m2" class="md-nav__link">
    <span class="md-ellipsis">
      M2
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#generalized-m2" class="md-nav__link">
    <span class="md-ellipsis">
      Generalized M2
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#adversarial-training" class="md-nav__link">
    <span class="md-ellipsis">
      Adversarial Training
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Adversarial Training">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#vae-gan" class="md-nav__link">
    <span class="md-ellipsis">
      VAE-GAN
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#drnet" class="md-nav__link">
    <span class="md-ellipsis">
      DrNet
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#latent-optimization" class="md-nav__link">
    <span class="md-ellipsis">
      Latent Optimization
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Latent Optimization">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lord" class="md-nav__link">
    <span class="md-ellipsis">
      LORD
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#learning-from-grouped-observations" class="md-nav__link">
    <span class="md-ellipsis">
      Learning from Grouped Observations
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Learning from Grouped Observations">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#ml-vae" class="md-nav__link">
    <span class="md-ellipsis">
      ML-VAE
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gvae" class="md-nav__link">
    <span class="md-ellipsis">
      GVAE
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#f-statistics" class="md-nav__link">
    <span class="md-ellipsis">
      F-Statistics
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#learning-from-paired-observations" class="md-nav__link">
    <span class="md-ellipsis">
      Learning from Paired Observations
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Learning from Paired Observations">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#ada-mlvae-and-ada-gvae" class="md-nav__link">
    <span class="md-ellipsis">
      Ada-MLVAE and Ada-GVAE
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
  
                  

  
  


<h1 id="disentangled-representation-learning">Disentangled Representation Learning<a class="headerlink" href="#disentangled-representation-learning" title="Permanent link">&para;</a></h1>
<p>General notations: <span class="arithmatex">\(x\)</span> denotes data points (generated by <span class="arithmatex">\(K\)</span> factors), <span class="arithmatex">\(y\)</span> denotes labels, <span class="arithmatex">\(z\)</span> denotes <span class="arithmatex">\(D\)</span>-dimensional representation.</p>
<h2 id="what-makes-a-good-representation">What makes a good representation?<a class="headerlink" href="#what-makes-a-good-representation" title="Permanent link">&para;</a></h2>
<blockquote>
<p>Adapted from S. Soatto's IPAM talk, 2018<sup id="fnref:1"><a class="footnote-ref" href="#fn:1">1</a></sup>.</p>
</blockquote>
<ul>
<li>Sufficiency: <span class="arithmatex">\(I(y;z) = I(y;x)\)</span></li>
<li>Invariance: <span class="arithmatex">\(n \perp y \rightarrow I(n;z) = 0\)</span>, where <span class="arithmatex">\(n\)</span> denotes nuisance factors.</li>
<li>Minimality: <span class="arithmatex">\(\min I(x; z)\)</span></li>
<li>Disentangling: <span class="arithmatex">\(\mathrm{TC}(z): \mathcal{D}_{\mathrm{KL}}(p(z) \Vert \prod_i p(z_i))\)</span></li>
</ul>
<p><strong>Invariance <span class="arithmatex">\(\Leftrightarrow\)</span> Minimality <span class="arithmatex">\(|\)</span> Sufficiency</strong> &emsp; If <span class="arithmatex">\(z\)</span> is sufficient, <span class="arithmatex">\(n\)</span> is a nuisance, then</p>
<div class="arithmatex">\[
\DeclareMathOperator{\Dcal}{\mathcal{D}}
\DeclareMathOperator{\Dsup}{\Dcal^{\mathrm{sup}}}
\DeclareMathOperator{\Lcal}{\mathcal{L}}
\DeclareMathOperator{\Bcal}{\mathcal{B}}
\DeclareMathOperator{\Ncal}{\mathcal{N}}
\DeclareMathOperator{\Gcal}{\mathcal{G}}
\DeclareMathOperator{\Xcal}{\mathcal{X}}
\DeclareMathOperator{\Zcal}{\mathcal{Z}}
\DeclareMathOperator{\ELBO}{\mathrm{ELBO}}
\DeclareMathOperator{\E}{\mathbb{E}}
\DeclareMathOperator{\R}{\mathbb{R}}
\DeclareMathOperator{\Cov}{\mathrm{Cov}}
\DeclareMathOperator{\Var}{\mathrm{Var}}
\DeclareMathOperator{\dom}{\mathrm{dom}}
\DeclareMathOperator{\sign}{\mathrm{sign}}
\DeclareMathOperator{\diag}{\mathrm{diag}}
\DeclareMathOperator{\argmax}{\mathrm{argmax}}
\DeclareMathOperator{\KL}{\Dcal_{\mathrm{KL}}}
\DeclareMathOperator{\MMD}{\Dcal_{\mathrm{MMD}}}
\DeclareMathOperator{\JS}{\Dcal_{\mathrm{JS}}}
\DeclareMathOperator{\stopgradient}{\mathrm{StopGradient}}
\def\dd{\mathrm{d}}
\def\ee{\mathrm{e}}
\def\xbf{\mathbf{x}}
\def\Xbf{\mathbf{X}}
\def\ybf{\mathbf{y}}
\def\sbf{\mathbf{s}}
\def\wbf{\mathbf{w}}
\def\vbf{\mathbf{v}}
\def\zbf{\mathbf{z}}
\def\cbf{\mathbf{c}}
\def\dd{\mathrm{d}}
I(z;n) \le I(z;x) - I(x;y)
\]</div>
<p>and there exists a nuisance for which equality holds.</p>
<h3 id="merits-of-disentangled-representation">Merits of Disentangled Representation<a class="headerlink" href="#merits-of-disentangled-representation" title="Permanent link">&para;</a></h3>
<p>While there is no widely-accepted formalized notion of disentanglement (yet), the key intuition is that a disentangled representation should separate the distinct, informative factors of variations in the data<sup id="fnref:3"><a class="footnote-ref" href="#fn:3">3</a></sup>.</p>
<p>Disentangled representations offer several advantages<sup id="fnref:9"><a class="footnote-ref" href="#fn:9">9</a></sup>:</p>
<ul>
<li><strong>Invariance</strong>: it is easier to derive representations that are invariant to nuisance factors by simply marginalizing over the corresponding dimensions</li>
<li><strong>Transferability</strong>: they are arguably more suitable for transfer learning as most of the key underlying generative factors appear segregated along feature dimensions</li>
<li><strong>Interpretability</strong>: a human expert may be able to assign meanings to the dimensions</li>
<li><strong>Conditioning and intervention</strong>: they allow for interpretable conditioning and/or intervention over a subset of the latents and observe the effects on other nodes in the graph.</li>
</ul>
<p>Current deep learning common practices are generally not disentangled, for instance, state-of-the-art deep learning models trained on speech data failed to capture basic conceps such as distributions on phonemes<sup id="fnref:8"><a class="footnote-ref" href="#fn:8">8</a></sup> (which could be discovered by a simple unsupervised clustering algorithm such as <span class="arithmatex">\(k\)</span>-means), since phoneme information account for only a few bits per second and is easily overwhelmed by other variables.</p>
<h2 id="metrics-of-disentanglement">Metrics of disentanglement<a class="headerlink" href="#metrics-of-disentanglement" title="Permanent link">&para;</a></h2>
<blockquote>
<p>Adapted from the benchmark paper by F. Locatello et al.<sup id="fnref3:2"><a class="footnote-ref" href="#fn:2">2</a></sup>, 2019.</p>
</blockquote>
<p>This section discusses the metrics of disentanglement. Readers interested only in learning disentangled representation can skip this section.</p>
<h3 id="beta-vae-metric"><span class="arithmatex">\(\beta\)</span>-VAE Metric<a class="headerlink" href="#beta-vae-metric" title="Permanent link">&para;</a></h3>
<p>The <span class="arithmatex">\(\beta\)</span>-VAE metric<sup id="fnref:4"><a class="footnote-ref" href="#fn:4">4</a></sup> measures disentanglement as the accuracy of a linear classifier that predicts the index of a fixed factor of variation.</p>
<p><img alt="beta-VAE-metric" class="image-center" src="../../assets/images/DRL-beta-VAE.png" style="width: 60%" /></p>
<p>Over a batch of <span class="arithmatex">\(L\)</span> samples, each pair of data points has a fixed value for one target generative factor <span class="arithmatex">\(y\)</span> (here <span class="arithmatex">\(y\)</span> is scale) and differs on all others. A linear classifier is then trained to identify the target factor using the average pairwise difference <span class="arithmatex">\(\zbf^b_{\mathrm{diff}}\)</span> in the latent space over <span class="arithmatex">\(L\)</span> samples.</p>
<p>This metric has several drawbacks as pointed out by follow-up works<sup id="fnref:5"><a class="footnote-ref" href="#fn:5">5</a></sup><sup>,</sup><sup id="fnref:6"><a class="footnote-ref" href="#fn:6">6</a></sup>:</p>
<ul>
<li>It is sensitive to hyperparameters of the linear classifier optimization</li>
<li>Classifying data-generating factors is not intuitive because factors can be represented as a linear combination of multiple independent latent dimensions<sup id="fnref5:5"><a class="footnote-ref" href="#fn:5">5</a></sup>. Contrary to this point, <sup id="fnref3:6"><a class="footnote-ref" href="#fn:6">6</a></sup> believes a truly disentangled model should only contain one latent variable related to each factor, and argues that this metric lacks axis-alignment detection.</li>
<li>It has a failure mode: when <span class="arithmatex">\(K-1\)</span> out of <span class="arithmatex">\(K\)</span> factors are disentangled, it outputs 100%.</li>
</ul>
<h3 id="factorvae-metric">FactorVAE Metric<a class="headerlink" href="#factorvae-metric" title="Permanent link">&para;</a></h3>
<p>The FactorVAE metric<sup id="fnref2:5"><a class="footnote-ref" href="#fn:5">5</a></sup> uses a majority vote classifier on a different feature vector, namely, the argmin of the per-dimension empirical variance.</p>
<p><img alt="FactorVAE-Metric" src="../../assets/images/DRL-FactorVAE.png" /></p>
<h3 id="mutual-information-gap-mig">Mutual Information Gap (MIG)<a class="headerlink" href="#mutual-information-gap-mig" title="Permanent link">&para;</a></h3>
<p>Mutual Information Gap (MIG)<sup id="fnref2:6"><a class="footnote-ref" href="#fn:6">6</a></sup> measures for each factor of variation the normalized gap in mutual information between the highest and second highest coordinate in <span class="arithmatex">\(z\)</span>.</p>
<div class="arithmatex">\[
\mathrm{MIG} = \frac{1}{K} \sum_{k=1}^K \frac{1}{H(v_k)} \Big(
    I_n(z_{j^{(k)}}; v_k) - \max_{j \ne j^{(k)}} I_n(z_{j}; v_k)
\Big)
\]</div>
<p>where <span class="arithmatex">\(j^{(k)} = \argmax_j I_n(z_j;v_k)\)</span>. MIG is bounded by 0 and 1.</p>
<p><strong>Estimation of <span class="arithmatex">\(I_n(z_j;v_k)\)</span></strong> &emsp; Let <span class="arithmatex">\(n \in \{1, 2, \dots, N\}\)</span> be the data point index and <span class="arithmatex">\(\Xcal_{v_k}\)</span> be the support of <span class="arithmatex">\(p(n|v_k)\)</span>. We make the following assumptions:</p>
<ul>
<li>Model: <span class="arithmatex">\(q(z, v) = \sum_x p(v)p(x|v)q(z|x)\)</span>.</li>
<li>The inference distribution <span class="arithmatex">\(q(z_j|x)\)</span> can be sampled from and is known for all <span class="arithmatex">\(j\)</span>.</li>
<li>The generating process <span class="arithmatex">\(p(n|v_k)\)</span> can be sampled from and is known.</li>
<li>Simplifying assumption: <span class="arithmatex">\(p(v_k)\)</span> and <span class="arithmatex">\(p(n|v_k)\)</span> are quantized.</li>
</ul>
<p>Then the mutual information can be estimated as the following:</p>
<div class="arithmatex">\[
\begin{align}
 &amp; I_n(z_j;v_k) \\
=&amp; \E_{q(z_j, v_k)} \left( \log \sum_{n=1}^N q(z_j, v_k, n) - \log q(z_j) - \log p(v_k) \right) \\
=&amp; \E_{p(v_k)p(n^\prime|v_k)q(z_j|n^\prime)} \left( \log \sum_{n=1}^N p(v_k)p(n|v_k)q(z_j|n) - \log q(z_j) - \log p(v_k) \right) \\
=&amp; \E_{p(v_k)p(n^\prime|v_k)q(z_j|n^\prime)} \left( \log \sum_{n=1}^N \frac{p(v_k)}{p(v_k)} p(n|v_k)q(z_j|n) \right) + H(z_j) \\
=&amp; \E_{p(v_k)p(n^\prime|v_k)q(z_j|n^\prime)} \left( \log \sum_{n \in \Xcal_{v_k}} p(n|v_k)q(z_j|n) \right) + H(z_j) \\
\end{align}
\]</div>
<h3 id="modularity-explicitness">Modularity &amp; Explicitness<a class="headerlink" href="#modularity-explicitness" title="Permanent link">&para;</a></h3>
<p><strong>Background: An Explicit Definition of Disentanglement</strong></p>
<p>In the paper<sup id="fnref:7"><a class="footnote-ref" href="#fn:7">7</a></sup>, the authors first give an explicit definition of Disentanglement:</p>
<ul>
<li><em>Modular</em>: each dimension of the representation conveys information about at most one factor</li>
<li><em>Compact</em>: a given factor is associated with only one or a few dimensions in the representation</li>
<li><em>Explicit</em>: there is a simple (e.g., linear) mapping from the representation to the value of a factor</li>
</ul>
<p>They use this definition to analyse the <a href="#beta-vae-metric"><span class="arithmatex">\(\beta\)</span>-VAE Metric</a> and the <a href="#factorvae-metric">FactorVAE Metric</a>, and found that the former consider only modularity, while the latter consider modularity and compactness. They argue that compactness is not a desiderata for disentangled representation learning since</p>
<ul>
<li>Forcing compactness can affect the representationâ€™s utility, e.g. <span class="arithmatex">\((\sin \theta, \cos \theta)\)</span> vs <span class="arithmatex">\(\theta\)</span>.</li>
<li>Compactness requirement highly constrain the solution space, while allowing redundancy enables many equivalent solutions.</li>
</ul>
<p>Based on these claims, the authors quantify modularity and explicitness in their metric.</p>
<p><strong>The Modularity Metric</strong></p>
<p>Note that for ideally modular representation, each latent dimension would have high mutual information with a single factor. Let <span class="arithmatex">\(m_{jk} = I(z_j;v_k)\)</span> and <span class="arithmatex">\(\theta_j = \max_k m_{jk}\)</span>. Define a template vector <span class="arithmatex">\(\mathbf{t}_j\)</span> such that </p>
<div class="arithmatex">\[
t_{jk} = \begin{cases}
\theta_j &amp; \text{if }k=\argmax_{k^\prime} m_{jk^\prime}\\
0        &amp; \text{otherwise}
\end{cases}
\]</div>
<p>The observed deviation from the template is given by</p>
<div class="arithmatex">\[
\delta_j = \frac{\sum_k (m_{jk} - t_{jk})^2}{\theta_j^2(K-1)}
\]</div>
<p>A deviation of 0 indicates perfect modularity and 1 indicates that the <span class="arithmatex">\(j\)</span>-th dimension has equal mutual information with every factor.</p>
<p>Thus, they use <span class="arithmatex">\(1 - \delta_j\)</span> as a modularity score for latent dimension <span class="arithmatex">\(j\)</span> and the mean over <span class="arithmatex">\(j\)</span> as the modularity score for the representation.</p>
<p><strong>The Explicitness Metric</strong></p>
<p>For explicitness, the author fit a one-versus-rest logistic-regression classifier that takes the entire representation as input, and record the area under the ROC curve (AUROC) of that classifier. The explicitness score is defined as the mean of AUROC over all <span class="arithmatex">\(K\)</span> factors and all possible values of the factors.</p>
<h3 id="dci-disentanglement-score">DCI Disentanglement Score<a class="headerlink" href="#dci-disentanglement-score" title="Permanent link">&para;</a></h3>
<p>The DCI Disentanglement Metric<sup id="fnref:10"><a class="footnote-ref" href="#fn:10">10</a></sup> computes the entropy of the normalized importance of each latent dimension <span class="arithmatex">\(j\)</span> for predicting the value of a factor of variation <span class="arithmatex">\(v_k\)</span>. The metric is calculated as follows:</p>
<ul>
<li>Train <span class="arithmatex">\(K\)</span> regressors to predict data-generating factors <span class="arithmatex">\(v_k\)</span> given latent representation <span class="arithmatex">\(z\)</span>.</li>
<li>Construct a matrix <span class="arithmatex">\(R\)</span> of relative importance  where <span class="arithmatex">\(R_{jk}\)</span> denotes the relative importance (e.g. absolute value of the LASSO weight) of <span class="arithmatex">\(z_j\)</span> in predicting <span class="arithmatex">\(v_k\)</span>.</li>
<li>Devide <span class="arithmatex">\(R\)</span> by its row-wise sum to obtain normalized importance <span class="arithmatex">\(P\)</span>, where <span class="arithmatex">\(P_{jk} = \frac{R_{jk}}{\sum_{k^\prime} R_{jk}}\)</span>. Note that the "disentanglement" here is equivalent to the "modularity" in the <a href="#modularity--explicitness">Modularity &amp; Explicitness Metric</a>.</li>
<li>Calculate the entropy of the normalized importance <span class="arithmatex">\(H(P_j) = -\sum_{k=1}^K P_{jk}\log P_{jk}\)</span> for each latent dimension <span class="arithmatex">\(j\)</span>.</li>
<li>Obtain the DCI Disentanglement Metric by weighted average of per-dimension disentanglement <span class="arithmatex">\(D = \sum_j \rho_j (1-H(P_j))\)</span>, where <span class="arithmatex">\(\rho_j = \frac{\sum_k R_{jk}}{\sum_{jk}R_{jk}}\)</span>.</li>
</ul>
<h3 id="sap-score">SAP Score<a class="headerlink" href="#sap-score" title="Permanent link">&para;</a></h3>
<p>The SAP Score<sup id="fnref2:9"><a class="footnote-ref" href="#fn:9">9</a></sup> is the average difference of the prediction error of the two most predictive latent dimensions for each factor. The metric is calculated as follows:</p>
<ul>
<li>Train <span class="arithmatex">\(K\)</span> regressors/classifiers to predict data-generating factors <span class="arithmatex">\(v_k\)</span> given the <span class="arithmatex">\(j\)</span>-th latent dimension <span class="arithmatex">\(z_j\)</span>.</li>
<li>Construct a score matrix <span class="arithmatex">\(R\)</span> where <span class="arithmatex">\(S_{jk}\)</span> denotes the relative importance (specifically, <span class="arithmatex">\(R^2\)</span> score for linear regression and balanced classification accuracy for thresholded classification) of <span class="arithmatex">\(z_j\)</span> in predicting <span class="arithmatex">\(v_k\)</span>. For inactive (low varince) dimensions we take <span class="arithmatex">\(S_{jk}\)</span> to be zero for all <span class="arithmatex">\(k\)</span>s.</li>
<li>For each factor <span class="arithmatex">\(k\)</span>, take the difference of top two entries of vector <span class="arithmatex">\(S_{\cdot k}\)</span>.</li>
<li>Obtain the SAP score as the mean of these differences over all factors.</li>
</ul>
<p>Note that SAP focuses on compactness, and thus a high SAP score does not rule out not-modular representations (one latent dimension capturing two or more generative factors well). Further, a low SAP score does not rule out good modularity in cases when two (or more) latent dimensions might be correlated strongly with the same generative factor and poorly with other generative factors.</p>
<h3 id="metric-agreement">Metric Agreement<a class="headerlink" href="#metric-agreement" title="Permanent link">&para;</a></h3>
<p><img alt="metric agreement" class="image-center" src="../../assets/images/DRL-metric-agreement.png" style="width: 70%" />
Taken from the benchmark paper<sup id="fnref:2"><a class="footnote-ref" href="#fn:2">2</a></sup>.</p>
<p>The authors observe that all metrics except <a href="#modularity--explicitness">Modularity</a> seem to be correlated strongly on the datasets dSprites, Color-dSprites and Scream-dSprites and mildly on the other data sets. There appear to be two pairs among these metrics that capture particularly similar notions: the <a href="#beta-vae-metric"><span class="arithmatex">\(\beta\)</span>-VAE</a> and the <a href="#factorvae-metric">FactorVAE</a> metric as well as the <a href="#mutual-information-gap-mig">MIG</a> and <a href="#dci-disentanglement-score">DCI Disentanglement</a>.</p>
<h2 id="unsupervised-disentanglement">Unsupervised Disentanglement<a class="headerlink" href="#unsupervised-disentanglement" title="Permanent link">&para;</a></h2>
<h3 id="constraining-the-vae-bottleneck-capacity">Constraining the VAE Bottleneck Capacity<a class="headerlink" href="#constraining-the-vae-bottleneck-capacity" title="Permanent link">&para;</a></h3>
<h4 id="beta-vae"><span class="arithmatex">\(\beta\)</span>-VAE<a class="headerlink" href="#beta-vae" title="Permanent link">&para;</a></h4>
<p>The <span class="arithmatex">\(\beta\)</span>-VAE<sup id="fnref2:4"><a class="footnote-ref" href="#fn:4">4</a></sup> introduces a hyperparameter <span class="arithmatex">\(\beta\)</span> in front of the KL regularizer of the VAE ELBO to constrain the capacity of the VAE bottleneck. By setting <span class="arithmatex">\(\beta &gt; 1\)</span>, the encoder distribution will be forced to better match the factorized unit Gaussian prior. This often results in decreased reconstruction capabilities.</p>
<div class="arithmatex">\[
\E_{p(\xbf)} \left[
    \E_{q_\phi(\zbf|\xbf)}\log p_\theta(\xbf|\zbf) - \beta \KL(q_\phi(\zbf|\xbf) \Vert p(\zbf))
\right]
\]</div>
<h4 id="annealedvae">AnnealedVAE<a class="headerlink" href="#annealedvae" title="Permanent link">&para;</a></h4>
<p>In a follow-up work<sup id="fnref:11"><a class="footnote-ref" href="#fn:11">11</a></sup>, the authors propose to progressively increase the bottleneck capacity, so that the encoder can focus on learning one factor of variation at a time and eventually capture more factors of variations than <span class="arithmatex">\(\beta\)</span>-VAE:</p>
<div class="arithmatex">\[
\E_{p(\xbf)} \left[
    \E_{q_\phi(\zbf|\xbf)}\log p_\theta(\xbf|\zbf) - \gamma |\KL(q_\phi(\zbf|\xbf) \Vert p(\zbf)) - C|
\right]
\]</div>
<p>where <span class="arithmatex">\(C\)</span> is annealed from zero to some value large enough to produce good representation.</p>
<h3 id="penalizing-the-total-correlation">Penalizing the total correlation<a class="headerlink" href="#penalizing-the-total-correlation" title="Permanent link">&para;</a></h3>
<p>Note that the KL regularizer term can be rewritten as</p>
<div class="arithmatex">\[
\begin{align}
 &amp; \E_{p(\xbf)} \KL(q_\phi(\zbf|\xbf) \Vert p(\zbf)) \\
=&amp; \E_{q(\zbf, \xbf)} \log \left( \frac{q(\zbf|\xbf)}{p(\zbf)}\frac{q(\zbf)}{q(\zbf)} \right) \\
=&amp; \E_{q(\zbf, \xbf)} \log \frac{q(\zbf|\xbf)}{q(\zbf)} + \E_{q(\zbf)} \log \frac{q(\zbf)}{p(\zbf)} \\
=&amp; I_q(\xbf; \zbf) + \KL(q(\zbf) \Vert p(\zbf)) \\
=&amp; I_q(\xbf; \zbf) + \E_{q(\zbf)} \log \left( \frac{q(\zbf)}{\prod_j p(z_j)} \frac{\prod_j q(z_j)}{\prod_j q(z_j)} \right) \\
=&amp; I_q(\xbf; \zbf) + \KL(q(\zbf) \Vert \prod_j(q(z_j))) + \sum_j\KL(q(z_j) \Vert p(z_j))
\end{align}
\]</div>
<p>where <span class="arithmatex">\(q(\zbf) = \sum_\xbf p(\xbf) q(\zbf|\xbf)\)</span> is the aggregated posterior. Kim and Minh <sup id="fnref3:5"><a class="footnote-ref" href="#fn:5">5</a></sup> argues that penalizing <span class="arithmatex">\(I_q(\xbf; \zbf)\)</span> is neither necessary nor desirable for disentanglement. Instead, they opt to minimize the <em>total correlation</em> of <span class="arithmatex">\(q(\zbf)\)</span> (the second term in the last line of above equation), yielding the following objective:</p>
<div class="arithmatex">\[
\E_{p(\xbf)} \left[
    \E_{q_\phi(\zbf|\xbf)}\log p_\theta(\xbf|\zbf) - \KL(q_\phi(\zbf|\xbf) \Vert p(\zbf))
\right] - \gamma \KL(q(\zbf) \Vert \prod_j q(z_j))
\]</div>
<h4 id="factorvae">FactorVAE<a class="headerlink" href="#factorvae" title="Permanent link">&para;</a></h4>
<p>Kim and Minh <sup id="fnref4:5"><a class="footnote-ref" href="#fn:5">5</a></sup> propose to estimate the total correlation using the density ratio trick, i.e. train an additional classifier <span class="arithmatex">\(D\)</span> to predict if the latent comes from <span class="arithmatex">\(q(\zbf)\)</span> or <span class="arithmatex">\(\prod_j q(z_j)\)</span>. Latent samples from <span class="arithmatex">\(\prod_j q(z_j)\)</span> are obtained by randomly permuting the dimensions within latents inferred in a minibatch.</p>
<h4 id="beta-tcvae"><span class="arithmatex">\(\beta\)</span>-TCVAE<a class="headerlink" href="#beta-tcvae" title="Permanent link">&para;</a></h4>
<p><span class="arithmatex">\(\beta\)</span>-TCVAE optimizes the same objective as FactorVAE, but uses a tractable biased Monte-Carlo estimate for the total correlation. Let <span class="arithmatex">\(\Bcal_M = (\xbf_1, \dots, \xbf_M)\)</span> be a minibatch of size <span class="arithmatex">\(M\)</span> sampled i.i.d. from the training set of size <span class="arithmatex">\(N\)</span>. Therefore, <span class="arithmatex">\(p(\Bcal_M) = (\frac{1}{N})^M\)</span>. Let <span class="arithmatex">\(r(\Bcal_M|n)\)</span> denote the probability of a sampled minibatch where one of the elements is fixed to be index <span class="arithmatex">\(n\)</span>, thus <span class="arithmatex">\(p(r(\Bcal_M|n)) = (\frac{1}{N})^{M-1}\)</span>. Then</p>
<div class="arithmatex">\[
\begin{align}
 &amp; \E_{q(\zbf)} \log q(\zbf) \\
=&amp; \E_{q(\zbf, n)} \log \E_{p(\Bcal_M)} \left[\frac{1}{M} \sum_{m=1}^M q(\zbf | n_m) \right] \\
\ge&amp; \E_{q(\zbf, n)} \log \E_{r(\Bcal_M|n)} \left[ \frac{p(\Bcal_M)}{r(\Bcal_M|n)}\frac{1}{M} \sum_{m=1}^M q(\zbf | n_m) \right] \\
=&amp; \E_{q(\zbf, n)} \log \E_{r(\Bcal_M|n)} \left[ \frac{1}{MN} \sum_{m=1}^M q(\zbf | n_m) \right] \\
\approx&amp; \frac{1}{M} \sum_{i=1}^M \left[ \log \frac{1}{NM} \sum_{m=1}^M q(\zbf_i | n_m) \right]
\end{align}
\]</div>
<p>The inequality is due to <span class="arithmatex">\(r\)</span> having a support that is a subset of that of <span class="arithmatex">\(p\)</span>. <span class="arithmatex">\(\E_{q(z_j)} \log q(z_j)\)</span> is estimated in the same way.</p>
<h3 id="disentangling-the-inferred-prior-aggregated-posterior">Disentangling the Inferred Prior (Aggregated Posterior)<a class="headerlink" href="#disentangling-the-inferred-prior-aggregated-posterior" title="Permanent link">&para;</a></h3>
<p>The authors of <sup id="fnref3:9"><a class="footnote-ref" href="#fn:9">9</a></sup> argue that a disentangled generative model requires a disentangled inferred prior (aggregated posterior) <span class="arithmatex">\(q(\zbf)\)</span>. They propose to optimize</p>
<div class="arithmatex">\[
\E_{p(\xbf)} \left[
    \E_{q_\phi(\zbf|\xbf)}\log p_\theta(\xbf|\zbf) - \KL(q_\phi(\zbf|\xbf) \Vert p(\zbf))
\right] - \lambda \Dcal(q(\zbf) \Vert p(\zbf))
\]</div>
<p>where <span class="arithmatex">\(\Dcal\)</span> is some arbitrary divergence. Since the additional term is intractable, we could either use <a href="../representation_learning_with_mutual_information_maximization/#variational-representation-of-the-f-divergence">variational formulation of KL-divergence</a> as in Adversarial AutoEncoeders (AAEs)<sup id="fnref:12"><a class="footnote-ref" href="#fn:12">12</a></sup> or simply perform moment matching as in DIP-VAE<sup id="fnref4:9"><a class="footnote-ref" href="#fn:9">9</a></sup>.</p>
<h4 id="dip-vae">DIP-VAE<a class="headerlink" href="#dip-vae" title="Permanent link">&para;</a></h4>
<p>DIP-VAE<sup id="fnref5:9"><a class="footnote-ref" href="#fn:9">9</a></sup> proposes to match the covariance of <span class="arithmatex">\(q(\zbf)\)</span> and <span class="arithmatex">\(p(\zbf)\)</span>. This amounts to decorrelating the dimensions of <span class="arithmatex">\(\zbf \sim q_\phi(\zbf)\)</span> if <span class="arithmatex">\(p(\zbf) = \Ncal(0, I)\)</span>. By <a href="https://en.wikipedia.org/wiki/Law_of_total_covariance">the law of covariance</a>, the covariance of <span class="arithmatex">\(\zbf \sim q(\zbf)\)</span> is given by</p>
<div class="arithmatex">\[
\Cov_{q_\phi(\zbf)}\zbf = \E_{p(\xbf)} \Cov_{q_\phi(\zbf|\xbf)}\zbf + \Cov_{p(\xbf)} \E_{q_\phi(\zbf|\xbf)}\zbf
\]</div>
<p>where <span class="arithmatex">\(\E_{q_\phi(\zbf|\xbf)}\zbf\)</span> and <span class="arithmatex">\(\Cov_{q_\phi(\zbf|\xbf)}\zbf\)</span> are functions of the random variable <span class="arithmatex">\(\xbf\)</span> (<span class="arithmatex">\(\zbf\)</span> is marginalized over). For VAE models with Gaussian posterior of <span class="arithmatex">\(\zbf\)</span>, i.e. <span class="arithmatex">\(q_\phi(\zbf, \xbf) = \Ncal(\mu_\phi(\xbf), \Sigma_\phi(\xbf))\)</span>, the covariance of <span class="arithmatex">\(\zbf \sim q(\zbf)\)</span> reduces to</p>
<div class="arithmatex">\[
\Cov_{q_\phi(\zbf)}\zbf = \E_{p(\xbf)} \Sigma_\phi(\xbf) + \Cov_{p(\xbf)} \mu_\phi(\xbf)
\]</div>
<p>which we want to be close to the identity matrix. We choose entry-wise squared <span class="arithmatex">\(\ell_2\)</span>-norm as the measure of proximity.</p>
<p>Further, <span class="arithmatex">\(\Sigma_\phi(\xbf)\)</span> is often taken as diagonal, which means that cross-correlations (off-diagonals) between the latents are due to only <span class="arithmatex">\(\Cov_{p(\xbf)} \mu_\phi(\xbf)\)</span>. This suggests two possible options for the disentangling regularizer:</p>
<ul>
<li><strong>DIP-VAE-I</strong> &emsp; Regularizing only <span class="arithmatex">\(\Cov_{p(\xbf)} \mu_\phi(\xbf)\)</span>. The objective becomes</li>
</ul>
<div class="arithmatex">\[
\max_{\theta, \phi} \ELBO(\theta, \phi) - \lambda_1 \sum_{i \ne j} \left[\Cov_{p(\xbf)\mu_{\phi}(\xbf)}\right]^2_{ij} - \lambda_2 \sum_i \left(\left[\Cov_{p(\xbf)} \mu_\phi(\xbf)\right]_{ii} - 1\right)^2
\]</div>
<ul>
<li><strong>DIP-VAE-II</strong> &emsp; Regularizing <span class="arithmatex">\(\Cov_{q_\phi(\zbf)}\zbf\)</span>. The objective is</li>
</ul>
<div class="arithmatex">\[
\max_{\theta, \phi} \ELBO(\theta, \phi) - \lambda_1 \sum_{i \ne j} \left[\Cov_{q_\phi(\zbf)}\zbf\right]^2_{ij} - \lambda_2 \sum_i \left(\left[\Cov_{q_\phi(\zbf)}\zbf\right]_{ii} - 1\right)^2
\]</div>
<p>The authors argue that in datasets where the number of generative factors is less than the latent dimension, DIP-VAE-II is more suitable than DIP-VAE-I as keeping all dimensions active (optimizing towards <span class="arithmatex">\(\Var_{q_\phi(\zbf)}z_i=1\)</span> for all <span class="arithmatex">\(i\)</span>'s) might result in splitting of an attribute across multiple dimensions, hurting the goal of disentanglement.</p>
<h4 id="aae">AAE<a class="headerlink" href="#aae" title="Permanent link">&para;</a></h4>
<p>Adversarial autoencoders<sup id="fnref2:12"><a class="footnote-ref" href="#fn:12">12</a></sup> attaches an adversarial discriminator on top of the latent representation of an autoencoder to match the aggregated posterior <span class="arithmatex">\(q(\zbf)\)</span> to an arbitrary easy-to-sample prior <span class="arithmatex">\(p(\zbf)\)</span>.</p>
<p>The network is trained jointly in two phases:</p>
<ol>
<li>In the <em>reconstruction</em> phase, the autoencoder minimizes the reconstruction error of the inputs.</li>
<li>In the <em>regularization</em> phase, the discriminator is first updated to tell apart the true samples (generated using the prior) from the generated samples (the hidden codes computed by the encoder). Then the encoder is updated to confuse the discriminator.</li>
</ol>
<p><img alt="Adversarial Autoencoders" src="../../assets/images/DRL-AAE.png" />
AAE has many variants aimed for different learning settings. (left) Adversarial Autoencoders; (middle) Semi-supervised AAE; (right) AAE for Dimensionality Reduction.</p>
<ul>
<li><strong>Semi-supervised AAE</strong> has two adversarial discriminator: one for label <span class="arithmatex">\(y\)</span> and another for style representation <span class="arithmatex">\(\zbf\)</span>.<ol>
<li>In the <em>reconstruction</em> phase, the autoencoder minimize the reconstruction error of the inputs on an unlabeled mini-batch.</li>
<li>In the <em>regularization</em> phase, each of the discriminators is updated to tell apart the true samples (generated using the Categorical and Gaussian priors) from the generated samples (the hidden codes computed by the encoder). Then the encoders are updated to confuse their discriminators.</li>
<li>In the <em>classification</em> phase, the encoder is updated to minimize the cross-entropy cost on a labeled mini-batch.</li>
</ol>
</li>
<li><strong>AAE for Unsupervised Clustering</strong> has the same architecture as the Semi-supervised AAE, while it does not have the classification phase.</li>
<li><strong>AAE for Dimensionality Reduction</strong> aims to learn a good representation that encodes both the label and style information. The final representation is achieved by adding the distributed representation of the cluster head with the style representation, both <span class="arithmatex">\(n\)</span>-dimensional. The cluster head representation is obtained by multiplying the <span class="arithmatex">\(m\)</span> dimensional one-hot class label vector by an <span class="arithmatex">\(m \times n\)</span> matrix <span class="arithmatex">\(W_C\)</span>. We introduce an additional cost function that linearly penalizes the Euclidean distance between every two cluster heads if they are greater than a threshold <span class="arithmatex">\(\eta\)</span>.</li>
</ul>
<h4 id="infovae">InfoVAE<a class="headerlink" href="#infovae" title="Permanent link">&para;</a></h4>
<p>The InfoVAE paper<sup id="fnref:13"><a class="footnote-ref" href="#fn:13">13</a></sup> points out that the amortized inference distribution <span class="arithmatex">\(q_\phi(\zbf|\xbf)\)</span> of VAEs might fail to approximate the true posterior <span class="arithmatex">\(p_\theta(\zbf|\xbf)\)</span> for two reasons:</p>
<ul>
<li>The ELBO can be maximized (even to infinity in extreme cases) with a very inaccurate variational posterior.</li>
<li>Implicit modeling bias: common modeling choices (such as the high dimensionality of <span class="arithmatex">\(\xbf\)</span> compared to <span class="arithmatex">\(\zbf\)</span>) tend to sacrifice variational inference for data fit when modeling capacity is not sufficient to achieve both.</li>
</ul>
<p>Regardless of the cause, this is generally an undesirable phenomenon for two reasons:</p>
<ul>
<li>One may care about accurate inference (e.g. in representation learning) more than generating sharp samples.</li>
<li>Overfitting: Because <span class="arithmatex">\(p_{\mathrm{data}}\)</span> is an empirical (finite) distribution in practice, matching it too closely can lead to poor generalization.</li>
</ul>
<p>To remedy this issue, the authors propose an additional mutual information term between <span class="arithmatex">\(\xbf\)</span> and <span class="arithmatex">\(\zbf\)</span> under the joint "inference" distribution <span class="arithmatex">\(q_\phi(\xbf, \zbf)\)</span>, which is very similar to the additional term in DIP-VAE discussed <a href="#disentangle-the-inferred-prior">above</a>.</p>
<div class="arithmatex">\[
\begin{align}
&amp;\Lcal_{\mathrm{InfoVAE}} \\
=&amp; -\lambda\KL(q_\phi(\zbf) \Vert p(\zbf)) - \E_{q(\zbf)}\left[ \KL(q_\phi(\xbf|\zbf) \Vert p_\theta(\xbf|\zbf)) \right] + \alpha I_q(\xbf;\zbf)\\
=&amp; \E_{p(\xbf)} \left[
    \E_{q_\phi(\zbf|\xbf)}\log p_\theta(\xbf|\zbf) - (1 - \alpha) \KL(q_\phi(\zbf|\xbf) \Vert p(\zbf))
\right] - (\alpha + \lambda - 1) \KL(q_\phi(\zbf) \Vert p(\zbf))
\end{align}
\]</div>
<p>We can easily see that <a href="#beta-vae"><span class="arithmatex">\(\beta\)</span>-VAE</a> (<span class="arithmatex">\(\lambda &gt; 0\)</span> and <span class="arithmatex">\(\alpha = 1 - \lambda\)</span>) and <a href="#aae">AAE</a> (<span class="arithmatex">\(\alpha = \lambda = 1\)</span>, Use Jenson-Shannon Divergence to approximate <span class="arithmatex">\(\KL(q(\zbf) \Vert p(\zbf))\)</span>) belongs to this framework. If we relax the last KL term to any strict divergence (strict as in <span class="arithmatex">\(\Dcal(q\Vert p) = 0\)</span> iff <span class="arithmatex">\(q = p\)</span>), we can get three categories of InfoVAE:</p>
<ul>
<li><strong>Adversarial Training</strong> as discussed in <a href="#aae">AAE</a>.</li>
<li><strong>Stein Variational Gradient<sup id="fnref:14"><a class="footnote-ref" href="#fn:14">14</a></sup></strong> aims to find the steepest direction <span class="arithmatex">\(\phi_{q,p}\)</span> that transforms <span class="arithmatex">\(q(\zbf)\)</span> towards <span class="arithmatex">\(p(\zbf)\)</span>. For a small step size <span class="arithmatex">\(\epsilon\)</span>, the transformation <span class="arithmatex">\(T(\zbf)\)</span> is defined as <span class="arithmatex">\(T(\zbf) = z + \epsilon \phi_{q,p}(\zbf)\)</span>. The <span class="arithmatex">\(\phi^\star\)</span> that minimizes <span class="arithmatex">\(\KL(T(\zbf) \Vert p(\zbf))\)</span> is
    $$
    \phi_{q,p}^\star(\cdot) = \E_{q(\zbf)} \left[ k(\zbf,\cdot)\nabla_\zbf \log p(\zbf) + \nabla_\zbf k(\zbf,\cdot) \right]
    $$
    where <span class="arithmatex">\(k\)</span> is a positive definite kernel function. In practice we use minibatches to estimate the Stein gradients and use the surrogate loss <span class="arithmatex">\(\widehat{\Dcal}(q_\phi(\zbf) \Vert p(\zbf)) = z^\top \stopgradient(\phi^\star_{q_\phi, p}(\zbf))\)</span></li>
<li><strong>Maximum-Mean Discrepancy</strong> is a framework to quantify the distance between two distributions by comparing all of their moments. It can be efficiently implemented using the kernel trick. Letting <span class="arithmatex">\(k(\cdot, \cdot)\)</span> be any positive definite kernel, the MMD between <span class="arithmatex">\(p\)</span> and <span class="arithmatex">\(q\)</span> is
    $$
    \MMD(q \Vert p) = \E_{p(\zbf), p(\zbf^\prime)} k(\zbf,\zbf^\prime) - 2\E_{q(\zbf), p(\zbf^\prime)} k(\zbf,\zbf^\prime) + \E_{q(\zbf), q(\zbf^\prime)} k(\zbf,\zbf^\prime)
    $$
    <span class="arithmatex">\(\MMD(q \Vert p)\)</span> if and only if <span class="arithmatex">\(p = q\)</span>.</li>
</ul>
<p>The authors empirically shows that the MMD variant perform on-par or better than the other two in terms of distance between the aggregated posterior <span class="arithmatex">\(q_\phi(\zbf)\)</span> and the prior <span class="arithmatex">\(p(\zbf)\)</span>, distance between sample label distribution (obtained by a pretrained classifier) and the true label distribution, training speed and stablity, semi-supervised learning on 1k labeled data.</p>
<h3 id="maximizing-mi-bt-latent-code-and-gan-samples">Maximizing MI b/t Latent Code and GAN Samples<a class="headerlink" href="#maximizing-mi-bt-latent-code-and-gan-samples" title="Permanent link">&para;</a></h3>
<p>Generative Adversarial Networks (GANs) are usually not used for representation learning since it does not have an encoder. But if one wants to generate samples in an interpretable and disentangled way, InfoGAN is one option.</p>
<h4 id="infogan">InfoGAN<a class="headerlink" href="#infogan" title="Permanent link">&para;</a></h4>
<p>InfoGAN<sup id="fnref:15"><a class="footnote-ref" href="#fn:15">15</a></sup> splits the latent vector of GAN into two parts: the incompressible noise <span class="arithmatex">\(\zbf\)</span>, and the latent code <span class="arithmatex">\(\cbf\)</span> that target the salient structured semantic features of the data distribution. In its simplest form, we may assume a factored distribution for <span class="arithmatex">\(\cbf\)</span>, given by <span class="arithmatex">\(P(\cbf) = \prod_{i=1}^K P(c_i)\)</span>. To avoid the generator ignoring <span class="arithmatex">\(\cbf\)</span>, a mutual information constraint is added between the latent code and the generated samples, making the overall objective (<span class="arithmatex">\(Q\)</span> will be introduced below)</p>
<div class="arithmatex">\[
\min_{G,Q} \max_D \E_{\xbf \sim p_{\mathrm{data}}}\log D(\xbf) + \E_{[\zbf;\cbf] \sim p_{\mathrm{noise}}} \log (1 - D(G(\zbf, \cbf))) - \lambda I(\cbf, G(\zbf, \cbf))
\]</div>
<p>To maximize mutual information, the authors employ variational MI maximization.</p>
<div class="arithmatex">\[
\begin{align}
&amp;I(\cbf, G(\zbf, \cbf)) \\
=&amp; H(\cbf) - H(\cbf|G(\zbf,\cbf)) \\
=&amp; \E_{\xbf \sim G(\zbf,\cbf)} \E_{\cbf^\prime \sim P(\cbf|\xbf)} \log P(\cbf^\prime|\xbf) + H(\cbf) \\
=&amp; \E_{\xbf \sim G(\zbf,\cbf)} \left[ \KL(P(\cdot|\xbf) \Vert Q(\cdot|\xbf)) + \E_{\cbf^\prime \sim P(\cbf|\xbf)} \log Q(\cbf^\prime|\xbf) \right] + H(\cbf) \\
\ge&amp; \E_{\xbf \sim G(\zbf,\cbf)} \E_{\cbf^\prime \sim P(\cbf|\xbf)} \log Q(\cbf^\prime|\xbf) + H(\cbf) \\
=&amp; \E_{\cbf \sim P(\cbf)} \E_{\xbf \sim G(\zbf, \cbf)} \log Q(\cbf|\xbf) + H(\cbf)
\end{align}
\]</div>
<p>For simplicity, the authors fix the latent code distribution and treat <span class="arithmatex">\(H(\cbf)\)</span> as a constant. Note that this variational objective <span class="arithmatex">\(\Lcal_{\mathrm{Info}}(G, Q)\)</span> is easy to approximate with Monte Carlo simulation. In particular, it can be maximized w.r.t. <span class="arithmatex">\(Q\)</span> directly and w.r.t. <span class="arithmatex">\(G\)</span> via the reparametrization trick.</p>
<p>From a representation learning perspective, <span class="arithmatex">\(Q\)</span> can be seen as an encoder. The discriminator <span class="arithmatex">\(D\)</span> and the encoder <span class="arithmatex">\(Q\)</span> share all convolutional layers and have separate fully-connected final layers. For categorical latent code <span class="arithmatex">\(\cbf\)</span>, we use the natural choice of softmax nonlinearity to represent <span class="arithmatex">\(Q(c_i|\xbf)\)</span>. For continuous latent code <span class="arithmatex">\(\cbf\)</span>, there are more options depending on what is the true posterior <span class="arithmatex">\(P(c_i |\xbf)\)</span>. The authors found that simply treating <span class="arithmatex">\(Q(c_i |\xbf)\)</span> as a factored Gaussian is sufficient.</p>
<p>Note that the InfoGAN-CR authors found<sup id="fnref:16"><a class="footnote-ref" href="#fn:16">16</a></sup> that InfoGAN performance can be significantly improved with better architectural and hyperparameter choices.</p>
<h4 id="infogan-cr">InfoGAN-CR<a class="headerlink" href="#infogan-cr" title="Permanent link">&para;</a></h4>
<p>InfoGAN-CR<sup id="fnref2:16"><a class="footnote-ref" href="#fn:16">16</a></sup> is inspired by the idea that disentanglement is characterized by distinct changes in the images when traversing the latent space. A constrastive regularizer (CR) is designed to encourage this behavior: Pairs of images <span class="arithmatex">\((x, x^\prime) \sim R^{(i)}\)</span> are generated by fixing the <span class="arithmatex">\(i\)</span>-th latent code <span class="arithmatex">\(c_i\)</span> and drawing the rest of the latent codes with a controlled contrastive gap (details below). The authors propose to maximize the distinctness of this latent traversal with generalized Jensen-Shannon divergence among <span class="arithmatex">\(R^{(i)}\)</span>'s</p>
<div class="arithmatex">\[
\JS(R^{1}, \dots, R^{(K)}) = \frac{1}{K}\sum_{i \in [K]} \KL(R^{(i)} \Vert \bar{R})
\]</div>
<p>where <span class="arithmatex">\(\bar{R} = \frac{1}{K}\sum_{j \in [K]} R^{(j)}\)</span>.</p>
<p><img alt="InfoGAN-CR" class="image-center" src="../../assets/images/DRL-InfoGAN-CR.png" style="width: 70%" /></p>
<p>To implement the CR, the authors introduce a CR discriminator <span class="arithmatex">\(H: \R^N \times \R^N \to \R^K\)</span> that performs multi-way hypothesis testing, where <span class="arithmatex">\(N\)</span> is the input dimension and <span class="arithmatex">\(K\)</span> is the latent code dimension. The discriminator <span class="arithmatex">\(H\)</span> tries to identify which code <span class="arithmatex">\(i\)</span> was shared between the paired images. Both the generator <span class="arithmatex">\(G\)</span> and the discriminator <span class="arithmatex">\(H\)</span> try to make the <span class="arithmatex">\(K\)</span>-way hypothesis testing successful. We add the following cross-entropy loss to the training objective</p>
<div class="arithmatex">\[
\min_{G,H}\Lcal_{\mathrm{CR}}(G, H) = -\E_{i \sim \mathrm{U}([K]), (x, x^\prime) \sim R^{(i)}} \log H_i(x, x^\prime)
\]</div>
<p>The overall objective then becomes</p>
<div class="arithmatex">\[
\min_{G, H, Q}\max_{D} \Lcal_{\mathrm{GAN}}(G, D) - \lambda\Lcal_{\mathrm{Info}}(G, Q) + \alpha \Lcal_{\mathrm{CR}}(G, H)
\]</div>
<p><strong>Justification of the CR implementation.</strong> We can verify using Langrange's Multiplier that <span class="arithmatex">\(H_i(x, x^\prime) = \frac{R^{(i)}(x, x^\prime)}{\sum_j R^{(j)}(x, x^\prime)}\)</span> is a minimizer of <span class="arithmatex">\(\Lcal_{\mathrm{CR}}\)</span>. The minimum loss is</p>
<div class="arithmatex">\[
\begin{align}
&amp;-\E_{i \sim \mathrm{U}([K]), (x, x^\prime) \sim R^{(i)}} \log H_i(x, x^\prime) \\
=&amp; -\frac{1}{K} \sum_{i=1}^K \E_{(x, x^\prime) \in R^{(i)}} \log \frac{R^{(i)}(x, x^\prime)/K}{\sum_i R^{(i)}(x, x^\prime)/K} \\
=&amp; \log K - \frac{1}{K} \sum_{i=1}^K \KL \left( R^{(i)} \Vert \bar{R} \right) \\
=&amp; \log K - \JS(R^{(1)}, \dots, R^{(K)})
\end{align}
\]</div>
<p><strong>Progressive training.</strong> The authors propose to progressively narrow the <em>contrastive gap</em>, defined as <span class="arithmatex">\(\min_{j \in [K]\backslash{i}}|c_j - c_j^\prime|\)</span>, during training. This means the varying dimensions of the contrastive samples would get smaller and smaller variance, making them look like the fixed dimension <span class="arithmatex">\(i\)</span> more and more and thus result in more and more difficult prediction for <span class="arithmatex">\(H\)</span>.</p>
<h4 id="elastic-infogan">Elastic-InfoGAN<a class="headerlink" href="#elastic-infogan" title="Permanent link">&para;</a></h4>
<h3 id="leveraging-the-hierarchy-of-latent-variables">Leveraging the Hierarchy of Latent Variables<a class="headerlink" href="#leveraging-the-hierarchy-of-latent-variables" title="Permanent link">&para;</a></h3>
<h4 id="stylegan">StyleGAN<a class="headerlink" href="#stylegan" title="Permanent link">&para;</a></h4>
<h4 id="very-deep-vae">Very Deep VAE<a class="headerlink" href="#very-deep-vae" title="Permanent link">&para;</a></h4>
<h2 id="the-impossibility-of-unsupervised-disentanglement">The Impossibility of Unsupervised Disentanglement<a class="headerlink" href="#the-impossibility-of-unsupervised-disentanglement" title="Permanent link">&para;</a></h2>
<p>F. Locatello et al.<sup id="fnref2:2"><a class="footnote-ref" href="#fn:2">2</a></sup> showed that the unsupervised learning of disentangled representations is theoretically impossible from i.i.d. observations without inductive biases.
After observing <span class="arithmatex">\(\xbf\)</span>, we can construct infinitely many generative models which have the same marginal distribution of <span class="arithmatex">\(\xbf\)</span>.
Any one of these models could be the true causal generative model for the data, and the right model cannot be identified given only the distribution of <span class="arithmatex">\(\xbf\)</span>.</p>
<h2 id="learning-from-partial-class-labels">Learning from Partial Class Labels<a class="headerlink" href="#learning-from-partial-class-labels" title="Permanent link">&para;</a></h2>
<h3 id="latent-feature-discriminative-model-m1">Latent Feature Discriminative Model (M1)<a class="headerlink" href="#latent-feature-discriminative-model-m1" title="Permanent link">&para;</a></h3>
<p>Proposed by D. P. Kingma et al.<sup id="fnref:19"><a class="footnote-ref" href="#fn:19">19</a></sup>.</p>
<ol>
<li>Use VAE to learn an embedding space for all the training data.</li>
<li>Train a classifier (transductive SVM / multinomial regression) on the labeled data in the embedding space.</li>
</ol>
<h3 id="semi-supervised-vae">Semi-Supervised VAE<a class="headerlink" href="#semi-supervised-vae" title="Permanent link">&para;</a></h3>
<h4 id="m2">M2<a class="headerlink" href="#m2" title="Permanent link">&para;</a></h4>
<p>Proposed by D. P. Kingma et al.<sup id="fnref2:19"><a class="footnote-ref" href="#fn:19">19</a></sup>. The generative process is described as</p>
<div class="arithmatex">\[
p(\ybf) = \mathrm{Cat}(\ybf|\pi) \qquad p(\zbf) = \Ncal(\zbf|\mathbf{0}, I) \qquad p_\theta(\xbf| \ybf, \zbf) = f_\theta(\xbf; \ybf, \zbf)
\]</div>
<p>The inference distribution is parameterized as</p>
<div class="arithmatex">\[
q_\phi(\zbf|\ybf, \xbf) = \Ncal(\zbf | \mu_\phi(\ybf, \xbf), \diag(\sigma^2_\phi (\ybf, \xbf))) \qquad q_\phi(\ybf|\xbf) = \mathrm{Cat}(\ybf|\pi_\phi(\xbf))
\]</div>
<p>The learning objective is a weighted sum of the ELBO loss of <span class="arithmatex">\(\log p_\theta(\xbf, \ybf)\)</span>, and the classification loss on <span class="arithmatex">\(q_\phi(\ybf | \xbf)\)</span>. As this model is an instance of <a href="#generalized-m2">the Generalized M2 model</a>, we leave the detailed discussion below.</p>
<p>For a systematic benchmark of the performance of models combining M2 and the unsupervised VAE disentanglement methods discussed <a href="#unsupervised-disentanglement">above</a>, see<sup id="fnref:21"><a class="footnote-ref" href="#fn:21">21</a></sup>.</p>
<h4 id="generalized-m2">Generalized M2<a class="headerlink" href="#generalized-m2" title="Permanent link">&para;</a></h4>
<p>N. Siddharth et al.<sup id="fnref:17"><a class="footnote-ref" href="#fn:17">17</a></sup> focus on the setting where some factors <span class="arithmatex">\(\ybf\)</span> are interpretable and partly labeled. They generalize the M2 model to arbitrary conditional dependency structure. They first define an objective over <span class="arithmatex">\(N\)</span> unsupervised data points <span class="arithmatex">\(\Dcal = \{\xbf^i\}_{i=1}^N\)</span> and <span class="arithmatex">\(M\)</span> supervised data points <span class="arithmatex">\(\Dsup = \{(\xbf^i, \ybf^i)\}_{i=1}^M\)</span>.</p>
<div class="arithmatex">\[
\Lcal(\theta, \phi; \Dcal, \Dsup) = \sum_{i=1}^N \ELBO(\theta, \phi; \xbf^n) + \gamma \sum_{i=1}^M \Lcal_{\mathrm{sup}}(\theta, \phi; \xbf^m, \ybf^m)
\]</div>
<p>The supervised loss is the ELBO loss of <span class="arithmatex">\(\log p_\theta(\xbf, \ybf)\)</span>, and the cross entropy loss encouraging accurate inference of <span class="arithmatex">\(\ybf\)</span>.</p>
<div class="arithmatex">\[
\Lcal_{\mathrm{sup}}(\theta, \phi; \xbf^m, \ybf^m) = \E_{q_\phi(\zbf|\xbf^m, \ybf^m)} \log \frac{p_\theta(\xbf^m, \ybf^m, \zbf)}{q_\phi(\zbf|\xbf^m,\ybf^m)} + \alpha\log q_\phi(\ybf^m|\xbf^m)
\]</div>
<p><strong>Calculating the supervised loss.</strong> Note that this definition implicitly assumes that we can evaluate the conditional probability <span class="arithmatex">\(q_\phi(\zbf | \xbf, \ybf)\)</span> and the marginal <span class="arithmatex">\(q_\phi(\ybf|\xbf) = \int q_\phi(\ybf, \zbf|\xbf) \dd \zbf\)</span>. This would be the case if we factor <span class="arithmatex">\(q_\phi(\ybf, \zbf|\xbf)\)</span> into <span class="arithmatex">\(q_\phi(\zbf | \xbf, \ybf)q_\phi(\ybf|\xbf)\)</span>, as in <a href="#m2-generative-semi-supervised-model">the M2 model</a>. But if we have another conditional dependence structure such as <span class="arithmatex">\(q_\phi(\ybf, \zbf|\xbf) = q_\phi(\ybf | \xbf, \zbf)q_\phi(\zbf|\xbf)\)</span>, we would run into the following difficulties:</p>
<ol>
<li>Evaluating the density <span class="arithmatex">\(q_\phi(\zbf|\xbf, \ybf) = \frac{q_\phi(\ybf, \zbf|\xbf)}{\int q_\phi(\ybf, \zbf|\xbf) \dd \zbf}\)</span> becomes difficult</li>
<li>Generating samples, e.g. sampling from <span class="arithmatex">\(q_\phi(\zbf|\xbf, \ybf)\)</span> requires inference</li>
</ol>
<p>We first re-express <span class="arithmatex">\(\Lcal_{\mathrm{sup}}\)</span> as follows, which alleviates (1.) by removing the need to evaluate <span class="arithmatex">\(q_\phi(\zbf|\xbf, \ybf)\)</span>.
$$
\Lcal_{\mathrm{sup}}(\theta, \phi; \xbf^m, \ybf^m) = \E_{q_\phi(\zbf|\xbf^m, \ybf^m)} \log \frac{p_\theta(\xbf^m, \ybf^m, \zbf)}{q_\phi(\ybf^m,\zbf|\xbf^m)} + (1 + \alpha)\log q_\phi(\ybf^m|\xbf^m)
$$</p>
<p>We can then use <em>(self-normalised) importance sampling</em> to approximate the supervsied loss. Specifically, we sample <em>proposals</em> <span class="arithmatex">\(\zbf^{m,s} \sim q_\phi(\zbf|\xbf^m)\)</span> from the unconditioned encoder distribution and estimate the first supervised term as follows:</p>
<div class="arithmatex">\[
\E_{q_\phi(\zbf|\xbf^m, \ybf^m)} \log \frac{p_\theta(\xbf^m, \ybf^m, \zbf)}{q_\phi(\ybf^m,\zbf|\xbf^m)} \approx \frac{1}{S}\sum_{s=1}^S \frac{w^{m,s}}{Z^m} \log \frac{p_\theta(\xbf^m, \ybf^m, \zbf)}{q_\phi(\ybf^m,\zbf|\xbf^m)}
\]</div>
<p>where the unnormalized weight <span class="arithmatex">\(w^{m,s}\)</span> and the normalizer <span class="arithmatex">\(Z^m\)</span> are defined as</p>
<div class="arithmatex">\[
w^{m,s} = \frac{q_\phi(\ybf^m, \zbf^{m,s}|\xbf^m)}{q_\phi(\zbf^{m,s}|\xbf^m)} = q_\phi(\ybf^m|\zbf^{m,s},\xbf^m) \qquad Z^m = \sum_{s=1}^S w^{m,s}
\]</div>
<p>Note that the unnormalized weight is the product of variational conditional probabilities of the supervised variables <span class="arithmatex">\(\ybf\)</span>. We then estimate a lower bound of <span class="arithmatex">\(\log q_\phi(\ybf^m|\xbf^m)\)</span> as follows:</p>
<div class="arithmatex">\[
\log q_\phi(\ybf^m | \xbf^m) \ge \E_{q_\phi(\zbf|\xbf^m)} \log \frac{q_\phi(\ybf^m, \zbf^{m,s}|\xbf^m)}{q_\phi(\zbf^{m,s}|\xbf^m)} \approx \frac{1}{S}\sum_{s=1}^S \log w^{m,s}
\]</div>
<p>The estimated supervised loss is</p>
<div class="arithmatex">\[
\widehat{\Lcal}_{\mathrm{sup}}(\theta, \phi; \xbf^m, \ybf^m) = \frac{1}{S}\sum_{s=1}^S \left[\frac{w^{m,s}}{Z^m} \log \frac{p_\theta(\xbf^m, \ybf^m, \zbf)}{q_\phi(\ybf^m,\zbf|\xbf^m)} + (1+\alpha) \log w^{m,s}\right]
\]</div>
<p><strong>Universal applicability of the framework.</strong> The above framework is applicable to any conditional dependency structure, e.g., if we have</p>
<div class="arithmatex">\[
q_\phi(\zbf_2, \ybf, \zbf_1|\xbf) = q_\phi(\zbf_2|\ybf,\zbf_1,\xbf) q_\phi(\ybf|\zbf_1,\xbf) q_\phi(\zbf_1|\xbf)
\]</div>
<p>we propose <span class="arithmatex">\(\zbf_1 \sim q_\phi(\zbf_1|\xbf)\)</span> and <span class="arithmatex">\(\zbf_2 \sim q_\phi(\zbf_2|\ybf,\zbf_1,\xbf)\)</span>, then the unnormalized importance weight is defined as</p>
<div class="arithmatex">\[
w^{m,s} = \frac{q_\phi(\zbf_2, \ybf, \zbf_1|\xbf)}{q_\phi(\zbf_1|\xbf)q_\phi(\zbf_2|\ybf,\zbf_1,\xbf)} = q_\phi(\ybf|\zbf_1,\xbf)
\]</div>
<p>which is, again, the product of variational conditional probabilities of the supervised variables <span class="arithmatex">\(\ybf\)</span>.</p>
<p><img alt="Separation of Latent Variables" class="image-center" src="../../assets/images/DRL-sep-latent.png" style="width: 90%" /></p>
<p><strong>Demonstration of disentanglement on Yale B.</strong> The recognition model (encoder) is defined as <span class="arithmatex">\(q(r, l, i, s | x) = q(r|x)q(l|x)q(i|x)q(s|l, i)\)</span>, with <span class="arithmatex">\(r\)</span>, <span class="arithmatex">\(l\)</span>, <span class="arithmatex">\(i\)</span> and <span class="arithmatex">\(s\)</span> denote reflectance, lighting, identity and shading, respectively. Only <span class="arithmatex">\(l\)</span> and <span class="arithmatex">\(i\)</span> are supervised. The generative model (decoder) is fully factored, i.e. all latents are independent under the prior.</p>
<h3 id="adversarial-training">Adversarial Training<a class="headerlink" href="#adversarial-training" title="Permanent link">&para;</a></h3>
<h4 id="vae-gan">VAE-GAN<a class="headerlink" href="#vae-gan" title="Permanent link">&para;</a></h4>
<p>M. Mathieu and Y. Lecun<sup id="fnref:26"><a class="footnote-ref" href="#fn:26">26</a></sup> proposed to use GAN to disentangle the specified (content) factors of variations <span class="arithmatex">\(\sbf\)</span> and unspecified (style/pose) factors <span class="arithmatex">\(\zbf\)</span>. It is similar to <a href="#aae">AAE</a> except that (1) the autoencoder is KL regularized; (2) the discriminator input is in the observation space.</p>
<p>The generative model is given by</p>
<div class="arithmatex">\[
\zbf \sim p(\zbf) = \Ncal(\mathbf{0}, I) \qquad \xbf \sim p_\theta(\xbf|\zbf, \sbf)
\]</div>
<p>The latent variables are inferred by the encoder</p>
<div class="arithmatex">\[
q_\phi(\zbf | \xbf, \sbf) = \Ncal\Big(\mu_\phi\big(\xbf, s_\phi(\xbf)\big), \diag\big(\sigma^2_\phi\big(\xbf, s_\phi(\xbf)\big)\big)\Big)
\]</div>
<p>Note that</p>
<ul>
<li>This work does not aggregate information from data points of the same class (see <a href="#learning-from-grouped-observations">Learning from Grouped Observations</a>). Instead the authors swap the specified factors <span class="arithmatex">\(\sbf\)</span> of a pair of observations <span class="arithmatex">\((\xbf_1, \xbf_1^\prime)\)</span> from the same class, and use the reconstruction loss to ensure data points in the same class have similar <span class="arithmatex">\(\sbf\)</span>'s.</li>
<li><span class="arithmatex">\(s_\phi\)</span> is a deterministic encoder thus all sources of stochasticity of <span class="arithmatex">\(\sbf\)</span> come from the data distribution. In other words, the conditional likelihood given above can be written as <span class="arithmatex">\(\xbf \sim p_\theta(\xbf|\zbf, s_\phi(\xbf^\prime))\)</span> where <span class="arithmatex">\(\xbf^\prime\)</span> and <span class="arithmatex">\(\xbf\)</span> share the same class label.</li>
</ul>
<p>To prevent the model from ignoring <span class="arithmatex">\(\sbf\)</span> and degenerating to a standard VAE, the authors swap the unspecified components <span class="arithmatex">\(\zbf\)</span> of a pair of observations <span class="arithmatex">\((\xbf_1, \xbf_2)\)</span> or sample from the prior distribution <span class="arithmatex">\(\zbf \sim \Ncal(\mathbf{0}, I)\)</span>, decode them back into the observation space, then ensure the reconstructions are assigned high probabilities of belonging to their original classes by an external discriminator. The adversarial discriminator, instead, aims to correctly classify these generated reconstructions from the true samples from class <span class="arithmatex">\(y\)</span>.</p>
<div class="arithmatex">\[
\min_G \max_D \log D(\xbf|y) + \log (1 - D(G(\zbf, \sbf_y), y))
\]</div>
<p>where <span class="arithmatex">\(\sbf_y = s_\phi(\xbf)\)</span> where <span class="arithmatex">\(\xbf\)</span> has class label <span class="arithmatex">\(y\)</span>.</p>
<h4 id="drnet">DrNet<a class="headerlink" href="#drnet" title="Permanent link">&para;</a></h4>
<p>DrNet<sup id="fnref:25"><a class="footnote-ref" href="#fn:25">25</a></sup> factorizes the video representation into <em>time-varying</em> (pose) and <em>time-independent</em> (content) components.</p>
<p>Let <span class="arithmatex">\(\xbf_i = (\xbf_i^1, \dots, \xbf_i^T)\)</span> denote a sequence of <span class="arithmatex">\(T\)</span> images from video <span class="arithmatex">\(i\)</span>. The DrNet model has four components: a pose encoder <span class="arithmatex">\(E_p\)</span>, a content encoder <span class="arithmatex">\(E_c\)</span>, a future frame decoder <span class="arithmatex">\(G\)</span>, and a scene discriminator <span class="arithmatex">\(D\)</span>.</p>
<p>The loss function is a weighted sum of the following terms</p>
<ul>
<li>
<p>Reconstruction loss: standard per-pixel <span class="arithmatex">\(\ell_2\)</span> loss between the decoder-predicted future frame <span class="arithmatex">\(\widetilde{\xbf}^{t+k}\)</span> and the actual frame <span class="arithmatex">\(\xbf^{t+k}\)</span> for some random frame offset <span class="arithmatex">\(k \in [0, K]\)</span>.</p>
<div class="arithmatex">\[
\min_{E_c, E_p, G} \left\Vert G\big( E_c(\xbf^t), E_p(\xbf^{t+k}) \big) - \xbf^{t+k} \right\Vert_2^2
\]</div>
</li>
<li>
<p>Similarity Loss: to ensure the content encoder extracts time-invariant representations, content representations across time frames should be similar to each other.</p>
<div class="arithmatex">\[
\min_{E_c} \left\Vert E_c(\xbf^t) - E_c(\xbf^{t+k}) \right\Vert_2^2
\]</div>
</li>
<li>
<p>Adversarial Loss: Exploiting the fact that the objects present do not typically change within a video but do between different videos, the scene discriminator <span class="arithmatex">\(D\)</span> attempts to determine if a pose feature pair <span class="arithmatex">\((E_p(\xbf_i^t), E_p(\xbf_j^{t+k}))\)</span> come from the same video, i.e. if <span class="arithmatex">\(i = j\)</span>. The pose encoder, which is not supposed to carry any content information, tries to maximize the uncertainty of the discriminator output (targeting 1/2) on pairs of frames from <em>the same</em> clip.</p>
<div class="arithmatex">\[\begin{align}
\min_D &amp; \Lcal_{\mathrm{BCE}}\Big(\mathbb{1}_{i=j}, D\big( E_p(\xbf_i^t), E_p(\xbf_j^{t+k}) \big)\Big) \\
&amp;= -\log D \big( E_p(\xbf_i^t), E_p(\xbf_i^{t+k}) \big) - \log \Big(1 - D\big( E_p(\xbf_i^t), E_p(\xbf_j^{t+k}) \big) \Big) \\
\min_{E_p} &amp; \Lcal_{\mathrm{BCE}}\Big(\frac{1}{2},  D\big( E_p(\xbf_i^t), E_p(\xbf_i^{t+k}) \big)\Big) \\
&amp;= -\frac{1}{2}\log D \big( E_p(\xbf_i^t), E_p(\xbf_i^{t+k}) \big) - \frac{1}{2}\log \Big(1 - D\big( E_p(\xbf_i^t), E_p(\xbf_i^{t+k}) \big) \Big)
\end{align}\]</div>
</li>
</ul>
<h3 id="latent-optimization">Latent Optimization<a class="headerlink" href="#latent-optimization" title="Permanent link">&para;</a></h3>
<h4 id="lord">LORD<a class="headerlink" href="#lord" title="Permanent link">&para;</a></h4>
<p>Gabbay et al.<sup id="fnref:20"><a class="footnote-ref" href="#fn:20">20</a></sup> propose that Latent Optimization for Representation Disentanglement (LORD) is superior to amortized inference for achieving disentangled representations.</p>
<p><strong>Latent Optimization.</strong> The generative model is defined as (we use the notations of the LORD paper to align with the image below)</p>
<div class="arithmatex">\[
x_i = G_\theta(e_{y_i}, c_i)
\]</div>
<p>where the class embedding <span class="arithmatex">\(e_y\)</span> is shared between all images of the same class, and the content embedding <span class="arithmatex">\(c\)</span> is different for every data point <span class="arithmatex">\(x\)</span>.
The benefit of this model is as follows</p>
<ul>
<li>As the class embedding is shared exactly between all images of the same class, it is impossible to include any content information in the class embedding.</li>
<li>As we learn per-class representations directly rather than using previous techniques as group averaging, each mini-batch can contain images randomly sampled from all classes allowing maximal diversity (which cannot be achieved by <a href="#ml-vae">ML-VAE</a>).</li>
</ul>
<p><strong>Asymmetric Noise Regularization.</strong> To ensure that class information does not leak into the content representation, LORD regularizes the content code to enforce minimality of information with an additive Gaussian noise of a fixed variance, and an activation decay penalty, resulting in the following objective</p>
<div class="arithmatex">\[
\Lcal = \sum_{i=1}^n \Lcal_{\mathrm{VGG}} (G_\theta(e_{y_i}, c_i + z_i), x_i) + \lambda \Vert c_i \Vert^2, \quad z_i \sim \Ncal(0, \sigma^2I)
\]</div>
<p>where the first term is the VGG perceptual loss<sup id="fnref2:21"><a class="footnote-ref" href="#fn:21">21</a></sup>.</p>
<p><img alt="LORD" class="image-center" src="../../assets/images/DRL-LORD.png" style="width: 70%" /></p>
<p><strong>Amortization for One-Shot Inference.</strong> At test time, a data point from an unknown class is observed. Optimizing over the latent representations for a single data points leads to overfitting which results in entangled representations. Moreover, it requires iterative test-time inference which is time-consuming. LORD proposes to learn a class encoder <span class="arithmatex">\(E_y\)</span> and a content encoder <span class="arithmatex">\(E_c\)</span> after stage-one training (described above) to enable efficient test-time inference.</p>
<div class="arithmatex">\[
\Lcal = \sum_{i=1}^n \Lcal_{\mathrm{VGG}} (G_\theta(E_y(x_i), E_c(x_i)), x_i) + \alpha_e \Vert E_y(x_i) - e_{y_i} \Vert^2  + \alpha_c \Vert E_c(x_i) - c_{i} \Vert^2
\]</div>
<p>The stage-two objective is a weighted sum of the reconstruction loss and the latent MSE loss.</p>
<p><img alt="LORD-demo" class="image-center" src="../../assets/images/DRL-LORD-demo.png" style="width: 85%" /></p>
<h2 id="learning-from-grouped-observations">Learning from Grouped Observations<a class="headerlink" href="#learning-from-grouped-observations" title="Permanent link">&para;</a></h2>
<h3 id="ml-vae">ML-VAE<a class="headerlink" href="#ml-vae" title="Permanent link">&para;</a></h3>
<p>Multi-Layer VAE (ML-VAE)<sup id="fnref:18"><a class="footnote-ref" href="#fn:18">18</a></sup> focuses on how to perform amortized inference in the context of non-i.i.d., grouped observations. The authors assume disjoint groups, where within each group, a factor of variation (or <em>content</em>) is shared among all observations, and each observation has an independent representation for its <em>style</em>.</p>
<p>Traditional stochastic variational inference (SVI) would easily extend to this setting, but suffers from expensive test-time inference since it does not perform amortized inference for observations or groups. VAE performs amortized inference for observations, but assumes i.i.d. observations and fails to incorporate the grouping information. ML-VAE attemps to extend the VAE framework to perform amortized inference on groups of observations.</p>
<p><img alt="ML-VAE" class="image-center" src="../../assets/images/DRL-ML-VAE.png" style="width: 90%" /></p>
<p>First we assume independence between grouped observations</p>
<div class="arithmatex">\[\log p(\Xbf|\theta) = \sum_{G \in \Gcal} \log p(\Xbf_G|\theta)\]</div>
<p>For each group, we derive the ELBO for group <span class="arithmatex">\(G\)</span> and maximize the sum of ELBOs over all groups.</p>
<div class="arithmatex">\[
\begin{align}
\ELBO(G; \theta, \phi_s, \phi_c) =&amp; \E_{q_{\phi_c}(C_G|\Xbf)} \sum_{i \in G}  \E_{q_{\phi_s}(S_i|X_i)} \log p_\theta(X_i | C_G, S_i) - \\
&amp;\sum_{i \in G} \KL(q_{\phi_s}(S_i\Vert X_i)) -\KL(q_{\phi_c}(C_G|\Xbf_G) \Vert p(C_G))
\end{align}
\]</div>
<p><strong>Accumulating Evidence with Product of Gaussians.</strong> How do we infer the group content <span class="arithmatex">\(q_{\phi_c}(C_G|\Xbf_G)\)</span>? The authors choose the product of Gaussian distribution</p>
<div class="arithmatex">\[
q_{\phi_c}(C_G = c | \Xbf_G = \xbf_G) \propto \prod_{i\in G} q_{\phi_c}(C_G = c | X_i = x_i)
\]</div>
<p>since the product of Gaussians is still a Gaussian.</p>
<div class="arithmatex">\[
\Sigma_G^{-1} = \sum_{i\in G} \Sigma_i^{-1} \qquad \mu_G^\top \Sigma_G^{-1} = \sum_{i \in G} \mu_i^\top \Sigma_i^{-1}
\]</div>
<p>Note that by increasing the number of observations in a group, the variance of the resulting distribution decreases. In other words, we accumulate evidence within a group to infer an accurate group content representation.</p>
<p><img alt="ML-VAE-demo" class="image-center" src="../../assets/images/DRL-ML-VAE-demo.png" style="width: 90%" /></p>
<h3 id="gvae">GVAE<a class="headerlink" href="#gvae" title="Permanent link">&para;</a></h3>
<p>Grouped VAE (GVAE)<sup id="fnref:23"><a class="footnote-ref" href="#fn:23">23</a></sup> is very similar to <a href="#ml-vae">ML-VAE</a>. The only difference lies in how they calculate the group content representation. Instead of  accumulating evidence with product of Gaussians, GVAE averages the mean and variance of the Gaussians inferred from individual samples within a group.</p>
<div class="arithmatex">\[
q_{\phi_c}(C_G = c | \Xbf_G = \xbf_G) = \Ncal \left( \frac{1}{|\Gcal|}\sum_{k=1}^{|\Gcal|} \mu_\phi(\xbf_k), \frac{1}{|\Gcal|}\sum_{k=1}^{|\Gcal|} \diag(\sigma_\phi(\xbf_k)) \right)
\]</div>
<p>The authors compared GVAE with ML-VAE and argues that ML-VAE has style-dependent coding of content, and thus has generally larger number of effective content dimensions<sup id="fnref2:23"><a class="footnote-ref" href="#fn:23">23</a></sup>.</p>
<h3 id="f-statistics">F-Statistics<a class="headerlink" href="#f-statistics" title="Permanent link">&para;</a></h3>
<h2 id="learning-from-paired-observations">Learning from Paired Observations<a class="headerlink" href="#learning-from-paired-observations" title="Permanent link">&para;</a></h2>
<h3 id="ada-mlvae-and-ada-gvae">Ada-MLVAE and Ada-GVAE<a class="headerlink" href="#ada-mlvae-and-ada-gvae" title="Permanent link">&para;</a></h3>
<p>Adaptive MLVAE/GVAE<sup id="fnref:24"><a class="footnote-ref" href="#fn:24">24</a></sup> considers learning disentangled representations with even weaker supervision. They try to learn from pairs of non-i.i.d. images sharing at least one of the underlying factors of variation. In their paper<sup id="fnref2:24"><a class="footnote-ref" href="#fn:24">24</a></sup>, they first theoretically show that only knowing how many factors have changed, but not which ones, is sufficient to learn disentangled representations.</p>
<p>Assuming the two observations <span class="arithmatex">\(\xbf_1\)</span> and <span class="arithmatex">\(\xbf_2\)</span> differ in <span class="arithmatex">\(F\)</span> number of factors which form a set <span class="arithmatex">\(\bar{S} = [D] - S\)</span>, the generative model is given by:</p>
<div class="arithmatex">\[
\begin{align}
&amp;p(\zbf) = \prod_{i=1}^D p(\zbf_i) \qquad p(\widetilde{\zbf}) = \prod_{i=1}^F p(\widetilde{z}_i) \qquad S \sim p(S) \\
&amp;\xbf_1 = g^\star(\zbf) \qquad \xbf_2 = g^\star(f(\zbf, \widetilde{\zbf}, S))
\end{align}
\]</div>
<p>where <span class="arithmatex">\(g^\star: \Zcal \to \Xcal\)</span> is a mapping from the latent space to the observation space, and <span class="arithmatex">\(f\)</span> is a function which selects entries from <span class="arithmatex">\(\zbf\)</span> with index in <span class="arithmatex">\(S\)</span> and substitutes the remaining factors with <span class="arithmatex">\(\widetilde{\zbf}\)</span>.</p>
<p>Note that the alignment constraints imposed by the generative model imply that for the true posterior, with probability 1,</p>
<div class="arithmatex">\[
p(z_i|\xbf_1) = p(z_i|\xbf_2), \forall i \in S \qquad p(z_i|\xbf_1) \ne p(z_i|\xbf_2), \forall i \in \bar{S}
\]</div>
<p>We enforce these constraints on the approximate posterior <span class="arithmatex">\(q_\phi(\widehat{\zbf}|\xbf)\)</span> of our learned model with the following steps:</p>
<ul>
<li>
<p>Estimate <span class="arithmatex">\(S\)</span>. Choose for every pair <span class="arithmatex">\((\xbf_1, \xbf_2)\)</span> the <span class="arithmatex">\(D-F\)</span> factors with the smallest KL.</p>
<div class="arithmatex">\[\delta_i :=\KL( q_\phi(\widehat{z}_i|\xbf_1) \Vert q_\phi(\widehat{z}_i|\xbf_2) )\]</div>
<p>In the (practical) scenario where <span class="arithmatex">\(F\)</span> is unknown, we use the threshold</p>
<div class="arithmatex">\[\tau = \frac{1}{2} (\max_i \delta_i + \min_i \delta_i)\]</div>
<p>and choose <span class="arithmatex">\(F\)</span> as the number of coordinates <span class="arithmatex">\(i\)</span>'s satisfying <span class="arithmatex">\(d_i &lt; \tau\)</span>.</p>
</li>
<li>
<p>Replace each shared coordinate with some aggregation <span class="arithmatex">\(a\)</span> of the two posteriors (as in <a href="#gvae">GVAE</a> or <a href="#ml-vae">ML-VAE</a>)</p>
<div class="arithmatex">\[
\widetilde{q}_\phi(\widehat{z}_i|\xbf_1) = \widetilde{q}_\phi(\widehat{z}_i|\xbf_2) = a(q_\phi(\widehat{z}_i|\xbf_1), q_\phi(\widehat{z}_i|\xbf_2)), \forall i \in \widehat{S}
\]</div>
</li>
<li>
<p>Optimize the <span class="arithmatex">\(\beta\)</span>-VAE objective</p>
<div class="arithmatex">\[
\max_{\theta, \phi} \E_{(\xbf_1, \xbf_2)} \sum_{j=1}^2 \left[
    \E_{\widetilde{q}_\phi(\widehat{\zbf}|\xbf_j)} \log p_\theta(\xbf_j | \widehat{\zbf}) - \beta \KL( \widetilde{q}_\phi(\widehat{\zbf}|\xbf_j) \Vert p(\widehat{\zbf}) )
\right]
\]</div>
</li>
</ul>
<p>The aggregation imposes a hard constraint which forces the shared dimensions to encode only one value, and thus implicitly enforces the non-shared dimensions to efficiently encode all the non-shared factors.</p>
<ul>
<li>CVPR 2021, Learning View-Disentangled Human Pose Representation by Contrastive Cross-View Mutual Information Maximization</li>
</ul>
<div class="footnote">
<hr />
<ol>
<li id="fn:1">
<p>A. Achille; S. Soatto, Emergence of Invariance and Disentanglement in Deep Representations, JMLR 2018. <a href="https://arxiv.org/pdf/1706.01350.pdf">paper</a> <a href="https://www.youtube.com/watch?v=zbg49SMP5kY">talk</a>&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:2">
<p>F. Locatello et al., Challenging Common Assumptions in the Unsupervised Learning of Disentangled Representations, ICLR 2019. <a href="https://arxiv.org/abs/1811.12359">paper</a> <a href="https://ai.googleblog.com/2019/04/evaluating-unsupervised-learning-of.html">blog</a> <a href="https://github.com/google-research/disentanglement_lib">code</a>&#160;<a class="footnote-backref" href="#fnref:2" title="Jump back to footnote 2 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:2" title="Jump back to footnote 2 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:2" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
<li id="fn:3">
<p>Y. Bengio; A. Courville; P. Vincent, <a href="https://arxiv.org/abs/1206.5538">Representation Learning: A Review and New Perspectives</a>, IEEE-PAMI 2013.&#160;<a class="footnote-backref" href="#fnref:3" title="Jump back to footnote 3 in the text">&#8617;</a></p>
</li>
<li id="fn:4">
<p>I. Higgins et al., <a href="https://openreview.net/pdf?id=Sy2fzU9gl">Î²-VAE - Learning Basic Visual Concepts with a Constrained Variational Framework</a>, ICLR 2017.&#160;<a class="footnote-backref" href="#fnref:4" title="Jump back to footnote 4 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:4" title="Jump back to footnote 4 in the text">&#8617;</a></p>
</li>
<li id="fn:5">
<p>H. Kim; A. Mnih, <a href="https://arxiv.org/abs/1802.05983">Disentangling by Factorising</a>, NeurIPS 2017.&#160;<a class="footnote-backref" href="#fnref:5" title="Jump back to footnote 5 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:5" title="Jump back to footnote 5 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:5" title="Jump back to footnote 5 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:5" title="Jump back to footnote 5 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:5" title="Jump back to footnote 5 in the text">&#8617;</a></p>
</li>
<li id="fn:6">
<p>T. Q. Chen et al., <a href="https://arxiv.org/abs/1802.04942">Isolating Sources of Disentanglement in VAEs</a>, NeurIPS 2018.&#160;<a class="footnote-backref" href="#fnref:6" title="Jump back to footnote 6 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:6" title="Jump back to footnote 6 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:6" title="Jump back to footnote 6 in the text">&#8617;</a></p>
</li>
<li id="fn:7">
<p>K. Ridgeway; M. C. Mozer, <a href="https://papers.nips.cc/paper/7303-learning-deep-disentangled-embeddings-with-the-f-statistic-loss.pdf">Learning Deep Disentangled Embeddings With the F-Statistic Loss</a>, NeurIPS 2018.&#160;<a class="footnote-backref" href="#fnref:7" title="Jump back to footnote 7 in the text">&#8617;</a></p>
</li>
<li id="fn:8">
<p>Y. Bengio, <a href="https://www.youtube.com/watch?v=Yr1mOzC93xs">From Deep Learning of Disentangled Representations to Higher-level Cognition</a>, MSR AI Distinguished Lectures and Fireside Chats, 2018.&#160;<a class="footnote-backref" href="#fnref:8" title="Jump back to footnote 8 in the text">&#8617;</a></p>
</li>
<li id="fn:9">
<p>A. Kumar; P. Sattigeri; A. Balakrishnan, <a href="https://arxiv.org/abs/1802.05983">Variational Inference of Disentangled Latent Concepts from Unlabeled Observations</a>, ICLR 2018.&#160;<a class="footnote-backref" href="#fnref:9" title="Jump back to footnote 9 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:9" title="Jump back to footnote 9 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:9" title="Jump back to footnote 9 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:9" title="Jump back to footnote 9 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:9" title="Jump back to footnote 9 in the text">&#8617;</a></p>
</li>
<li id="fn:10">
<p>C. Eastwood; C. K. I. Williams, <a href="https://openreview.net/pdf?id=By-7dz-AZ">A Framework for the Quantitative Evaluation of Disentangled Representations</a>, ICLR 2018.&#160;<a class="footnote-backref" href="#fnref:10" title="Jump back to footnote 10 in the text">&#8617;</a></p>
</li>
<li id="fn:11">
<p>C. P. Burgess et al., <a href="https://arxiv.org/abs/1804.03599">Understanding disentangling in beta-vae</a>, Workshop on Learning Disentangled Representations, NeurIPS 2017.&#160;<a class="footnote-backref" href="#fnref:11" title="Jump back to footnote 11 in the text">&#8617;</a></p>
</li>
<li id="fn:12">
<p>A. Makhzani et al., <a href="https://arxiv.org/abs/1511.05644">Adversarial Autoencoders</a>, NeurIPS 2015.&#160;<a class="footnote-backref" href="#fnref:12" title="Jump back to footnote 12 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:12" title="Jump back to footnote 12 in the text">&#8617;</a></p>
</li>
<li id="fn:13">
<p>S. Zhao; J. Song; S. Ermon, <a href="https://www.aaai.org/ojs/index.php/AAAI/article/view/4538">InfoVAE: Balancing Learning and Inference in Variational Autoencoders</a>, AAAI 2019.&#160;<a class="footnote-backref" href="#fnref:13" title="Jump back to footnote 13 in the text">&#8617;</a></p>
</li>
<li id="fn:14">
<p>Q. Liu; D. Wang, <a href="https://arxiv.org/abs/1608.04471">Stein Variational Gradient Descent: A General Purpose Bayesian Inference Algorithm</a>, NeurIPS 2016.&#160;<a class="footnote-backref" href="#fnref:14" title="Jump back to footnote 14 in the text">&#8617;</a></p>
</li>
<li id="fn:15">
<p>X. Chen et al., <a href="https://arxiv.org/abs/1606.03657">InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets</a>, NeurIPS 2016.&#160;<a class="footnote-backref" href="#fnref:15" title="Jump back to footnote 15 in the text">&#8617;</a></p>
</li>
<li id="fn:16">
<p>Z. Lin et al., <a href="https://arxiv.org/abs/1906.06034">InfoGAN-CR and ModelCentrality: Self-supervised Model Training and Selection for Disentangling GANs</a>, ICML 2020.&#160;<a class="footnote-backref" href="#fnref:16" title="Jump back to footnote 16 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:16" title="Jump back to footnote 16 in the text">&#8617;</a></p>
</li>
<li id="fn:17">
<p>N. Siddharth et al., <a href="https://arxiv.org/abs/1706.00400">Learning Disentangled Representations with Semi-Supervised Deep Generative Models</a>, NeurIPS 2017.&#160;<a class="footnote-backref" href="#fnref:17" title="Jump back to footnote 17 in the text">&#8617;</a></p>
</li>
<li id="fn:18">
<p>D. Bouchacourt; R. Tomioka; S. Nowozin, <a href="https://arxiv.org/abs/1705.08841">Multi-Level Variational Autoencoder: Learning Disentangled Representations from Grouped Observations</a>, AAAI 2018.&#160;<a class="footnote-backref" href="#fnref:18" title="Jump back to footnote 18 in the text">&#8617;</a></p>
</li>
<li id="fn:19">
<p>D. P. Kingma; D. J. Rezende; S. Mohamed; M. Welling. <a href="https://arxiv.org/pdf/1406.5298.pdf">Semisupervised learning with deep generative models</a>, NeurIPS 2014.&#160;<a class="footnote-backref" href="#fnref:19" title="Jump back to footnote 19 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:19" title="Jump back to footnote 19 in the text">&#8617;</a></p>
</li>
<li id="fn:20">
<p>A. Gabbay; Y. Hoshen, <a href="https://iclr.cc/virtua\ell_2020/poster_Hyl9xxHYPr.html">Demystifying Inter-Class Disentanglement</a>, ICLR 2020.&#160;<a class="footnote-backref" href="#fnref:20" title="Jump back to footnote 20 in the text">&#8617;</a></p>
</li>
<li id="fn:21">
<p>Y. Hoshen; J. Malik, <a href="https://arxiv.org/abs/1812.08985">Non-Adversarial Image Synthesis with Generative Latent Nearest Neighbors</a>, CVPR 2019.&#160;<a class="footnote-backref" href="#fnref:21" title="Jump back to footnote 21 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:21" title="Jump back to footnote 21 in the text">&#8617;</a></p>
</li>
<li id="fn:22">
<p>F. Locatello et al, <a href="https://arxiv.org/abs/1905.01258">Disentangling Factors of Variation Using Few Labels</a>, ICLR 2020.&#160;<a class="footnote-backref" href="#fnref:22" title="Jump back to footnote 22 in the text">&#8617;</a></p>
</li>
<li id="fn:23">
<p>H. Hosoya, <a href="https://arxiv.org/abs/1809.02383">Group-based Learning of Disentangled Representations with Generalizability for Novel Contents</a>, IJCAI 2019.&#160;<a class="footnote-backref" href="#fnref:23" title="Jump back to footnote 23 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:23" title="Jump back to footnote 23 in the text">&#8617;</a></p>
</li>
<li id="fn:24">
<p>F. Locatello et al., <a href="https://arxiv.org/abs/2002.02886">Weakly-Supervised Disentanglement Without Compromises</a>, PMLR 2020.&#160;<a class="footnote-backref" href="#fnref:24" title="Jump back to footnote 24 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:24" title="Jump back to footnote 24 in the text">&#8617;</a></p>
</li>
<li id="fn:25">
<p>E. Denton; V. Birodkar, <a href="https://arxiv.org/abs/1705.10915">Unsupervised Learning of Disentangled Representations from Video</a>, NeurIPS 2017.&#160;<a class="footnote-backref" href="#fnref:25" title="Jump back to footnote 25 in the text">&#8617;</a></p>
</li>
<li id="fn:26">
<p>M. Mathieu et al., <a href="https://arxiv.org/abs/1611.03383">Disentangling factors of variation in deep representations using adversarial training</a>, NeurIPS 2016.&#160;<a class="footnote-backref" href="#fnref:26" title="Jump back to footnote 26 in the text">&#8617;</a></p>
</li>
</ol>
</div>







  
  






                
  <h2 id="__comments">Comments</h2>
  <script src="https://giscus.app/client.js"
        data-repo="hui2000ji/hui2000ji.github.io"
        data-repo-id="MDEwOlJlcG9zaXRvcnkzMzc2ODAwMzQ="
        data-category-id="DIC_kwDOFCCWos4CBIqC"
        data-mapping="pathname"
        data-reactions-enabled="1"
        data-emit-metadata="0"
        data-input-position="top"
        data-theme="light"
        data-lang="en"
        crossorigin="anonymous"
        async>
  </script>
  <!-- Reload on palette change -->
  <script>
    var palette = __md_get("__palette")
    if (palette && typeof palette.color === "object")
      if (palette.color.scheme === "slate") {
        var giscus = document.querySelector("script[src*=giscus]")
        giscus.setAttribute("data-theme", "dark") 
      }

    /* Register event handlers after documented loaded */
    document.addEventListener("DOMContentLoaded", function() {
      var ref = document.querySelector("[data-md-component=palette]")
      ref.addEventListener("change", function() {
        var palette = __md_get("__palette")
        if (palette && typeof palette.color === "object") {
          var theme = palette.color.scheme === "slate" ? "dark" : "light"

          /* Instruct Giscus to change theme */
          var frame = document.querySelector(".giscus-frame")
          frame.contentWindow.postMessage(
            { giscus: { setConfig: { theme } } },
            "https://giscus.app"
          )
        }
      })
    })
  </script>

              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg>
  Back to top
</button>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
        <div class="md-social">
  
    
    
    
    
    <a href="mailto:huiyu.cai@umontreal.ca" target="_blank" rel="noopener" title="" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M48 64C21.5 64 0 85.5 0 112c0 15.1 7.1 29.3 19.2 38.4l217.6 163.2c11.4 8.5 27 8.5 38.4 0l217.6-163.2c12.1-9.1 19.2-23.3 19.2-38.4 0-26.5-21.5-48-48-48zM0 176v208c0 35.3 28.7 64 64 64h384c35.3 0 64-28.7 64-64V176L294.4 339.2a63.9 63.9 0 0 1-76.8 0z"/></svg>
    </a>
  
    
    
    
    
      
      
    
    <a href="https://twitter.com/hui2000ji" target="_blank" rel="noopener" title="twitter.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M459.37 151.716c.325 4.548.325 9.097.325 13.645 0 138.72-105.583 298.558-298.558 298.558-59.452 0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055 0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421 0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391 0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04 0-57.828 46.782-104.934 104.934-104.934 30.213 0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253"/></svg>
    </a>
  
    
    
    
    
      
      
    
    <a href="https://github.com/hui2000ji" target="_blank" rel="noopener" title="github.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8M97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg>
    </a>
  
    
    
    
    
      
      
    
    <a href="https://scholar.google.com/citations?user=ZQ2VZ0sAAAAJ" target="_blank" rel="noopener" title="scholar.google.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M219.3.5c3.1-.6 6.3-.6 9.4 0l200 40C439.9 42.7 448 52.6 448 64s-8.1 21.3-19.3 23.5L352 102.9V160c0 70.7-57.3 128-128 128S96 230.7 96 160v-57.1l-48-9.6v65.1l15.7 78.4c.9 4.7-.3 9.6-3.3 13.3S52.8 256 48 256H16c-4.8 0-9.3-2.1-12.4-5.9s-4.3-8.6-3.3-13.3L16 158.4V86.6C6.5 83.3 0 74.3 0 64c0-11.4 8.1-21.3 19.3-23.5zM111.9 327.7c10.5-3.4 21.8.4 29.4 8.5l71 75.5c6.3 6.7 17 6.7 23.3 0l71-75.5c7.6-8.1 18.9-11.9 29.4-8.5 65 20.9 112 81.7 112 153.6 0 17-13.8 30.7-30.7 30.7H30.7C13.8 512 0 498.2 0 481.3c0-71.9 47-132.7 111.9-153.6"/></svg>
    </a>
  
    
    
    
    
      
      
    
    <a href="https://orcid.org/0000-0001-8506-537X" target="_blank" rel="noopener" title="orcid.org" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M294.75 188.19h-45.92V342h47.47c67.62 0 83.12-51.34 83.12-76.91 0-41.64-26.54-76.9-84.67-76.9M256 8C119 8 8 119 8 256s111 248 248 248 248-111 248-248S393 8 256 8m-80.79 360.76h-29.84v-207.5h29.84zm-14.92-231.14a19.57 19.57 0 1 1 19.57-19.57 19.64 19.64 0 0 1-19.57 19.57M300 369h-81V161.26h80.6c76.73 0 110.44 54.83 110.44 103.85C410 318.39 368.38 369 300 369"/></svg>
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    <script id="__config" type="application/json">{"base": "../..", "features": ["navigation.tabs", "navigation.top", "search.highlight"], "search": "../../assets/javascripts/workers/search.6ce7567c.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    
      <script src="../../assets/javascripts/bundle.56dfad97.min.js"></script>
      
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.6.0/highlight.min.js"></script>
      
        <script src="../../assets/js/config.js"></script>
      
    
  </body>
</html>