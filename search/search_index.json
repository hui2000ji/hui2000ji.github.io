{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Huiyu CAI \u8521\u8f89\u5b87","text":"<p>I am a third-year PhD candidate at Mila - Quebec AI Institute and Department of Computer Science and Operations Research, University of Montreal, supervised by Prof. Jian Tang. I graduated from the Department of Machine Intelligence, Peking University as an Excellent Graduate in Beijing. I was previously a member of the Language Computing and Web Mining Group, led by Prof. Xiaojun Wan.</p> <p>I am interested in deep generative models, geometric deep learning and their wide applications, such as biomolecular interaction modeling, drug discovery, single-cell data analysis, etc.</p> <p>Feel free to reach out (links at webpage footer)!</p>"},{"location":"#publications","title":"Publications","text":""},{"location":"#2024","title":"2024","text":"<ul> <li>GearBind: Pretrainable geometric graph neural network for antibody affinity maturationNature Communications, 2024Huiyu Cai*, Zuobai Zhang*, Mingkai Wang*, Bozitao Zhong*, Quanxiao Li, Yuxuan Zhong, Yanling Wu, Tianlei Ying, Jian Tang *Equal contribution</li> </ul>"},{"location":"#2023","title":"2023","text":"<ul> <li>E3Bind: An End-to-End Equivariant Network for Protein-Ligand DockingICLR, 2023Yangtian Zhang*, Huiyu Cai*, Chence Shi, Jian Tang *Equal contribution</li> </ul>"},{"location":"#2022","title":"2022","text":"<ul> <li>Neural Structured Prediction for Inductive Node ClassificationICLR, 2022 (oral)Meng Qu, Huiyu Cai, Jian Tang *Equal contribution <ul> <li>Structured Multi-task Learning for Molecular Property PredictionAISTATS, 2022Shengchao Liu, Meng Qu, Zuobai Zhang, Huiyu Cai, Jian Tang</li> </ul>"},{"location":"#2021-and-before","title":"2021 and before","text":"<ul> <li>Learning interpretable cellular and gene signature embeddings from single-cell transcriptomic dataNature Communications, 2021 \u2003codeYifan Zhao*, Huiyu Cai*, Zuobai Zhang, Jian Tang, Yue Li *Equal contribution</li> </ul> <ul> <li>Multi-Modal Sarcasm Detection in Twitter with Hierarchical Fusion Model ACL, 2019 \u2003dataYitao Cai, Huiyu Cai, Xiaojun Wan</li> </ul>"},{"location":"#projects","title":"Projects","text":"<ul> <li>TorchDrug: A powerful and flexible machine learning platform for drug discovery (Nov. 2021 - now)</li> <li>Implementation of the EWLS Algorithm for the Maximum Clique Problem (Dec. 2020 - Jan. 2021)</li> <li>Music Source Separation: Theory and Applications (Apr. 2020 - Jun. 2020)</li> <li>Raiden Game Implementation in Java (Jan. 2020 - Jun. 2020)</li> <li>Fine-grained Face Manipulation via DLGAN (Oct. 2019 - Jan. 2020)</li> <li>Bird Sound Classification with CNN (Mar. 2019 - Jun. 2019)</li> <li>Mahjong AI Based on Deep Supervised Learning (Mar. 2019 - Jun. 2019)</li> <li>Rule-based Doudizhu &amp; Ataxx &amp; Reversi &amp; pysc2-minimap AI (Oct. 2017 - May. 2019)</li> </ul>"},{"location":"machine_learning/","title":"Machine Learning","text":"<p>Here are my posts about machine learning.</p> <p>Click on the navigation entries on the left to navigate between posts.</p>"},{"location":"machine_learning/disentangled_representation_learning/","title":"Disentangled Representation Learning","text":"<p>General notations: \\(x\\) denotes data points (generated by \\(K\\) factors), \\(y\\) denotes labels, \\(z\\) denotes \\(D\\)-dimensional representation.</p>"},{"location":"machine_learning/disentangled_representation_learning/#what-makes-a-good-representation","title":"What makes a good representation?","text":"<p>Adapted from S. Soatto's IPAM talk, 2018<sup>1</sup>.</p> <ul> <li>Sufficiency: \\(I(y;z) = I(y;x)\\)</li> <li>Invariance: \\(n \\perp y \\rightarrow I(n;z) = 0\\), where \\(n\\) denotes nuisance factors.</li> <li>Minimality: \\(\\min I(x; z)\\)</li> <li>Disentangling: \\(\\mathrm{TC}(z): \\mathcal{D}_{\\mathrm{KL}}(p(z) \\Vert \\prod_i p(z_i))\\)</li> </ul> <p>Invariance \\(\\Leftrightarrow\\) Minimality \\(|\\) Sufficiency \u2003 If \\(z\\) is sufficient, \\(n\\) is a nuisance, then</p> \\[ \\DeclareMathOperator{\\Dcal}{\\mathcal{D}} \\DeclareMathOperator{\\Dsup}{\\Dcal^{\\mathrm{sup}}} \\DeclareMathOperator{\\Lcal}{\\mathcal{L}} \\DeclareMathOperator{\\Bcal}{\\mathcal{B}} \\DeclareMathOperator{\\Ncal}{\\mathcal{N}} \\DeclareMathOperator{\\Gcal}{\\mathcal{G}} \\DeclareMathOperator{\\Xcal}{\\mathcal{X}} \\DeclareMathOperator{\\Zcal}{\\mathcal{Z}} \\DeclareMathOperator{\\ELBO}{\\mathrm{ELBO}} \\DeclareMathOperator{\\E}{\\mathbb{E}} \\DeclareMathOperator{\\R}{\\mathbb{R}} \\DeclareMathOperator{\\Cov}{\\mathrm{Cov}} \\DeclareMathOperator{\\Var}{\\mathrm{Var}} \\DeclareMathOperator{\\dom}{\\mathrm{dom}} \\DeclareMathOperator{\\sign}{\\mathrm{sign}} \\DeclareMathOperator{\\diag}{\\mathrm{diag}} \\DeclareMathOperator{\\argmax}{\\mathrm{argmax}} \\DeclareMathOperator{\\KL}{\\Dcal_{\\mathrm{KL}}} \\DeclareMathOperator{\\MMD}{\\Dcal_{\\mathrm{MMD}}} \\DeclareMathOperator{\\JS}{\\Dcal_{\\mathrm{JS}}} \\DeclareMathOperator{\\stopgradient}{\\mathrm{StopGradient}} \\def\\dd{\\mathrm{d}} \\def\\ee{\\mathrm{e}} \\def\\xbf{\\mathbf{x}} \\def\\Xbf{\\mathbf{X}} \\def\\ybf{\\mathbf{y}} \\def\\sbf{\\mathbf{s}} \\def\\wbf{\\mathbf{w}} \\def\\vbf{\\mathbf{v}} \\def\\zbf{\\mathbf{z}} \\def\\cbf{\\mathbf{c}} \\def\\dd{\\mathrm{d}} I(z;n) \\le I(z;x) - I(x;y) \\] <p>and there exists a nuisance for which equality holds.</p>"},{"location":"machine_learning/disentangled_representation_learning/#merits-of-disentangled-representation","title":"Merits of Disentangled Representation","text":"<p>While there is no widely-accepted formalized notion of disentanglement (yet), the key intuition is that a disentangled representation should separate the distinct, informative factors of variations in the data<sup>3</sup>.</p> <p>Disentangled representations offer several advantages<sup>9</sup>:</p> <ul> <li>Invariance: it is easier to derive representations that are invariant to nuisance factors by simply marginalizing over the corresponding dimensions</li> <li>Transferability: they are arguably more suitable for transfer learning as most of the key underlying generative factors appear segregated along feature dimensions</li> <li>Interpretability: a human expert may be able to assign meanings to the dimensions</li> <li>Conditioning and intervention: they allow for interpretable conditioning and/or intervention over a subset of the latents and observe the effects on other nodes in the graph.</li> </ul> <p>Current deep learning common practices are generally not disentangled, for instance, state-of-the-art deep learning models trained on speech data failed to capture basic conceps such as distributions on phonemes<sup>8</sup> (which could be discovered by a simple unsupervised clustering algorithm such as \\(k\\)-means), since phoneme information account for only a few bits per second and is easily overwhelmed by other variables.</p>"},{"location":"machine_learning/disentangled_representation_learning/#metrics-of-disentanglement","title":"Metrics of disentanglement","text":"<p>Adapted from the benchmark paper by F. Locatello et al.<sup>2</sup>, 2019.</p> <p>This section discusses the metrics of disentanglement. Readers interested only in learning disentangled representation can skip this section.</p>"},{"location":"machine_learning/disentangled_representation_learning/#beta-vae-metric","title":"\\(\\beta\\)-VAE Metric","text":"<p>The \\(\\beta\\)-VAE metric<sup>4</sup> measures disentanglement as the accuracy of a linear classifier that predicts the index of a fixed factor of variation.</p> <p></p> <p>Over a batch of \\(L\\) samples, each pair of data points has a fixed value for one target generative factor \\(y\\) (here \\(y\\) is scale) and differs on all others. A linear classifier is then trained to identify the target factor using the average pairwise difference \\(\\zbf^b_{\\mathrm{diff}}\\) in the latent space over \\(L\\) samples.</p> <p>This metric has several drawbacks as pointed out by follow-up works<sup>5</sup><sup>,</sup><sup>6</sup>:</p> <ul> <li>It is sensitive to hyperparameters of the linear classifier optimization</li> <li>Classifying data-generating factors is not intuitive because factors can be represented as a linear combination of multiple independent latent dimensions<sup>5</sup>. Contrary to this point, <sup>6</sup> believes a truly disentangled model should only contain one latent variable related to each factor, and argues that this metric lacks axis-alignment detection.</li> <li>It has a failure mode: when \\(K-1\\) out of \\(K\\) factors are disentangled, it outputs 100%.</li> </ul>"},{"location":"machine_learning/disentangled_representation_learning/#factorvae-metric","title":"FactorVAE Metric","text":"<p>The FactorVAE metric<sup>5</sup> uses a majority vote classifier on a different feature vector, namely, the argmin of the per-dimension empirical variance.</p> <p></p>"},{"location":"machine_learning/disentangled_representation_learning/#mutual-information-gap-mig","title":"Mutual Information Gap (MIG)","text":"<p>Mutual Information Gap (MIG)<sup>6</sup> measures for each factor of variation the normalized gap in mutual information between the highest and second highest coordinate in \\(z\\).</p> \\[ \\mathrm{MIG} = \\frac{1}{K} \\sum_{k=1}^K \\frac{1}{H(v_k)} \\Big(     I_n(z_{j^{(k)}}; v_k) - \\max_{j \\ne j^{(k)}} I_n(z_{j}; v_k) \\Big) \\] <p>where \\(j^{(k)} = \\argmax_j I_n(z_j;v_k)\\). MIG is bounded by 0 and 1.</p> <p>Estimation of \\(I_n(z_j;v_k)\\) \u2003 Let \\(n \\in \\{1, 2, \\dots, N\\}\\) be the data point index and \\(\\Xcal_{v_k}\\) be the support of \\(p(n|v_k)\\). We make the following assumptions:</p> <ul> <li>Model: \\(q(z, v) = \\sum_x p(v)p(x|v)q(z|x)\\).</li> <li>The inference distribution \\(q(z_j|x)\\) can be sampled from and is known for all \\(j\\).</li> <li>The generating process \\(p(n|v_k)\\) can be sampled from and is known.</li> <li>Simplifying assumption: \\(p(v_k)\\) and \\(p(n|v_k)\\) are quantized.</li> </ul> <p>Then the mutual information can be estimated as the following:</p> \\[ \\begin{align}  &amp; I_n(z_j;v_k) \\\\ =&amp; \\E_{q(z_j, v_k)} \\left( \\log \\sum_{n=1}^N q(z_j, v_k, n) - \\log q(z_j) - \\log p(v_k) \\right) \\\\ =&amp; \\E_{p(v_k)p(n^\\prime|v_k)q(z_j|n^\\prime)} \\left( \\log \\sum_{n=1}^N p(v_k)p(n|v_k)q(z_j|n) - \\log q(z_j) - \\log p(v_k) \\right) \\\\ =&amp; \\E_{p(v_k)p(n^\\prime|v_k)q(z_j|n^\\prime)} \\left( \\log \\sum_{n=1}^N \\frac{p(v_k)}{p(v_k)} p(n|v_k)q(z_j|n) \\right) + H(z_j) \\\\ =&amp; \\E_{p(v_k)p(n^\\prime|v_k)q(z_j|n^\\prime)} \\left( \\log \\sum_{n \\in \\Xcal_{v_k}} p(n|v_k)q(z_j|n) \\right) + H(z_j) \\\\ \\end{align} \\]"},{"location":"machine_learning/disentangled_representation_learning/#modularity-explicitness","title":"Modularity &amp; Explicitness","text":"<p>Background: An Explicit Definition of Disentanglement</p> <p>In the paper<sup>7</sup>, the authors first give an explicit definition of Disentanglement:</p> <ul> <li>Modular: each dimension of the representation conveys information about at most one factor</li> <li>Compact: a given factor is associated with only one or a few dimensions in the representation</li> <li>Explicit: there is a simple (e.g., linear) mapping from the representation to the value of a factor</li> </ul> <p>They use this definition to analyse the \\(\\beta\\)-VAE Metric and the FactorVAE Metric, and found that the former consider only modularity, while the latter consider modularity and compactness. They argue that compactness is not a desiderata for disentangled representation learning since</p> <ul> <li>Forcing compactness can affect the representation\u2019s utility, e.g. \\((\\sin \\theta, \\cos \\theta)\\) vs \\(\\theta\\).</li> <li>Compactness requirement highly constrain the solution space, while allowing redundancy enables many equivalent solutions.</li> </ul> <p>Based on these claims, the authors quantify modularity and explicitness in their metric.</p> <p>The Modularity Metric</p> <p>Note that for ideally modular representation, each latent dimension would have high mutual information with a single factor. Let \\(m_{jk} = I(z_j;v_k)\\) and \\(\\theta_j = \\max_k m_{jk}\\). Define a template vector \\(\\mathbf{t}_j\\) such that </p> \\[ t_{jk} = \\begin{cases} \\theta_j &amp; \\text{if }k=\\argmax_{k^\\prime} m_{jk^\\prime}\\\\ 0        &amp; \\text{otherwise} \\end{cases} \\] <p>The observed deviation from the template is given by</p> \\[ \\delta_j = \\frac{\\sum_k (m_{jk} - t_{jk})^2}{\\theta_j^2(K-1)} \\] <p>A deviation of 0 indicates perfect modularity and 1 indicates that the \\(j\\)-th dimension has equal mutual information with every factor.</p> <p>Thus, they use \\(1 - \\delta_j\\) as a modularity score for latent dimension \\(j\\) and the mean over \\(j\\) as the modularity score for the representation.</p> <p>The Explicitness Metric</p> <p>For explicitness, the author fit a one-versus-rest logistic-regression classifier that takes the entire representation as input, and record the area under the ROC curve (AUROC) of that classifier. The explicitness score is defined as the mean of AUROC over all \\(K\\) factors and all possible values of the factors.</p>"},{"location":"machine_learning/disentangled_representation_learning/#dci-disentanglement-score","title":"DCI Disentanglement Score","text":"<p>The DCI Disentanglement Metric<sup>10</sup> computes the entropy of the normalized importance of each latent dimension \\(j\\) for predicting the value of a factor of variation \\(v_k\\). The metric is calculated as follows:</p> <ul> <li>Train \\(K\\) regressors to predict data-generating factors \\(v_k\\) given latent representation \\(z\\).</li> <li>Construct a matrix \\(R\\) of relative importance  where \\(R_{jk}\\) denotes the relative importance (e.g. absolute value of the LASSO weight) of \\(z_j\\) in predicting \\(v_k\\).</li> <li>Devide \\(R\\) by its row-wise sum to obtain normalized importance \\(P\\), where \\(P_{jk} = \\frac{R_{jk}}{\\sum_{k^\\prime} R_{jk}}\\). Note that the \"disentanglement\" here is equivalent to the \"modularity\" in the Modularity &amp; Explicitness Metric.</li> <li>Calculate the entropy of the normalized importance \\(H(P_j) = -\\sum_{k=1}^K P_{jk}\\log P_{jk}\\) for each latent dimension \\(j\\).</li> <li>Obtain the DCI Disentanglement Metric by weighted average of per-dimension disentanglement \\(D = \\sum_j \\rho_j (1-H(P_j))\\), where \\(\\rho_j = \\frac{\\sum_k R_{jk}}{\\sum_{jk}R_{jk}}\\).</li> </ul>"},{"location":"machine_learning/disentangled_representation_learning/#sap-score","title":"SAP Score","text":"<p>The SAP Score<sup>9</sup> is the average difference of the prediction error of the two most predictive latent dimensions for each factor. The metric is calculated as follows:</p> <ul> <li>Train \\(K\\) regressors/classifiers to predict data-generating factors \\(v_k\\) given the \\(j\\)-th latent dimension \\(z_j\\).</li> <li>Construct a score matrix \\(R\\) where \\(S_{jk}\\) denotes the relative importance (specifically, \\(R^2\\) score for linear regression and balanced classification accuracy for thresholded classification) of \\(z_j\\) in predicting \\(v_k\\). For inactive (low varince) dimensions we take \\(S_{jk}\\) to be zero for all \\(k\\)s.</li> <li>For each factor \\(k\\), take the difference of top two entries of vector \\(S_{\\cdot k}\\).</li> <li>Obtain the SAP score as the mean of these differences over all factors.</li> </ul> <p>Note that SAP focuses on compactness, and thus a high SAP score does not rule out not-modular representations (one latent dimension capturing two or more generative factors well). Further, a low SAP score does not rule out good modularity in cases when two (or more) latent dimensions might be correlated strongly with the same generative factor and poorly with other generative factors.</p>"},{"location":"machine_learning/disentangled_representation_learning/#metric-agreement","title":"Metric Agreement","text":"<p> Taken from the benchmark paper<sup>2</sup>.</p> <p>The authors observe that all metrics except Modularity seem to be correlated strongly on the datasets dSprites, Color-dSprites and Scream-dSprites and mildly on the other data sets. There appear to be two pairs among these metrics that capture particularly similar notions: the \\(\\beta\\)-VAE and the FactorVAE metric as well as the MIG and DCI Disentanglement.</p>"},{"location":"machine_learning/disentangled_representation_learning/#unsupervised-disentanglement","title":"Unsupervised Disentanglement","text":""},{"location":"machine_learning/disentangled_representation_learning/#constraining-the-vae-bottleneck-capacity","title":"Constraining the VAE Bottleneck Capacity","text":""},{"location":"machine_learning/disentangled_representation_learning/#beta-vae","title":"\\(\\beta\\)-VAE","text":"<p>The \\(\\beta\\)-VAE<sup>4</sup> introduces a hyperparameter \\(\\beta\\) in front of the KL regularizer of the VAE ELBO to constrain the capacity of the VAE bottleneck. By setting \\(\\beta &gt; 1\\), the encoder distribution will be forced to better match the factorized unit Gaussian prior. This often results in decreased reconstruction capabilities.</p> \\[ \\E_{p(\\xbf)} \\left[     \\E_{q_\\phi(\\zbf|\\xbf)}\\log p_\\theta(\\xbf|\\zbf) - \\beta \\KL(q_\\phi(\\zbf|\\xbf) \\Vert p(\\zbf)) \\right] \\]"},{"location":"machine_learning/disentangled_representation_learning/#annealedvae","title":"AnnealedVAE","text":"<p>In a follow-up work<sup>11</sup>, the authors propose to progressively increase the bottleneck capacity, so that the encoder can focus on learning one factor of variation at a time and eventually capture more factors of variations than \\(\\beta\\)-VAE:</p> \\[ \\E_{p(\\xbf)} \\left[     \\E_{q_\\phi(\\zbf|\\xbf)}\\log p_\\theta(\\xbf|\\zbf) - \\gamma |\\KL(q_\\phi(\\zbf|\\xbf) \\Vert p(\\zbf)) - C| \\right] \\] <p>where \\(C\\) is annealed from zero to some value large enough to produce good representation.</p>"},{"location":"machine_learning/disentangled_representation_learning/#penalizing-the-total-correlation","title":"Penalizing the total correlation","text":"<p>Note that the KL regularizer term can be rewritten as</p> \\[ \\begin{align}  &amp; \\E_{p(\\xbf)} \\KL(q_\\phi(\\zbf|\\xbf) \\Vert p(\\zbf)) \\\\ =&amp; \\E_{q(\\zbf, \\xbf)} \\log \\left( \\frac{q(\\zbf|\\xbf)}{p(\\zbf)}\\frac{q(\\zbf)}{q(\\zbf)} \\right) \\\\ =&amp; \\E_{q(\\zbf, \\xbf)} \\log \\frac{q(\\zbf|\\xbf)}{q(\\zbf)} + \\E_{q(\\zbf)} \\log \\frac{q(\\zbf)}{p(\\zbf)} \\\\ =&amp; I_q(\\xbf; \\zbf) + \\KL(q(\\zbf) \\Vert p(\\zbf)) \\\\ =&amp; I_q(\\xbf; \\zbf) + \\E_{q(\\zbf)} \\log \\left( \\frac{q(\\zbf)}{\\prod_j p(z_j)} \\frac{\\prod_j q(z_j)}{\\prod_j q(z_j)} \\right) \\\\ =&amp; I_q(\\xbf; \\zbf) + \\KL(q(\\zbf) \\Vert \\prod_j(q(z_j))) + \\sum_j\\KL(q(z_j) \\Vert p(z_j)) \\end{align} \\] <p>where \\(q(\\zbf) = \\sum_\\xbf p(\\xbf) q(\\zbf|\\xbf)\\) is the aggregated posterior. Kim and Minh <sup>5</sup> argues that penalizing \\(I_q(\\xbf; \\zbf)\\) is neither necessary nor desirable for disentanglement. Instead, they opt to minimize the total correlation of \\(q(\\zbf)\\) (the second term in the last line of above equation), yielding the following objective:</p> \\[ \\E_{p(\\xbf)} \\left[     \\E_{q_\\phi(\\zbf|\\xbf)}\\log p_\\theta(\\xbf|\\zbf) - \\KL(q_\\phi(\\zbf|\\xbf) \\Vert p(\\zbf)) \\right] - \\gamma \\KL(q(\\zbf) \\Vert \\prod_j q(z_j)) \\]"},{"location":"machine_learning/disentangled_representation_learning/#factorvae","title":"FactorVAE","text":"<p>Kim and Minh <sup>5</sup> propose to estimate the total correlation using the density ratio trick, i.e. train an additional classifier \\(D\\) to predict if the latent comes from \\(q(\\zbf)\\) or \\(\\prod_j q(z_j)\\). Latent samples from \\(\\prod_j q(z_j)\\) are obtained by randomly permuting the dimensions within latents inferred in a minibatch.</p>"},{"location":"machine_learning/disentangled_representation_learning/#beta-tcvae","title":"\\(\\beta\\)-TCVAE","text":"<p>\\(\\beta\\)-TCVAE optimizes the same objective as FactorVAE, but uses a tractable biased Monte-Carlo estimate for the total correlation. Let \\(\\Bcal_M = (\\xbf_1, \\dots, \\xbf_M)\\) be a minibatch of size \\(M\\) sampled i.i.d. from the training set of size \\(N\\). Therefore, \\(p(\\Bcal_M) = (\\frac{1}{N})^M\\). Let \\(r(\\Bcal_M|n)\\) denote the probability of a sampled minibatch where one of the elements is fixed to be index \\(n\\), thus \\(p(r(\\Bcal_M|n)) = (\\frac{1}{N})^{M-1}\\). Then</p> \\[ \\begin{align}  &amp; \\E_{q(\\zbf)} \\log q(\\zbf) \\\\ =&amp; \\E_{q(\\zbf, n)} \\log \\E_{p(\\Bcal_M)} \\left[\\frac{1}{M} \\sum_{m=1}^M q(\\zbf | n_m) \\right] \\\\ \\ge&amp; \\E_{q(\\zbf, n)} \\log \\E_{r(\\Bcal_M|n)} \\left[ \\frac{p(\\Bcal_M)}{r(\\Bcal_M|n)}\\frac{1}{M} \\sum_{m=1}^M q(\\zbf | n_m) \\right] \\\\ =&amp; \\E_{q(\\zbf, n)} \\log \\E_{r(\\Bcal_M|n)} \\left[ \\frac{1}{MN} \\sum_{m=1}^M q(\\zbf | n_m) \\right] \\\\ \\approx&amp; \\frac{1}{M} \\sum_{i=1}^M \\left[ \\log \\frac{1}{NM} \\sum_{m=1}^M q(\\zbf_i | n_m) \\right] \\end{align} \\] <p>The inequality is due to \\(r\\) having a support that is a subset of that of \\(p\\). \\(\\E_{q(z_j)} \\log q(z_j)\\) is estimated in the same way.</p>"},{"location":"machine_learning/disentangled_representation_learning/#disentangling-the-inferred-prior-aggregated-posterior","title":"Disentangling the Inferred Prior (Aggregated Posterior)","text":"<p>The authors of <sup>9</sup> argue that a disentangled generative model requires a disentangled inferred prior (aggregated posterior) \\(q(\\zbf)\\). They propose to optimize</p> \\[ \\E_{p(\\xbf)} \\left[     \\E_{q_\\phi(\\zbf|\\xbf)}\\log p_\\theta(\\xbf|\\zbf) - \\KL(q_\\phi(\\zbf|\\xbf) \\Vert p(\\zbf)) \\right] - \\lambda \\Dcal(q(\\zbf) \\Vert p(\\zbf)) \\] <p>where \\(\\Dcal\\) is some arbitrary divergence. Since the additional term is intractable, we could either use variational formulation of KL-divergence as in Adversarial AutoEncoeders (AAEs)<sup>12</sup> or simply perform moment matching as in DIP-VAE<sup>9</sup>.</p>"},{"location":"machine_learning/disentangled_representation_learning/#dip-vae","title":"DIP-VAE","text":"<p>DIP-VAE<sup>9</sup> proposes to match the covariance of \\(q(\\zbf)\\) and \\(p(\\zbf)\\). This amounts to decorrelating the dimensions of \\(\\zbf \\sim q_\\phi(\\zbf)\\) if \\(p(\\zbf) = \\Ncal(0, I)\\). By the law of covariance, the covariance of \\(\\zbf \\sim q(\\zbf)\\) is given by</p> \\[ \\Cov_{q_\\phi(\\zbf)}\\zbf = \\E_{p(\\xbf)} \\Cov_{q_\\phi(\\zbf|\\xbf)}\\zbf + \\Cov_{p(\\xbf)} \\E_{q_\\phi(\\zbf|\\xbf)}\\zbf \\] <p>where \\(\\E_{q_\\phi(\\zbf|\\xbf)}\\zbf\\) and \\(\\Cov_{q_\\phi(\\zbf|\\xbf)}\\zbf\\) are functions of the random variable \\(\\xbf\\) (\\(\\zbf\\) is marginalized over). For VAE models with Gaussian posterior of \\(\\zbf\\), i.e. \\(q_\\phi(\\zbf, \\xbf) = \\Ncal(\\mu_\\phi(\\xbf), \\Sigma_\\phi(\\xbf))\\), the covariance of \\(\\zbf \\sim q(\\zbf)\\) reduces to</p> \\[ \\Cov_{q_\\phi(\\zbf)}\\zbf = \\E_{p(\\xbf)} \\Sigma_\\phi(\\xbf) + \\Cov_{p(\\xbf)} \\mu_\\phi(\\xbf) \\] <p>which we want to be close to the identity matrix. We choose entry-wise squared \\(\\ell_2\\)-norm as the measure of proximity.</p> <p>Further, \\(\\Sigma_\\phi(\\xbf)\\) is often taken as diagonal, which means that cross-correlations (off-diagonals) between the latents are due to only \\(\\Cov_{p(\\xbf)} \\mu_\\phi(\\xbf)\\). This suggests two possible options for the disentangling regularizer:</p> <ul> <li>DIP-VAE-I \u2003 Regularizing only \\(\\Cov_{p(\\xbf)} \\mu_\\phi(\\xbf)\\). The objective becomes</li> </ul> \\[ \\max_{\\theta, \\phi} \\ELBO(\\theta, \\phi) - \\lambda_1 \\sum_{i \\ne j} \\left[\\Cov_{p(\\xbf)\\mu_{\\phi}(\\xbf)}\\right]^2_{ij} - \\lambda_2 \\sum_i \\left(\\left[\\Cov_{p(\\xbf)} \\mu_\\phi(\\xbf)\\right]_{ii} - 1\\right)^2 \\] <ul> <li>DIP-VAE-II \u2003 Regularizing \\(\\Cov_{q_\\phi(\\zbf)}\\zbf\\). The objective is</li> </ul> \\[ \\max_{\\theta, \\phi} \\ELBO(\\theta, \\phi) - \\lambda_1 \\sum_{i \\ne j} \\left[\\Cov_{q_\\phi(\\zbf)}\\zbf\\right]^2_{ij} - \\lambda_2 \\sum_i \\left(\\left[\\Cov_{q_\\phi(\\zbf)}\\zbf\\right]_{ii} - 1\\right)^2 \\] <p>The authors argue that in datasets where the number of generative factors is less than the latent dimension, DIP-VAE-II is more suitable than DIP-VAE-I as keeping all dimensions active (optimizing towards \\(\\Var_{q_\\phi(\\zbf)}z_i=1\\) for all \\(i\\)'s) might result in splitting of an attribute across multiple dimensions, hurting the goal of disentanglement.</p>"},{"location":"machine_learning/disentangled_representation_learning/#aae","title":"AAE","text":"<p>Adversarial autoencoders<sup>12</sup> attaches an adversarial discriminator on top of the latent representation of an autoencoder to match the aggregated posterior \\(q(\\zbf)\\) to an arbitrary easy-to-sample prior \\(p(\\zbf)\\).</p> <p>The network is trained jointly in two phases:</p> <ol> <li>In the reconstruction phase, the autoencoder minimizes the reconstruction error of the inputs.</li> <li>In the regularization phase, the discriminator is first updated to tell apart the true samples (generated using the prior) from the generated samples (the hidden codes computed by the encoder). Then the encoder is updated to confuse the discriminator.</li> </ol> <p> AAE has many variants aimed for different learning settings. (left) Adversarial Autoencoders; (middle) Semi-supervised AAE; (right) AAE for Dimensionality Reduction.</p> <ul> <li>Semi-supervised AAE has two adversarial discriminator: one for label \\(y\\) and another for style representation \\(\\zbf\\).<ol> <li>In the reconstruction phase, the autoencoder minimize the reconstruction error of the inputs on an unlabeled mini-batch.</li> <li>In the regularization phase, each of the discriminators is updated to tell apart the true samples (generated using the Categorical and Gaussian priors) from the generated samples (the hidden codes computed by the encoder). Then the encoders are updated to confuse their discriminators.</li> <li>In the classification phase, the encoder is updated to minimize the cross-entropy cost on a labeled mini-batch.</li> </ol> </li> <li>AAE for Unsupervised Clustering has the same architecture as the Semi-supervised AAE, while it does not have the classification phase.</li> <li>AAE for Dimensionality Reduction aims to learn a good representation that encodes both the label and style information. The final representation is achieved by adding the distributed representation of the cluster head with the style representation, both \\(n\\)-dimensional. The cluster head representation is obtained by multiplying the \\(m\\) dimensional one-hot class label vector by an \\(m \\times n\\) matrix \\(W_C\\). We introduce an additional cost function that linearly penalizes the Euclidean distance between every two cluster heads if they are greater than a threshold \\(\\eta\\).</li> </ul>"},{"location":"machine_learning/disentangled_representation_learning/#infovae","title":"InfoVAE","text":"<p>The InfoVAE paper<sup>13</sup> points out that the amortized inference distribution \\(q_\\phi(\\zbf|\\xbf)\\) of VAEs might fail to approximate the true posterior \\(p_\\theta(\\zbf|\\xbf)\\) for two reasons:</p> <ul> <li>The ELBO can be maximized (even to infinity in extreme cases) with a very inaccurate variational posterior.</li> <li>Implicit modeling bias: common modeling choices (such as the high dimensionality of \\(\\xbf\\) compared to \\(\\zbf\\)) tend to sacrifice variational inference for data fit when modeling capacity is not sufficient to achieve both.</li> </ul> <p>Regardless of the cause, this is generally an undesirable phenomenon for two reasons:</p> <ul> <li>One may care about accurate inference (e.g. in representation learning) more than generating sharp samples.</li> <li>Overfitting: Because \\(p_{\\mathrm{data}}\\) is an empirical (finite) distribution in practice, matching it too closely can lead to poor generalization.</li> </ul> <p>To remedy this issue, the authors propose an additional mutual information term between \\(\\xbf\\) and \\(\\zbf\\) under the joint \"inference\" distribution \\(q_\\phi(\\xbf, \\zbf)\\), which is very similar to the additional term in DIP-VAE discussed above.</p> \\[ \\begin{align} &amp;\\Lcal_{\\mathrm{InfoVAE}} \\\\ =&amp; -\\lambda\\KL(q_\\phi(\\zbf) \\Vert p(\\zbf)) - \\E_{q(\\zbf)}\\left[ \\KL(q_\\phi(\\xbf|\\zbf) \\Vert p_\\theta(\\xbf|\\zbf)) \\right] + \\alpha I_q(\\xbf;\\zbf)\\\\ =&amp; \\E_{p(\\xbf)} \\left[     \\E_{q_\\phi(\\zbf|\\xbf)}\\log p_\\theta(\\xbf|\\zbf) - (1 - \\alpha) \\KL(q_\\phi(\\zbf|\\xbf) \\Vert p(\\zbf)) \\right] - (\\alpha + \\lambda - 1) \\KL(q_\\phi(\\zbf) \\Vert p(\\zbf)) \\end{align} \\] <p>We can easily see that \\(\\beta\\)-VAE (\\(\\lambda &gt; 0\\) and \\(\\alpha = 1 - \\lambda\\)) and AAE (\\(\\alpha = \\lambda = 1\\), Use Jenson-Shannon Divergence to approximate \\(\\KL(q(\\zbf) \\Vert p(\\zbf))\\)) belongs to this framework. If we relax the last KL term to any strict divergence (strict as in \\(\\Dcal(q\\Vert p) = 0\\) iff \\(q = p\\)), we can get three categories of InfoVAE:</p> <ul> <li>Adversarial Training as discussed in AAE.</li> <li>Stein Variational Gradient<sup>14</sup> aims to find the steepest direction \\(\\phi_{q,p}\\) that transforms \\(q(\\zbf)\\) towards \\(p(\\zbf)\\). For a small step size \\(\\epsilon\\), the transformation \\(T(\\zbf)\\) is defined as \\(T(\\zbf) = z + \\epsilon \\phi_{q,p}(\\zbf)\\). The \\(\\phi^\\star\\) that minimizes \\(\\KL(T(\\zbf) \\Vert p(\\zbf))\\) is     $$     \\phi_{q,p}^\\star(\\cdot) = \\E_{q(\\zbf)} \\left[ k(\\zbf,\\cdot)\\nabla_\\zbf \\log p(\\zbf) + \\nabla_\\zbf k(\\zbf,\\cdot) \\right]     $$     where \\(k\\) is a positive definite kernel function. In practice we use minibatches to estimate the Stein gradients and use the surrogate loss \\(\\widehat{\\Dcal}(q_\\phi(\\zbf) \\Vert p(\\zbf)) = z^\\top \\stopgradient(\\phi^\\star_{q_\\phi, p}(\\zbf))\\)</li> <li>Maximum-Mean Discrepancy is a framework to quantify the distance between two distributions by comparing all of their moments. It can be efficiently implemented using the kernel trick. Letting \\(k(\\cdot, \\cdot)\\) be any positive definite kernel, the MMD between \\(p\\) and \\(q\\) is     $$     \\MMD(q \\Vert p) = \\E_{p(\\zbf), p(\\zbf^\\prime)} k(\\zbf,\\zbf^\\prime) - 2\\E_{q(\\zbf), p(\\zbf^\\prime)} k(\\zbf,\\zbf^\\prime) + \\E_{q(\\zbf), q(\\zbf^\\prime)} k(\\zbf,\\zbf^\\prime)     $$     \\(\\MMD(q \\Vert p)\\) if and only if \\(p = q\\).</li> </ul> <p>The authors empirically shows that the MMD variant perform on-par or better than the other two in terms of distance between the aggregated posterior \\(q_\\phi(\\zbf)\\) and the prior \\(p(\\zbf)\\), distance between sample label distribution (obtained by a pretrained classifier) and the true label distribution, training speed and stablity, semi-supervised learning on 1k labeled data.</p>"},{"location":"machine_learning/disentangled_representation_learning/#maximizing-mi-bt-latent-code-and-gan-samples","title":"Maximizing MI b/t Latent Code and GAN Samples","text":"<p>Generative Adversarial Networks (GANs) are usually not used for representation learning since it does not have an encoder. But if one wants to generate samples in an interpretable and disentangled way, InfoGAN is one option.</p>"},{"location":"machine_learning/disentangled_representation_learning/#infogan","title":"InfoGAN","text":"<p>InfoGAN<sup>15</sup> splits the latent vector of GAN into two parts: the incompressible noise \\(\\zbf\\), and the latent code \\(\\cbf\\) that target the salient structured semantic features of the data distribution. In its simplest form, we may assume a factored distribution for \\(\\cbf\\), given by \\(P(\\cbf) = \\prod_{i=1}^K P(c_i)\\). To avoid the generator ignoring \\(\\cbf\\), a mutual information constraint is added between the latent code and the generated samples, making the overall objective (\\(Q\\) will be introduced below)</p> \\[ \\min_{G,Q} \\max_D \\E_{\\xbf \\sim p_{\\mathrm{data}}}\\log D(\\xbf) + \\E_{[\\zbf;\\cbf] \\sim p_{\\mathrm{noise}}} \\log (1 - D(G(\\zbf, \\cbf))) - \\lambda I(\\cbf, G(\\zbf, \\cbf)) \\] <p>To maximize mutual information, the authors employ variational MI maximization.</p> \\[ \\begin{align} &amp;I(\\cbf, G(\\zbf, \\cbf)) \\\\ =&amp; H(\\cbf) - H(\\cbf|G(\\zbf,\\cbf)) \\\\ =&amp; \\E_{\\xbf \\sim G(\\zbf,\\cbf)} \\E_{\\cbf^\\prime \\sim P(\\cbf|\\xbf)} \\log P(\\cbf^\\prime|\\xbf) + H(\\cbf) \\\\ =&amp; \\E_{\\xbf \\sim G(\\zbf,\\cbf)} \\left[ \\KL(P(\\cdot|\\xbf) \\Vert Q(\\cdot|\\xbf)) + \\E_{\\cbf^\\prime \\sim P(\\cbf|\\xbf)} \\log Q(\\cbf^\\prime|\\xbf) \\right] + H(\\cbf) \\\\ \\ge&amp; \\E_{\\xbf \\sim G(\\zbf,\\cbf)} \\E_{\\cbf^\\prime \\sim P(\\cbf|\\xbf)} \\log Q(\\cbf^\\prime|\\xbf) + H(\\cbf) \\\\ =&amp; \\E_{\\cbf \\sim P(\\cbf)} \\E_{\\xbf \\sim G(\\zbf, \\cbf)} \\log Q(\\cbf|\\xbf) + H(\\cbf) \\end{align} \\] <p>For simplicity, the authors fix the latent code distribution and treat \\(H(\\cbf)\\) as a constant. Note that this variational objective \\(\\Lcal_{\\mathrm{Info}}(G, Q)\\) is easy to approximate with Monte Carlo simulation. In particular, it can be maximized w.r.t. \\(Q\\) directly and w.r.t. \\(G\\) via the reparametrization trick.</p> <p>From a representation learning perspective, \\(Q\\) can be seen as an encoder. The discriminator \\(D\\) and the encoder \\(Q\\) share all convolutional layers and have separate fully-connected final layers. For categorical latent code \\(\\cbf\\), we use the natural choice of softmax nonlinearity to represent \\(Q(c_i|\\xbf)\\). For continuous latent code \\(\\cbf\\), there are more options depending on what is the true posterior \\(P(c_i |\\xbf)\\). The authors found that simply treating \\(Q(c_i |\\xbf)\\) as a factored Gaussian is sufficient.</p> <p>Note that the InfoGAN-CR authors found<sup>16</sup> that InfoGAN performance can be significantly improved with better architectural and hyperparameter choices.</p>"},{"location":"machine_learning/disentangled_representation_learning/#infogan-cr","title":"InfoGAN-CR","text":"<p>InfoGAN-CR<sup>16</sup> is inspired by the idea that disentanglement is characterized by distinct changes in the images when traversing the latent space. A constrastive regularizer (CR) is designed to encourage this behavior: Pairs of images \\((x, x^\\prime) \\sim R^{(i)}\\) are generated by fixing the \\(i\\)-th latent code \\(c_i\\) and drawing the rest of the latent codes with a controlled contrastive gap (details below). The authors propose to maximize the distinctness of this latent traversal with generalized Jensen-Shannon divergence among \\(R^{(i)}\\)'s</p> \\[ \\JS(R^{1}, \\dots, R^{(K)}) = \\frac{1}{K}\\sum_{i \\in [K]} \\KL(R^{(i)} \\Vert \\bar{R}) \\] <p>where \\(\\bar{R} = \\frac{1}{K}\\sum_{j \\in [K]} R^{(j)}\\).</p> <p></p> <p>To implement the CR, the authors introduce a CR discriminator \\(H: \\R^N \\times \\R^N \\to \\R^K\\) that performs multi-way hypothesis testing, where \\(N\\) is the input dimension and \\(K\\) is the latent code dimension. The discriminator \\(H\\) tries to identify which code \\(i\\) was shared between the paired images. Both the generator \\(G\\) and the discriminator \\(H\\) try to make the \\(K\\)-way hypothesis testing successful. We add the following cross-entropy loss to the training objective</p> \\[ \\min_{G,H}\\Lcal_{\\mathrm{CR}}(G, H) = -\\E_{i \\sim \\mathrm{U}([K]), (x, x^\\prime) \\sim R^{(i)}} \\log H_i(x, x^\\prime) \\] <p>The overall objective then becomes</p> \\[ \\min_{G, H, Q}\\max_{D} \\Lcal_{\\mathrm{GAN}}(G, D) - \\lambda\\Lcal_{\\mathrm{Info}}(G, Q) + \\alpha \\Lcal_{\\mathrm{CR}}(G, H) \\] <p>Justification of the CR implementation. We can verify using Langrange's Multiplier that \\(H_i(x, x^\\prime) = \\frac{R^{(i)}(x, x^\\prime)}{\\sum_j R^{(j)}(x, x^\\prime)}\\) is a minimizer of \\(\\Lcal_{\\mathrm{CR}}\\). The minimum loss is</p> \\[ \\begin{align} &amp;-\\E_{i \\sim \\mathrm{U}([K]), (x, x^\\prime) \\sim R^{(i)}} \\log H_i(x, x^\\prime) \\\\ =&amp; -\\frac{1}{K} \\sum_{i=1}^K \\E_{(x, x^\\prime) \\in R^{(i)}} \\log \\frac{R^{(i)}(x, x^\\prime)/K}{\\sum_i R^{(i)}(x, x^\\prime)/K} \\\\ =&amp; \\log K - \\frac{1}{K} \\sum_{i=1}^K \\KL \\left( R^{(i)} \\Vert \\bar{R} \\right) \\\\ =&amp; \\log K - \\JS(R^{(1)}, \\dots, R^{(K)}) \\end{align} \\] <p>Progressive training. The authors propose to progressively narrow the contrastive gap, defined as \\(\\min_{j \\in [K]\\backslash{i}}|c_j - c_j^\\prime|\\), during training. This means the varying dimensions of the contrastive samples would get smaller and smaller variance, making them look like the fixed dimension \\(i\\) more and more and thus result in more and more difficult prediction for \\(H\\).</p>"},{"location":"machine_learning/disentangled_representation_learning/#elastic-infogan","title":"Elastic-InfoGAN","text":""},{"location":"machine_learning/disentangled_representation_learning/#leveraging-the-hierarchy-of-latent-variables","title":"Leveraging the Hierarchy of Latent Variables","text":""},{"location":"machine_learning/disentangled_representation_learning/#stylegan","title":"StyleGAN","text":""},{"location":"machine_learning/disentangled_representation_learning/#very-deep-vae","title":"Very Deep VAE","text":""},{"location":"machine_learning/disentangled_representation_learning/#the-impossibility-of-unsupervised-disentanglement","title":"The Impossibility of Unsupervised Disentanglement","text":"<p>F. Locatello et al.<sup>2</sup> showed that the unsupervised learning of disentangled representations is theoretically impossible from i.i.d. observations without inductive biases. After observing \\(\\xbf\\), we can construct infinitely many generative models which have the same marginal distribution of \\(\\xbf\\). Any one of these models could be the true causal generative model for the data, and the right model cannot be identified given only the distribution of \\(\\xbf\\).</p>"},{"location":"machine_learning/disentangled_representation_learning/#learning-from-partial-class-labels","title":"Learning from Partial Class Labels","text":""},{"location":"machine_learning/disentangled_representation_learning/#latent-feature-discriminative-model-m1","title":"Latent Feature Discriminative Model (M1)","text":"<p>Proposed by D. P. Kingma et al.<sup>19</sup>.</p> <ol> <li>Use VAE to learn an embedding space for all the training data.</li> <li>Train a classifier (transductive SVM / multinomial regression) on the labeled data in the embedding space.</li> </ol>"},{"location":"machine_learning/disentangled_representation_learning/#semi-supervised-vae","title":"Semi-Supervised VAE","text":""},{"location":"machine_learning/disentangled_representation_learning/#m2","title":"M2","text":"<p>Proposed by D. P. Kingma et al.<sup>19</sup>. The generative process is described as</p> \\[ p(\\ybf) = \\mathrm{Cat}(\\ybf|\\pi) \\qquad p(\\zbf) = \\Ncal(\\zbf|\\mathbf{0}, I) \\qquad p_\\theta(\\xbf| \\ybf, \\zbf) = f_\\theta(\\xbf; \\ybf, \\zbf) \\] <p>The inference distribution is parameterized as</p> \\[ q_\\phi(\\zbf|\\ybf, \\xbf) = \\Ncal(\\zbf | \\mu_\\phi(\\ybf, \\xbf), \\diag(\\sigma^2_\\phi (\\ybf, \\xbf))) \\qquad q_\\phi(\\ybf|\\xbf) = \\mathrm{Cat}(\\ybf|\\pi_\\phi(\\xbf)) \\] <p>The learning objective is a weighted sum of the ELBO loss of \\(\\log p_\\theta(\\xbf, \\ybf)\\), and the classification loss on \\(q_\\phi(\\ybf | \\xbf)\\). As this model is an instance of the Generalized M2 model, we leave the detailed discussion below.</p> <p>For a systematic benchmark of the performance of models combining M2 and the unsupervised VAE disentanglement methods discussed above, see<sup>21</sup>.</p>"},{"location":"machine_learning/disentangled_representation_learning/#generalized-m2","title":"Generalized M2","text":"<p>N. Siddharth et al.<sup>17</sup> focus on the setting where some factors \\(\\ybf\\) are interpretable and partly labeled. They generalize the M2 model to arbitrary conditional dependency structure. They first define an objective over \\(N\\) unsupervised data points \\(\\Dcal = \\{\\xbf^i\\}_{i=1}^N\\) and \\(M\\) supervised data points \\(\\Dsup = \\{(\\xbf^i, \\ybf^i)\\}_{i=1}^M\\).</p> \\[ \\Lcal(\\theta, \\phi; \\Dcal, \\Dsup) = \\sum_{i=1}^N \\ELBO(\\theta, \\phi; \\xbf^n) + \\gamma \\sum_{i=1}^M \\Lcal_{\\mathrm{sup}}(\\theta, \\phi; \\xbf^m, \\ybf^m) \\] <p>The supervised loss is the ELBO loss of \\(\\log p_\\theta(\\xbf, \\ybf)\\), and the cross entropy loss encouraging accurate inference of \\(\\ybf\\).</p> \\[ \\Lcal_{\\mathrm{sup}}(\\theta, \\phi; \\xbf^m, \\ybf^m) = \\E_{q_\\phi(\\zbf|\\xbf^m, \\ybf^m)} \\log \\frac{p_\\theta(\\xbf^m, \\ybf^m, \\zbf)}{q_\\phi(\\zbf|\\xbf^m,\\ybf^m)} + \\alpha\\log q_\\phi(\\ybf^m|\\xbf^m) \\] <p>Calculating the supervised loss. Note that this definition implicitly assumes that we can evaluate the conditional probability \\(q_\\phi(\\zbf | \\xbf, \\ybf)\\) and the marginal \\(q_\\phi(\\ybf|\\xbf) = \\int q_\\phi(\\ybf, \\zbf|\\xbf) \\dd \\zbf\\). This would be the case if we factor \\(q_\\phi(\\ybf, \\zbf|\\xbf)\\) into \\(q_\\phi(\\zbf | \\xbf, \\ybf)q_\\phi(\\ybf|\\xbf)\\), as in the M2 model. But if we have another conditional dependence structure such as \\(q_\\phi(\\ybf, \\zbf|\\xbf) = q_\\phi(\\ybf | \\xbf, \\zbf)q_\\phi(\\zbf|\\xbf)\\), we would run into the following difficulties:</p> <ol> <li>Evaluating the density \\(q_\\phi(\\zbf|\\xbf, \\ybf) = \\frac{q_\\phi(\\ybf, \\zbf|\\xbf)}{\\int q_\\phi(\\ybf, \\zbf|\\xbf) \\dd \\zbf}\\) becomes difficult</li> <li>Generating samples, e.g. sampling from \\(q_\\phi(\\zbf|\\xbf, \\ybf)\\) requires inference</li> </ol> <p>We first re-express \\(\\Lcal_{\\mathrm{sup}}\\) as follows, which alleviates (1.) by removing the need to evaluate \\(q_\\phi(\\zbf|\\xbf, \\ybf)\\). $$ \\Lcal_{\\mathrm{sup}}(\\theta, \\phi; \\xbf^m, \\ybf^m) = \\E_{q_\\phi(\\zbf|\\xbf^m, \\ybf^m)} \\log \\frac{p_\\theta(\\xbf^m, \\ybf^m, \\zbf)}{q_\\phi(\\ybf^m,\\zbf|\\xbf^m)} + (1 + \\alpha)\\log q_\\phi(\\ybf^m|\\xbf^m) $$</p> <p>We can then use (self-normalised) importance sampling to approximate the supervsied loss. Specifically, we sample proposals \\(\\zbf^{m,s} \\sim q_\\phi(\\zbf|\\xbf^m)\\) from the unconditioned encoder distribution and estimate the first supervised term as follows:</p> \\[ \\E_{q_\\phi(\\zbf|\\xbf^m, \\ybf^m)} \\log \\frac{p_\\theta(\\xbf^m, \\ybf^m, \\zbf)}{q_\\phi(\\ybf^m,\\zbf|\\xbf^m)} \\approx \\frac{1}{S}\\sum_{s=1}^S \\frac{w^{m,s}}{Z^m} \\log \\frac{p_\\theta(\\xbf^m, \\ybf^m, \\zbf)}{q_\\phi(\\ybf^m,\\zbf|\\xbf^m)} \\] <p>where the unnormalized weight \\(w^{m,s}\\) and the normalizer \\(Z^m\\) are defined as</p> \\[ w^{m,s} = \\frac{q_\\phi(\\ybf^m, \\zbf^{m,s}|\\xbf^m)}{q_\\phi(\\zbf^{m,s}|\\xbf^m)} = q_\\phi(\\ybf^m|\\zbf^{m,s},\\xbf^m) \\qquad Z^m = \\sum_{s=1}^S w^{m,s} \\] <p>Note that the unnormalized weight is the product of variational conditional probabilities of the supervised variables \\(\\ybf\\). We then estimate a lower bound of \\(\\log q_\\phi(\\ybf^m|\\xbf^m)\\) as follows:</p> \\[ \\log q_\\phi(\\ybf^m | \\xbf^m) \\ge \\E_{q_\\phi(\\zbf|\\xbf^m)} \\log \\frac{q_\\phi(\\ybf^m, \\zbf^{m,s}|\\xbf^m)}{q_\\phi(\\zbf^{m,s}|\\xbf^m)} \\approx \\frac{1}{S}\\sum_{s=1}^S \\log w^{m,s} \\] <p>The estimated supervised loss is</p> \\[ \\widehat{\\Lcal}_{\\mathrm{sup}}(\\theta, \\phi; \\xbf^m, \\ybf^m) = \\frac{1}{S}\\sum_{s=1}^S \\left[\\frac{w^{m,s}}{Z^m} \\log \\frac{p_\\theta(\\xbf^m, \\ybf^m, \\zbf)}{q_\\phi(\\ybf^m,\\zbf|\\xbf^m)} + (1+\\alpha) \\log w^{m,s}\\right] \\] <p>Universal applicability of the framework. The above framework is applicable to any conditional dependency structure, e.g., if we have</p> \\[ q_\\phi(\\zbf_2, \\ybf, \\zbf_1|\\xbf) = q_\\phi(\\zbf_2|\\ybf,\\zbf_1,\\xbf) q_\\phi(\\ybf|\\zbf_1,\\xbf) q_\\phi(\\zbf_1|\\xbf) \\] <p>we propose \\(\\zbf_1 \\sim q_\\phi(\\zbf_1|\\xbf)\\) and \\(\\zbf_2 \\sim q_\\phi(\\zbf_2|\\ybf,\\zbf_1,\\xbf)\\), then the unnormalized importance weight is defined as</p> \\[ w^{m,s} = \\frac{q_\\phi(\\zbf_2, \\ybf, \\zbf_1|\\xbf)}{q_\\phi(\\zbf_1|\\xbf)q_\\phi(\\zbf_2|\\ybf,\\zbf_1,\\xbf)} = q_\\phi(\\ybf|\\zbf_1,\\xbf) \\] <p>which is, again, the product of variational conditional probabilities of the supervised variables \\(\\ybf\\).</p> <p></p> <p>Demonstration of disentanglement on Yale B. The recognition model (encoder) is defined as \\(q(r, l, i, s | x) = q(r|x)q(l|x)q(i|x)q(s|l, i)\\), with \\(r\\), \\(l\\), \\(i\\) and \\(s\\) denote reflectance, lighting, identity and shading, respectively. Only \\(l\\) and \\(i\\) are supervised. The generative model (decoder) is fully factored, i.e. all latents are independent under the prior.</p>"},{"location":"machine_learning/disentangled_representation_learning/#adversarial-training","title":"Adversarial Training","text":""},{"location":"machine_learning/disentangled_representation_learning/#vae-gan","title":"VAE-GAN","text":"<p>M. Mathieu and Y. Lecun<sup>26</sup> proposed to use GAN to disentangle the specified (content) factors of variations \\(\\sbf\\) and unspecified (style/pose) factors \\(\\zbf\\). It is similar to AAE except that (1) the autoencoder is KL regularized; (2) the discriminator input is in the observation space.</p> <p>The generative model is given by</p> \\[ \\zbf \\sim p(\\zbf) = \\Ncal(\\mathbf{0}, I) \\qquad \\xbf \\sim p_\\theta(\\xbf|\\zbf, \\sbf) \\] <p>The latent variables are inferred by the encoder</p> \\[ q_\\phi(\\zbf | \\xbf, \\sbf) = \\Ncal\\Big(\\mu_\\phi\\big(\\xbf, s_\\phi(\\xbf)\\big), \\diag\\big(\\sigma^2_\\phi\\big(\\xbf, s_\\phi(\\xbf)\\big)\\big)\\Big) \\] <p>Note that</p> <ul> <li>This work does not aggregate information from data points of the same class (see Learning from Grouped Observations). Instead the authors swap the specified factors \\(\\sbf\\) of a pair of observations \\((\\xbf_1, \\xbf_1^\\prime)\\) from the same class, and use the reconstruction loss to ensure data points in the same class have similar \\(\\sbf\\)'s.</li> <li>\\(s_\\phi\\) is a deterministic encoder thus all sources of stochasticity of \\(\\sbf\\) come from the data distribution. In other words, the conditional likelihood given above can be written as \\(\\xbf \\sim p_\\theta(\\xbf|\\zbf, s_\\phi(\\xbf^\\prime))\\) where \\(\\xbf^\\prime\\) and \\(\\xbf\\) share the same class label.</li> </ul> <p>To prevent the model from ignoring \\(\\sbf\\) and degenerating to a standard VAE, the authors swap the unspecified components \\(\\zbf\\) of a pair of observations \\((\\xbf_1, \\xbf_2)\\) or sample from the prior distribution \\(\\zbf \\sim \\Ncal(\\mathbf{0}, I)\\), decode them back into the observation space, then ensure the reconstructions are assigned high probabilities of belonging to their original classes by an external discriminator. The adversarial discriminator, instead, aims to correctly classify these generated reconstructions from the true samples from class \\(y\\).</p> \\[ \\min_G \\max_D \\log D(\\xbf|y) + \\log (1 - D(G(\\zbf, \\sbf_y), y)) \\] <p>where \\(\\sbf_y = s_\\phi(\\xbf)\\) where \\(\\xbf\\) has class label \\(y\\).</p>"},{"location":"machine_learning/disentangled_representation_learning/#drnet","title":"DrNet","text":"<p>DrNet<sup>25</sup> factorizes the video representation into time-varying (pose) and time-independent (content) components.</p> <p>Let \\(\\xbf_i = (\\xbf_i^1, \\dots, \\xbf_i^T)\\) denote a sequence of \\(T\\) images from video \\(i\\). The DrNet model has four components: a pose encoder \\(E_p\\), a content encoder \\(E_c\\), a future frame decoder \\(G\\), and a scene discriminator \\(D\\).</p> <p>The loss function is a weighted sum of the following terms</p> <ul> <li> <p>Reconstruction loss: standard per-pixel \\(\\ell_2\\) loss between the decoder-predicted future frame \\(\\widetilde{\\xbf}^{t+k}\\) and the actual frame \\(\\xbf^{t+k}\\) for some random frame offset \\(k \\in [0, K]\\).</p> \\[ \\min_{E_c, E_p, G} \\left\\Vert G\\big( E_c(\\xbf^t), E_p(\\xbf^{t+k}) \\big) - \\xbf^{t+k} \\right\\Vert_2^2 \\] </li> <li> <p>Similarity Loss: to ensure the content encoder extracts time-invariant representations, content representations across time frames should be similar to each other.</p> \\[ \\min_{E_c} \\left\\Vert E_c(\\xbf^t) - E_c(\\xbf^{t+k}) \\right\\Vert_2^2 \\] </li> <li> <p>Adversarial Loss: Exploiting the fact that the objects present do not typically change within a video but do between different videos, the scene discriminator \\(D\\) attempts to determine if a pose feature pair \\((E_p(\\xbf_i^t), E_p(\\xbf_j^{t+k}))\\) come from the same video, i.e. if \\(i = j\\). The pose encoder, which is not supposed to carry any content information, tries to maximize the uncertainty of the discriminator output (targeting 1/2) on pairs of frames from the same clip.</p> \\[\\begin{align} \\min_D &amp; \\Lcal_{\\mathrm{BCE}}\\Big(\\mathbb{1}_{i=j}, D\\big( E_p(\\xbf_i^t), E_p(\\xbf_j^{t+k}) \\big)\\Big) \\\\ &amp;= -\\log D \\big( E_p(\\xbf_i^t), E_p(\\xbf_i^{t+k}) \\big) - \\log \\Big(1 - D\\big( E_p(\\xbf_i^t), E_p(\\xbf_j^{t+k}) \\big) \\Big) \\\\ \\min_{E_p} &amp; \\Lcal_{\\mathrm{BCE}}\\Big(\\frac{1}{2},  D\\big( E_p(\\xbf_i^t), E_p(\\xbf_i^{t+k}) \\big)\\Big) \\\\ &amp;= -\\frac{1}{2}\\log D \\big( E_p(\\xbf_i^t), E_p(\\xbf_i^{t+k}) \\big) - \\frac{1}{2}\\log \\Big(1 - D\\big( E_p(\\xbf_i^t), E_p(\\xbf_i^{t+k}) \\big) \\Big) \\end{align}\\] </li> </ul>"},{"location":"machine_learning/disentangled_representation_learning/#latent-optimization","title":"Latent Optimization","text":""},{"location":"machine_learning/disentangled_representation_learning/#lord","title":"LORD","text":"<p>Gabbay et al.<sup>20</sup> propose that Latent Optimization for Representation Disentanglement (LORD) is superior to amortized inference for achieving disentangled representations.</p> <p>Latent Optimization. The generative model is defined as (we use the notations of the LORD paper to align with the image below)</p> \\[ x_i = G_\\theta(e_{y_i}, c_i) \\] <p>where the class embedding \\(e_y\\) is shared between all images of the same class, and the content embedding \\(c\\) is different for every data point \\(x\\). The benefit of this model is as follows</p> <ul> <li>As the class embedding is shared exactly between all images of the same class, it is impossible to include any content information in the class embedding.</li> <li>As we learn per-class representations directly rather than using previous techniques as group averaging, each mini-batch can contain images randomly sampled from all classes allowing maximal diversity (which cannot be achieved by ML-VAE).</li> </ul> <p>Asymmetric Noise Regularization. To ensure that class information does not leak into the content representation, LORD regularizes the content code to enforce minimality of information with an additive Gaussian noise of a fixed variance, and an activation decay penalty, resulting in the following objective</p> \\[ \\Lcal = \\sum_{i=1}^n \\Lcal_{\\mathrm{VGG}} (G_\\theta(e_{y_i}, c_i + z_i), x_i) + \\lambda \\Vert c_i \\Vert^2, \\quad z_i \\sim \\Ncal(0, \\sigma^2I) \\] <p>where the first term is the VGG perceptual loss<sup>21</sup>.</p> <p></p> <p>Amortization for One-Shot Inference. At test time, a data point from an unknown class is observed. Optimizing over the latent representations for a single data points leads to overfitting which results in entangled representations. Moreover, it requires iterative test-time inference which is time-consuming. LORD proposes to learn a class encoder \\(E_y\\) and a content encoder \\(E_c\\) after stage-one training (described above) to enable efficient test-time inference.</p> \\[ \\Lcal = \\sum_{i=1}^n \\Lcal_{\\mathrm{VGG}} (G_\\theta(E_y(x_i), E_c(x_i)), x_i) + \\alpha_e \\Vert E_y(x_i) - e_{y_i} \\Vert^2  + \\alpha_c \\Vert E_c(x_i) - c_{i} \\Vert^2 \\] <p>The stage-two objective is a weighted sum of the reconstruction loss and the latent MSE loss.</p> <p></p>"},{"location":"machine_learning/disentangled_representation_learning/#learning-from-grouped-observations","title":"Learning from Grouped Observations","text":""},{"location":"machine_learning/disentangled_representation_learning/#ml-vae","title":"ML-VAE","text":"<p>Multi-Layer VAE (ML-VAE)<sup>18</sup> focuses on how to perform amortized inference in the context of non-i.i.d., grouped observations. The authors assume disjoint groups, where within each group, a factor of variation (or content) is shared among all observations, and each observation has an independent representation for its style.</p> <p>Traditional stochastic variational inference (SVI) would easily extend to this setting, but suffers from expensive test-time inference since it does not perform amortized inference for observations or groups. VAE performs amortized inference for observations, but assumes i.i.d. observations and fails to incorporate the grouping information. ML-VAE attemps to extend the VAE framework to perform amortized inference on groups of observations.</p> <p></p> <p>First we assume independence between grouped observations</p> \\[\\log p(\\Xbf|\\theta) = \\sum_{G \\in \\Gcal} \\log p(\\Xbf_G|\\theta)\\] <p>For each group, we derive the ELBO for group \\(G\\) and maximize the sum of ELBOs over all groups.</p> \\[ \\begin{align} \\ELBO(G; \\theta, \\phi_s, \\phi_c) =&amp; \\E_{q_{\\phi_c}(C_G|\\Xbf)} \\sum_{i \\in G}  \\E_{q_{\\phi_s}(S_i|X_i)} \\log p_\\theta(X_i | C_G, S_i) - \\\\ &amp;\\sum_{i \\in G} \\KL(q_{\\phi_s}(S_i\\Vert X_i)) -\\KL(q_{\\phi_c}(C_G|\\Xbf_G) \\Vert p(C_G)) \\end{align} \\] <p>Accumulating Evidence with Product of Gaussians. How do we infer the group content \\(q_{\\phi_c}(C_G|\\Xbf_G)\\)? The authors choose the product of Gaussian distribution</p> \\[ q_{\\phi_c}(C_G = c | \\Xbf_G = \\xbf_G) \\propto \\prod_{i\\in G} q_{\\phi_c}(C_G = c | X_i = x_i) \\] <p>since the product of Gaussians is still a Gaussian.</p> \\[ \\Sigma_G^{-1} = \\sum_{i\\in G} \\Sigma_i^{-1} \\qquad \\mu_G^\\top \\Sigma_G^{-1} = \\sum_{i \\in G} \\mu_i^\\top \\Sigma_i^{-1} \\] <p>Note that by increasing the number of observations in a group, the variance of the resulting distribution decreases. In other words, we accumulate evidence within a group to infer an accurate group content representation.</p> <p></p>"},{"location":"machine_learning/disentangled_representation_learning/#gvae","title":"GVAE","text":"<p>Grouped VAE (GVAE)<sup>23</sup> is very similar to ML-VAE. The only difference lies in how they calculate the group content representation. Instead of  accumulating evidence with product of Gaussians, GVAE averages the mean and variance of the Gaussians inferred from individual samples within a group.</p> \\[ q_{\\phi_c}(C_G = c | \\Xbf_G = \\xbf_G) = \\Ncal \\left( \\frac{1}{|\\Gcal|}\\sum_{k=1}^{|\\Gcal|} \\mu_\\phi(\\xbf_k), \\frac{1}{|\\Gcal|}\\sum_{k=1}^{|\\Gcal|} \\diag(\\sigma_\\phi(\\xbf_k)) \\right) \\] <p>The authors compared GVAE with ML-VAE and argues that ML-VAE has style-dependent coding of content, and thus has generally larger number of effective content dimensions<sup>23</sup>.</p>"},{"location":"machine_learning/disentangled_representation_learning/#f-statistics","title":"F-Statistics","text":""},{"location":"machine_learning/disentangled_representation_learning/#learning-from-paired-observations","title":"Learning from Paired Observations","text":""},{"location":"machine_learning/disentangled_representation_learning/#ada-mlvae-and-ada-gvae","title":"Ada-MLVAE and Ada-GVAE","text":"<p>Adaptive MLVAE/GVAE<sup>24</sup> considers learning disentangled representations with even weaker supervision. They try to learn from pairs of non-i.i.d. images sharing at least one of the underlying factors of variation. In their paper<sup>24</sup>, they first theoretically show that only knowing how many factors have changed, but not which ones, is sufficient to learn disentangled representations.</p> <p>Assuming the two observations \\(\\xbf_1\\) and \\(\\xbf_2\\) differ in \\(F\\) number of factors which form a set \\(\\bar{S} = [D] - S\\), the generative model is given by:</p> \\[ \\begin{align} &amp;p(\\zbf) = \\prod_{i=1}^D p(\\zbf_i) \\qquad p(\\widetilde{\\zbf}) = \\prod_{i=1}^F p(\\widetilde{z}_i) \\qquad S \\sim p(S) \\\\ &amp;\\xbf_1 = g^\\star(\\zbf) \\qquad \\xbf_2 = g^\\star(f(\\zbf, \\widetilde{\\zbf}, S)) \\end{align} \\] <p>where \\(g^\\star: \\Zcal \\to \\Xcal\\) is a mapping from the latent space to the observation space, and \\(f\\) is a function which selects entries from \\(\\zbf\\) with index in \\(S\\) and substitutes the remaining factors with \\(\\widetilde{\\zbf}\\).</p> <p>Note that the alignment constraints imposed by the generative model imply that for the true posterior, with probability 1,</p> \\[ p(z_i|\\xbf_1) = p(z_i|\\xbf_2), \\forall i \\in S \\qquad p(z_i|\\xbf_1) \\ne p(z_i|\\xbf_2), \\forall i \\in \\bar{S} \\] <p>We enforce these constraints on the approximate posterior \\(q_\\phi(\\widehat{\\zbf}|\\xbf)\\) of our learned model with the following steps:</p> <ul> <li> <p>Estimate \\(S\\). Choose for every pair \\((\\xbf_1, \\xbf_2)\\) the \\(D-F\\) factors with the smallest KL.</p> \\[\\delta_i :=\\KL( q_\\phi(\\widehat{z}_i|\\xbf_1) \\Vert q_\\phi(\\widehat{z}_i|\\xbf_2) )\\] <p>In the (practical) scenario where \\(F\\) is unknown, we use the threshold</p> \\[\\tau = \\frac{1}{2} (\\max_i \\delta_i + \\min_i \\delta_i)\\] <p>and choose \\(F\\) as the number of coordinates \\(i\\)'s satisfying \\(d_i &lt; \\tau\\).</p> </li> <li> <p>Replace each shared coordinate with some aggregation \\(a\\) of the two posteriors (as in GVAE or ML-VAE)</p> \\[ \\widetilde{q}_\\phi(\\widehat{z}_i|\\xbf_1) = \\widetilde{q}_\\phi(\\widehat{z}_i|\\xbf_2) = a(q_\\phi(\\widehat{z}_i|\\xbf_1), q_\\phi(\\widehat{z}_i|\\xbf_2)), \\forall i \\in \\widehat{S} \\] </li> <li> <p>Optimize the \\(\\beta\\)-VAE objective</p> \\[ \\max_{\\theta, \\phi} \\E_{(\\xbf_1, \\xbf_2)} \\sum_{j=1}^2 \\left[     \\E_{\\widetilde{q}_\\phi(\\widehat{\\zbf}|\\xbf_j)} \\log p_\\theta(\\xbf_j | \\widehat{\\zbf}) - \\beta \\KL( \\widetilde{q}_\\phi(\\widehat{\\zbf}|\\xbf_j) \\Vert p(\\widehat{\\zbf}) ) \\right] \\] </li> </ul> <p>The aggregation imposes a hard constraint which forces the shared dimensions to encode only one value, and thus implicitly enforces the non-shared dimensions to efficiently encode all the non-shared factors.</p> <ul> <li>CVPR 2021, Learning View-Disentangled Human Pose Representation by Contrastive Cross-View Mutual Information Maximization</li> </ul> <ol> <li> <p>A. Achille; S. Soatto, Emergence of Invariance and Disentanglement in Deep Representations, JMLR 2018. paper talk \u21a9</p> </li> <li> <p>F. Locatello et al., Challenging Common Assumptions in the Unsupervised Learning of Disentangled Representations, ICLR 2019. paper blog code \u21a9\u21a9\u21a9</p> </li> <li> <p>Y. Bengio; A. Courville; P. Vincent, Representation Learning: A Review and New Perspectives, IEEE-PAMI 2013.\u00a0\u21a9</p> </li> <li> <p>I. Higgins et al., \u03b2-VAE - Learning Basic Visual Concepts with a Constrained Variational Framework, ICLR 2017.\u00a0\u21a9\u21a9</p> </li> <li> <p>H. Kim; A. Mnih, Disentangling by Factorising, NeurIPS 2017.\u00a0\u21a9\u21a9\u21a9\u21a9\u21a9</p> </li> <li> <p>T. Q. Chen et al., Isolating Sources of Disentanglement in VAEs, NeurIPS 2018.\u00a0\u21a9\u21a9\u21a9</p> </li> <li> <p>K. Ridgeway; M. C. Mozer, Learning Deep Disentangled Embeddings With the F-Statistic Loss, NeurIPS 2018.\u00a0\u21a9</p> </li> <li> <p>Y. Bengio, From Deep Learning of Disentangled Representations to Higher-level Cognition, MSR AI Distinguished Lectures and Fireside Chats, 2018.\u00a0\u21a9</p> </li> <li> <p>A. Kumar; P. Sattigeri; A. Balakrishnan, Variational Inference of Disentangled Latent Concepts from Unlabeled Observations, ICLR 2018.\u00a0\u21a9\u21a9\u21a9\u21a9\u21a9</p> </li> <li> <p>C. Eastwood; C. K. I. Williams, A Framework for the Quantitative Evaluation of Disentangled Representations, ICLR 2018.\u00a0\u21a9</p> </li> <li> <p>C. P. Burgess et al., Understanding disentangling in beta-vae, Workshop on Learning Disentangled Representations, NeurIPS 2017.\u00a0\u21a9</p> </li> <li> <p>A. Makhzani et al., Adversarial Autoencoders, NeurIPS 2015.\u00a0\u21a9\u21a9</p> </li> <li> <p>S. Zhao; J. Song; S. Ermon, InfoVAE: Balancing Learning and Inference in Variational Autoencoders, AAAI 2019.\u00a0\u21a9</p> </li> <li> <p>Q. Liu; D. Wang, Stein Variational Gradient Descent: A General Purpose Bayesian Inference Algorithm, NeurIPS 2016.\u00a0\u21a9</p> </li> <li> <p>X. Chen et al., InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets, NeurIPS 2016.\u00a0\u21a9</p> </li> <li> <p>Z. Lin et al., InfoGAN-CR and ModelCentrality: Self-supervised Model Training and Selection for Disentangling GANs, ICML 2020.\u00a0\u21a9\u21a9</p> </li> <li> <p>N. Siddharth et al., Learning Disentangled Representations with Semi-Supervised Deep Generative Models, NeurIPS 2017.\u00a0\u21a9</p> </li> <li> <p>D. Bouchacourt; R. Tomioka; S. Nowozin, Multi-Level Variational Autoencoder: Learning Disentangled Representations from Grouped Observations, AAAI 2018.\u00a0\u21a9</p> </li> <li> <p>D. P. Kingma; D. J. Rezende; S. Mohamed; M. Welling. Semisupervised learning with deep generative models, NeurIPS 2014.\u00a0\u21a9\u21a9</p> </li> <li> <p>A. Gabbay; Y. Hoshen, Demystifying Inter-Class Disentanglement, ICLR 2020.\u00a0\u21a9</p> </li> <li> <p>Y. Hoshen; J. Malik, Non-Adversarial Image Synthesis with Generative Latent Nearest Neighbors, CVPR 2019.\u00a0\u21a9\u21a9</p> </li> <li> <p>F. Locatello et al, Disentangling Factors of Variation Using Few Labels, ICLR 2020.\u00a0\u21a9</p> </li> <li> <p>H. Hosoya, Group-based Learning of Disentangled Representations with Generalizability for Novel Contents, IJCAI 2019.\u00a0\u21a9\u21a9</p> </li> <li> <p>F. Locatello et al., Weakly-Supervised Disentanglement Without Compromises, PMLR 2020.\u00a0\u21a9\u21a9</p> </li> <li> <p>E. Denton; V. Birodkar, Unsupervised Learning of Disentangled Representations from Video, NeurIPS 2017.\u00a0\u21a9</p> </li> <li> <p>M. Mathieu et al., Disentangling factors of variation in deep representations using adversarial training, NeurIPS 2016.\u00a0\u21a9</p> </li> </ol>"},{"location":"machine_learning/equivariant_gnns/","title":"Equivariant GNNs","text":""},{"location":"machine_learning/equivariant_gnns/#preliminaries-group-theory-and-equivariance","title":"Preliminaries: Group Theory and Equivariance","text":"<p>This section is based on the exellent materials on equivariant neural networks by Andrew White.</p> <ul> <li> <p>Group \\(G = \\langle V, \\cdot \\rangle\\): a set \\(V\\) equipped with a binary operation \\(\\cdot\\) that satisfies closure, associativity, identity and inverse properties.</p> </li> <li> <p>Group action \\(\\pi(g, x)\\): \\(\\newcommand{\\cX}{\\mathcal{X}} G\\times\\cX \\to \\cX\\) that satisfies identity and compatibility, i.e.\\(\\pi(g, \\pi(h, x)) = \\pi(gh, x)\\).</p> </li> </ul> <p>Notes on the group elements</p> <p>We focus on groups of transformations here. E.g. \"rotate 60\u00b0 around the \\(z\\)-axis\" is an element of the \\(\\mathrm{SO}(3)\\) group, which operates on 3D points (\\(\\newcommand{\\R}{\\mathbb{R}}\\cX = \\R^3\\)).</p>"},{"location":"machine_learning/equivariant_gnns/#combining-translations-and-rotations","title":"Combining translations and rotations","text":"<ul> <li> <p>Left coset: \\(gH := \\{ gh : h \\in H \\le G \\}, \\forall g \\in G\\)</p> </li> <li> <p>Normal subgroup: \\(N \\triangleleft G \\Longleftrightarrow \\forall g \\in G, gN = Ng \\Longleftrightarrow \\forall n \\in N, \\forall g \\in G, gng^{-1} \\in N\\). Normal subgroups are invariant under conjugation.</p> <ul> <li>\\(\\forall g \\in G, gNg^{-1}\\) is an automorphism of N, i.e., \\(gNg^{-1} \\in \\mathrm{Aut}(N)\\).</li> </ul> </li> <li> <p>(Outer) semidirect product: Given two groups \\(N\\) and \\(H\\) and a group homomorphism \\(\\phi: H \\to \\mathrm{Aut}(N)\\), their outer semidirect product \\(N \\rtimes_\\phi H\\) is defined as:</p> <ul> <li>Underlying set: \\(N\\times H\\),</li> <li>Group operation: \\((n_1, h_1) \\cdot (n_2, h_2) = (n_1\\phi(h_1)(n_2), h_1h_2)\\).</li> </ul> <p>For \\(N \\triangleleft G\\) and \\(H \\le G\\), we can define \\(\\phi\\) as \\(\\phi(h) = \\phi_h\\) where \\(\\phi_h(n) = hnh^{-1}\\).</p> </li> </ul> <p>Example: \\(\\mathrm{SE}(3) = T(3) \\rtimes \\mathrm{SO}(3)\\)</p> <ul> <li> <p>We first show that \\(\\mathrm{T}(3) \\triangleleft \\mathrm{SE}(3)\\). \\(\\forall g \\in \\mathrm{SE}(3)\\), we represent \\(g\\) as \\(\\newcommand{\\vs}{\\mathbf{s}}\\begin{pmatrix}R&amp;\\vs\\\\0&amp;1\\end{pmatrix}\\), \\(\\forall t \\in \\mathrm{T}(3)\\), we represent \\(t\\) as \\(\\newcommand{\\vt}{\\mathbf{t}}\\begin{pmatrix}I_3&amp;\\vt\\\\0&amp;1\\end{pmatrix}\\), then \\(g^{-1}tg = \\begin{pmatrix}R^{-1}&amp;-\\vs\\\\0&amp;1\\end{pmatrix} \\begin{pmatrix}I_3&amp;\\vt\\\\0&amp;1\\end{pmatrix} \\begin{pmatrix}R&amp;\\vs\\\\0&amp;1\\end{pmatrix} = \\begin{pmatrix}I&amp;R^{-1}(\\vs+\\vt)-\\vs\\\\0&amp;1\\end{pmatrix} \\in \\mathrm{T}(3)\\).</p> </li> <li> <p>We can decompose \\(\\forall g = \\begin{pmatrix}R&amp;\\vt\\\\0&amp;1\\end{pmatrix} \\in \\mathrm{SE}(3)\\) into a translation \\(t = \\begin{pmatrix}I_3&amp;\\vt\\\\0&amp;1\\end{pmatrix}\\) and a rotation \\(r = \\begin{pmatrix}R&amp;0\\\\0&amp;1\\end{pmatrix}\\), s.t. \\((t, r)\\) is equivalent to \\(g\\). We now verify that \\(g_2g_1 = (t_2\\phi_{r_2}(t_1), r_2r_1)\\). Since \\(g_2g_1 = \\begin{pmatrix}R_2R_1&amp;R_2\\vt_1+\\vt_2\\\\0&amp;1\\end{pmatrix}\\), it suffices to check that \\(t_2\\phi_{r_2}(t_1) = t_2r_2t_1r_2^{-1} = \\begin{pmatrix}0&amp;R_2\\vt_1+\\vt_2\\\\0&amp;1\\end{pmatrix}\\) as \\(r_2r_1 = \\begin{pmatrix}R_2R_1&amp;0\\\\0&amp;1\\end{pmatrix}\\).</p> </li> </ul>"},{"location":"machine_learning/equivariant_gnns/#equivariance","title":"Equivariance","text":"<ul> <li> <p>Input data \\(\\newcommand{\\vr}{\\mathbf{r}}\\newcommand{\\vx}{\\mathbf{x}}f(\\vr) = \\vx\\): function \\(\\cX \\to \\R^n\\) where the domain \\(\\cX\\) is a homogeneous space (usually just the 3D Euclidean space) and the image is an \\(n\\)-dim feature space.</p> </li> <li> <p>\\(G\\)-function transform \\(\\newcommand{\\bbT}{\\mathbb{T}}\\bbT_g: \\cX^{\\R^n} \\to \\cX^{\\R^n}, f \\mapsto f^\\prime\\): Given a group element \\(g \\in G\\), where \\(G\\) is on the homogeneous space \\(\\cX\\), \\(G\\)-function transform \\(\\bbT_g\\) transforms \\(n\\)-dim functions on \\(\\cX\\) such that \\(f^\\prime(gx) = f(x)\\), i.e., \\(\\bbT_g(f)(x) = f^\\prime(x) = f(g^{-1}x)\\).</p> </li> </ul> <p>Example: Translation of an image</p> <p>Input function: \\(f(x, y) = (r, g, b)\\) that specifies the pixel values for three channels at each pixel position.</p> <p>Group element: \\(t_{10,0}\\) that stands for moving the image to the right by 10 pixels.</p> <p>Output of G-function transform associated with \\(t_{10,0}\\): \\(f^\\prime(x, y) = f(x-10, y)\\).</p> <ul> <li>\\(G\\)-Equivariant Neural Network \\(\\phi: \\cX^{\\R^n} \\to \\cX^{\\R^d}\\): Given a group element \\(g \\in G\\), where \\(G\\) is on the homogeneous space \\(\\cX\\), and \\(\\bbT_g\\) and \\(\\bbT^\\prime_g\\) on \\(n\\)- and \\(d\\)-dim functions, respectively, \\(\\phi\\) is a linear map such that \\(\\phi(\\bbT_g f(x)) = \\bbT^\\prime_g\\phi(f(x)),\\ \\forall f(x): \\cX \\to \\R^n\\).</li> </ul> <p>Briefly...</p> <p>\"Transform then encode\" has the same effect as \"encode then transform\".</p> <ul> <li>\\(G\\)-Equivariant Convolution Theorem: A neural network layer (linear map) \\(\\phi\\) is \\(G\\)-equivariant if and only if its form is a convolution operator \\(*\\):     $$\\newcommand{\\dd}{\\,\\mathrm{d}}     \\phi(f)(u) = (f * \\omega)(u) = \\int_G f^{\\uparrow G}(u g^{-1}) \\omega^{\\uparrow G}(g) \\dd \\mu(g)     $$     where \\(f: H \\to \\R^n\\) and \\(\\omega: H^\\prime \\to \\R^n\\) are functions on quotient spaces \\(H\\) and \\(H^\\prime\\), and \\(\\mu\\) is the group Haar measure.<ul> <li>Orbit of element \\(x_0\\): \\(Gx_0 = \\{gx_0: g \\in G\\}\\).</li> <li>Stabilizer subgroup for \\(x_0\\): \\(H_{x_0} = \\{g \\in G : gx_0 = x_0\\}\\).</li> <li>Orbit-stabilizer theorem: there is a bijection between the set of cosets \\(G/H_{x_0}\\) for the stabilizer subgroup and the orbit \\(Gx_0\\), which sends \\(gH_{x_0} \\mapsto gx_0\\).</li> <li>Lifting up \\(f\\): \\(f^{\\uparrow G}(g) = f(gx_0)\\).</li> <li>Projecting down \\(f\\): \\(f_{\\downarrow \\cX}(x) = \\frac{1}{|H_{x_0}|}\\int_{gH_{x_0}}f(u)\\dd \\mu(u)\\), where \\(g\\) is found by solving \\(gx_0 = x\\).</li> <li>Haar measure: a generalization of the familiar integrand factor you see when doing integrals in polar coordinates or spherical coordinates. E.g. the Haar measure for SO(3) with \\(R_zR_yR_z\\) group representation is \\(\\frac{1}{8\\pi^2}\\sin\\beta\\).</li> </ul> </li> </ul> <p>Briefly...</p> <p>There is only one way to achieve equivariance in a neural network.</p> <p>Example: SO(3)</p> <ul> <li>Function: defined on points on the sphere \\(f(x) = \\sum_i \\delta(x - x_i)\\), where \\(\\Vert x_i \\Vert_2 = 1\\).</li> <li>Group representation: \\(R_z(\\alpha) R_y(\\beta) R_z(\\gamma)\\).</li> <li>Stabilizer \\(x_0\\): \\((0, 0, 1)\\). Note that \\((0, 0, 0)\\) is not on the sphere.</li> <li>Stabilizer subgroup \\(H_{x_0}\\): rotations that only involve \\(\\gamma\\), i.e. \\(R_z(0) R_y(0) R_z(\\gamma)\\).</li> <li>Coset for \\(g = R_z(\\alpha^\\prime) R_y(\\beta^\\prime) R_z(\\gamma^\\prime)\\): \\(gH_{x_0} = \\{ R_z(\\alpha^\\prime) R_y(\\beta^\\prime) R_z(\\gamma^\\prime + \\gamma): \\gamma \\in [0, 2\\pi]\\}\\).</li> <li>Point in \\(\\cX\\) associated w/ this coset (by the orbit-stabilizer theorem): \\(gx_0 = R_z(\\alpha^\\prime) R_y(\\beta^\\prime) R_z(\\gamma^\\prime + \\gamma) x_0 = R_z(\\alpha^\\prime) R_y(\\beta^\\prime) x_0\\).</li> <li>Quotient space \\(G/H_{x_0}\\): \\(\\mathrm{SO}(2)\\).</li> <li>Lifted \\(f\\): Suppose \\(g = R_z(\\alpha) R_y(\\beta) R_z(\\gamma) \\in G\\), then \\(f^{\\uparrow G}(g) = f(gx_0) = f(R_z(\\alpha) R_y(\\beta) x_0)\\).</li> </ul>"},{"location":"machine_learning/equivariant_gnns/#group-representation","title":"Group Representation","text":"<ul> <li>Linear representation of a group: Let \\(G\\) be a group on an \\(n\\)-dimensional vector space \\(V\\). A linear representation of \\(G\\) is a group homomorphism: \\(\\newcommand{\\bbC}{\\mathbb{C}} \\rho: G \\to \\mathrm{GL}(m, \\bbC)\\) where \\(\\mathrm{GL}(\\bbC)\\) is the space of \\(m\\)-by-\\(m\\) square invertible matrices with complex numbers, that satisfies \\(\\rho(g_1g_2) = \\rho(g_1)\\rho(g_2)\\).<ul> <li>\\(m\\) is termed the degree of representation \\(\\rho\\).</li> <li>Faithful representation: There is a unique representation for every group element.</li> <li>Unitary representation: \\(\\forall g \\in G,\\ \\rho(g)\\rho^\\dagger(g) = I_m\\). Equivalently, \\(\\forall u, v \\in V, \\langle \\rho(g)u, \\rho(g)v\\rangle = \\langle u, v\\rangle\\).</li> </ul> </li> </ul> <p>Trivial Representation</p> <p>The trivial representation maps every group element to \\(I_m\\).</p> <p>Linear group actions are valid representations</p> <p>Suppose \\(\\pi(g, x)\\) is a linear group action on \\(V\\). This implies \\(V\\) is a vector space. Define \\(\\rho(g): V \\to V\\) as a transformation on \\(V\\) s.t. \\(\\rho(g)(x) = \\pi(g, x)\\), then \\(\\rho(g_1g_2) = \\rho(g_1)\\rho(g_2)\\). Also \\(\\forall x,\\ \\rho(e)(x) = \\pi(e, x) = x \\). Thus \\(\\forall x,\\ \\rho(g^{-1})(\\rho(g)(x)) = \\rho(e)(x) = x\\), i.e. \\(\\rho(g)\\) is invertible. Therefore, \\(\\rho\\) is a linear representation of group \\(G\\).</p> <ul> <li> <p>The Unitarity Theorem: we can always choose an invertible matrix \\(S\\) (for finite groups) such that the representation \\(\\rho^\\prime(g) = S^{-1}\\rho(g)S\\) is unitary.</p> <ul> <li>Without loss of generality, we can assume all representations we use are unitary or can be trivially converted to unitary.</li> </ul> </li> <li> <p>Irreducible Representation (irrep): a representation that has only trivial subrepresentations.</p> <ul> <li>A linear subspace \\(W \\subset V\\) is called \\(G\\)-invariant if \\(\\forall g \\in G\\) and \\(\\forall w \\in W\\), \\(\\rho(g)w \\in W\\).</li> <li>The co-restriction of \\(\\rho\\) to \\(\\mathrm{GL}(W)\\) is a subrepresentation.</li> <li>Composing irreps:     $$\\DeclareMathOperator{\\diag}{diag}     \\rho^\\prime(g) = S^{-1}\\rho(g)S = \\diag \\left(\\rho^{(1)}(g), \\dots, \\rho^{(k)}(g)\\right) = \\rho^{(1)}(g) \\oplus \\dots \\oplus \\rho^{(k)}(g)     $$</li> </ul> </li> <li> <p>Encoding input function into the irrep space: using the generalized Fourier transform,</p> \\[\\newcommand{\\N}{\\mathbb{N}} \\hat{f}(\\rho_i) = \\int_G \\rho_i(u)f^{\\uparrow G}(u) \\dd \\mu(u), \\quad i \\in \\N. \\] <p>where \\(\\rho_i\\) is the \\(i\\)-th irrep of \\(G\\), and \\(\\dd \\mu(u)\\) is the group Haar measure.</p> </li> <li> <p>Irreps on SO(3): The Wigner D-matrix is a unitary matrix in an irreducible representation of the groups SU(2) and SO(3).     $$     D_{m'm}^{l}(\\alpha ,\\beta ,\\gamma )\\equiv \\langle lm'|{\\mathcal {R}}(\\alpha ,\\beta ,\\gamma )|lm\\rangle =e^{-im'\\alpha }d_{m'm}^{l}(\\beta )e^{-im\\gamma }     $$</p> \\[ d_{m'm}^{l}(\\beta )=[(l+m')!(l-m')!(l+m)!(l-m)!]^{\\frac {1}{2}}\\sum _{s=s_{\\mathrm {min} }}^{s_{\\mathrm {max} }}\\left[{\\frac {(-1)^{m'-m+s}\\left(\\cos {\\frac {\\beta }{2}}\\right)^{2l+m-m'-2s}\\left(\\sin {\\frac {\\beta }{2}}\\right)^{m'-m+2s}}{(l+m-s)!s!(m'-m+s)!(l-m'-s)!}}\\right] \\] <p>where \\(l \\in \\N\\) for SO(3). \\(D_{m'm}^{l}\\) has shape \\((2l+1)\\times(2l+1)\\) and acts on space spanned by \\(l\\)-th degree polynomials. See here for a more detailed explanation.</p> <ul> <li> <p>We can now replace convolutions of group functions with products of their irreducible representations. This is critical, since SO(3) convolutions would require a series of spherical integrals.</p> </li> <li> <p>The first columns of the Wigner D-matrices are propto the spherical harmonics: \\(D_{m0}^{l}(\\alpha, \\beta, \\gamma) = \\sqrt{2l+1} {Y_l^m}(\\alpha, \\beta)\\).</p> </li> </ul> </li> </ul> <p></p> Spherical harmonics visualized on polar plots. The radius of the plot at a given polar and azimuthal angle represents the magnitude of the spherical harmonic, and the hue represents the phase."},{"location":"machine_learning/equivariant_gnns/#tensor-field-networks","title":"Tensor-Field Networks","text":"<p>Tensor-field network (TFN)<sup>1</sup> is a locally equivariant convolutional neural network.</p>"},{"location":"machine_learning/equivariant_gnns/#layer-overview","title":"Layer Overview","text":"<p>Tensor field networks act on points with associated features.</p> <p>Input-Output \u2003 A layer \\(\\newcommand{\\cL}{\\mathcal{L}}\\cL\\) takes a finite set \\(S\\) of vectors in \\(\\R^3\\) and a vector in \\(\\cX\\) at each point in \\(S\\) and outputs a vector in \\(\\newcommand{\\cY}{\\mathcal{Y}}\\cY\\) at each point in \\(S\\), where \\(\\cX\\) and \\(\\cY\\) are vector spaces:</p> \\[ \\cL(\\vr_a, x_a) = (\\vr_a, y_a). \\] <p>Let \\(D^\\cX\\) be a representation of SO(3) on a vector space \\(\\cX\\). The desired rotation equivariance can be written as</p> \\[\\newcommand{\\cR}{\\mathcal{R}} \\cL \\circ \\left[\\cR(g)\\oplus D^{\\cX}(g)\\right] = \\left[\\cR(g)\\oplus D^{\\cX}(g)\\right] \\circ \\cL, \\] <p>where \\(\\left[\\cR(g)\\oplus D^{\\cX}(g)\\right](\\vr_a, x_a) = \\left(\\cR(g)\\vr_a, D^\\cX(g)x_a\\right)\\).</p> <p>Decomposing representation \u2003 There are multiple instances (channels) of each \\(l\\)-th rotation-order irreducible representations. For rotation order \\(l\\), we implement a tensor \\(V^{(l)}_{acm}\\) with shape \\((|S|, C_l, 2l+1)\\), where \\(C_l\\) is the number of channels.</p> <p>Choice of \\(D^\\cX\\) \u2003 The group elements are represented by Wigner D-matrices \\(D^{(l)}\\), which map \\(g \\in \\mathrm{SO}(3)\\) to \\((2l+1)\\times (2l+1)\\) matrices. Note that \\(D^{0}(g) = 1\\) and \\(D^{(1)}(g) = \\cR(g)\\).</p> <p>The real sperical harnomics \\(Y^{(l)}_m\\) are equivariant to SO(3) (See G-function transform):</p> \\[ Y^{(l)}_m\\left(\\cR(g)\\hat{\\vr}\\right) = \\sum_{m^\\prime=-l}^lD^{(l)}_{mm^\\prime}(g)Y_{m^\\prime}^{(l)}(\\hat{\\vr}) \\]"},{"location":"machine_learning/equivariant_gnns/#point-convolution","title":"Point Convolution","text":"<p>Notation \\(\\vr\\) is the coordinates of an input point relative to the convolution center, \\(\\hat{\\vr}\\) is \\(\\vr\\) normalized to unit length, and \\(r = \\Vert\\vr\\Vert_2\\).</p> <p>Convolution filters \u2003 With \\(l_i\\) and \\(l_f\\) as the rotation orders of the input and filter, respectively, we define the following rotation-equivariant filter \\(F^{(l_f, l_i)}: \\R^3 \\to \\R^{(2l_f+1) \\times (2l_i+1)}\\):</p> \\[ F_{cm}^{(l_f, l_i)}(\\vr) = R_c^{(l_f, l_i)}(r)Y_m^{(l_f)}(\\hat{\\vr}) \\] <p>where \\(R_c^{(l_f, l_i)}: \\R_+ \\to \\R\\) is a learnable kernel.</p> <p>Combining representations w/ tensor products \u2003 We need the layer output to also be a representation of SO(3), thus we have to combine the filter outputs.</p> <p>The tensor product of two irreps \\(u^{l_1}\\) and \\(v^{l_2}\\) of orders \\(l_1\\) and \\(l_2\\) can be calculated as</p> \\[ (u \\otimes v)_m^{(l)} = \\sum_{m_1=-l_1}^{l_1}\\sum_{m_2=-l_2}^{l_2} C^{(l, m)}_{(l_1, m_1)(l_2, m_2)}u_{m_1}^{(l_1)}v_{m_2}^{(l_2)} \\] <p>where the \\(C^{(l, l_1, l_2)} \\in \\R^{(2l+1) \\times (2l_1+1) \\times (2l_2+1)}\\)s are Clebsch-Gordan coefficients. \\((u \\otimes v)_m^{(l)}\\) is non-zero only when \\(|l_1-l_2| \\le l \\le l_1+l_2\\) and \\(m = m_1 + m_2\\).</p> <p>Clebsch-Gordan coefficients</p> <p>For \\(0\\otimes 0 \\to 0\\), \\(C^{(0, 0)}_{(0, 0)(0, 0)} \\propto 1\\), which correspondns to scalar multiplication.</p> <p>For \\(1\\otimes 0 \\to 1\\), \\(C^{(1, i)}_{(1, j)(0, 0)} \\propto \\delta_{ij}\\), which corresponds to scalar multiplication of a vector.</p> <p>For \\(1\\otimes 1 \\to 0\\), \\(C^{(0, 0)}_{(1, i)(1, j)} \\propto \\delta_{ij}\\), which corresponds to dot product of vectors.</p> <p>For \\(1\\otimes 1 \\to 1\\), \\(C^{(1, i)}_{(1, j)(1, k)} \\propto \\epsilon_{ijk}\\), which corresponds to cross products of vectors.</p> <p>Layer Definition \u2003 A point convolution of a type-\\(l_f\\) filter on a type-\\(l_i\\) input yields outputs at \\(2\\min(l_i,l_f)+1\\) different rotation orders \\(l_o\\) (b/t \\(|l_i - l_f|\\) and \\((l_i+l_f)\\), inclusive):</p> \\[ \\cL_{acm_o}^{(l_o)}(\\vr_a, V_{acm_i}^{(l_i)}) = \\sum_{m_f, m_i}C^{(l_o, m_o)}_{(l_f, m_f)(l_i, m_i)}\\sum_{b \\in S} F_{cm_f}^{(l_f, l_i)}(\\vr_a - \\vr_b)V_{bcm_i}^{(l_i)}. \\] <p>What are we convolving?</p> <p>We are convolving the kernel function \\(W^{(l_o, l_i)}\\) with the input function \\(f^{(l_i)}\\), where the latter is represented as Dirac function taking non-zero values on \\(x_a\\)'s only, i.e. \\(f^{(l_i)}(\\vr) = \\sum_a V^{(l_i)}_{a} \\delta(\\vr - \\vr_a)\\):</p> \\[ \\cL^{l_o}_a(\\vr_a, V^{(l_i)}) = \\int_{x^\\prime}W^{(l_o, l_i)}(\\vr^\\prime - \\vr_a)f^{(l_i)}(\\vr) \\dd \\vr^\\prime = \\sum_{b \\in S} W^{(l_o, l_i)}(\\vr^\\prime - \\vr_a) V^{(l_i)}_{a}. \\] <p>The kernel lies in the span of an equivariant basis \\(\\{W^{(l_o, l_i)}_{l_f}\\}_{l_f=|l_o-l_i|}^{l_o+l_i}\\):</p> \\[ W^{(l_o, l_i)}(\\vr) = \\sum_{l_f}^{} R(r)W^{(l_o, l_i)}_{l_f}(\\hat{\\vr}), \\qquad \\text{where}\\  W^{(l_o, l_i)}_{l_f}(\\hat{\\vr}) = \\sum_{m_f} Y_{m_f}^{(l_f)}(\\hat{\\vr})C^{(l_o, l_i, l_f)}_{\\cdot \\cdot m_f}. \\]"},{"location":"machine_learning/equivariant_gnns/#self-interaction","title":"Self-Interaction","text":"<p>Self-interaction layers mix the components of the feature vectors of the same type within each point together. They are analogous to 1x1 convolutions, and they act like \\(l_f = 0\\) (scalar) filters. Suppose \\(c^\\prime\\) and \\(c\\) are the input and output dimensions,</p> \\[ \\sum_{c^\\prime}W^{(l)}_{cc^\\prime}V^{(l)}_{ac^\\prime m}. \\] <p>The same weights are used for every \\(m\\) for a given order \\(l\\). Thus, the \\(D\\)-matrices commute with the weight matrix \\(W\\), i.e. this layer is equivariant for \\(l &gt; 0\\).</p> <p>Equivariance for \\(l = 0\\) is straightforward because \\(D(0) = 1\\) (we may also use biases in this case).</p>"},{"location":"machine_learning/equivariant_gnns/#non-linearity","title":"Non-Linearity","text":"<p>The non-linearity layer acts as a scalar transform in the \\(l\\) spaces, i.e. along the \\(m\\) dimension.</p> \\[ \\begin{cases} \\eta^{(0)}\\left( V_{ac}^{(0)} + b_c^{(0)} \\right), &amp; l=0; \\\\ \\eta^{(l)}\\left( \\Vert V \\Vert_{ac}^{(l)} + b_c^{(l)} \\right) V_{acm}^{(l)}, &amp; l&gt;0. \\end{cases} \\] <p>where \\(\\Vert V \\Vert_{ac}^{(l)} = \\sqrt{\\sum_m (V_{acm}^{(l)})^2}\\) and \\(\\eta^{(l)}: \\R \\to \\R\\).</p> <p>Since \\(D\\) unitary, \\(\\Vert D(g)V\\Vert = \\Vert V\\Vert\\), i.e. this layer is equivariant.</p>"},{"location":"machine_learning/equivariant_gnns/#se3-transformers","title":"SE(3)-Transformers","text":"<p>SE(3)-Transformer<sup>2</sup> is an equivariant attention-based model for 3D point cloud / graph data. It is basically a GAT with TFN weight matrices.</p> <p>Compared to TFN, SE(3)-Transformers (1) allow a natural handling of edge features, (2) allow a nonlinear equivariant layer and (3) relieve the strong angular constraints on the filter (TFN filters only have learnable parameters in the angular direction).</p> <p></p> SE(3) Transformer layer."},{"location":"machine_learning/equivariant_gnns/#layer-overview_1","title":"Layer Overview","text":"<p>Recall that, given rotation orders \\(k\\) and \\(l\\), an \\(k\\)-to-\\(l\\) TFN layer (w/o nonlinearity) can be written as</p> \\[\\newcommand{\\mW}{\\mathbf{W}}\\newcommand{\\mQ}{\\mathbf{Q}} \\newcommand{\\vf}{\\mathbf{f}} \\vf^{l}_{\\text{out}, i} = w^{ll}\\vf^l_{\\text{in}, i} + \\sum_{k \\ge 0} \\sum_{j \\ne i}^n \\mW^{lk}(\\vx_j - \\vx_i) \\vf^k_{\\text{in}, j} \\] <p>where \\(\\mW^{lk}(\\vx) = \\sum_{J=|k-l|}^{k+l}R^{lk}_J(\\Vert\\vx\\Vert)\\mW_J^{lk}(\\vx)\\) and \\(\\mW_J^{lk}(\\vx) = \\sum_{m=-J}^J Y^J_m(\\hat{\\vx})\\mQ^{lkJ}_{m}\\). Here \\(k\\), \\(l\\), \\(J\\) correspond to \\(l_i\\), \\(l_o\\) and \\(l_f\\) in the TFN notations. The first term denotes self-interaction, or 1x1 convolution, and the second term denotes convolution.</p> <p>The SE(3)-Transformer layer is defined as (\\(\\bigoplus\\) is the direct sum, i.e. vector concatenation):</p> \\[\\begin{align} \\newcommand{\\cN}{\\mathcal{N}} \\newcommand{\\mS}{\\mathbf{S}} \\newcommand{\\vq}{\\mathbf{q}} \\newcommand{\\vk}{\\mathbf{k}} \\DeclareMathOperator{\\softmax}{softmax} \\vq_i &amp;= \\bigoplus_{l \\ge 0}\\sum_{k \\ge 0}\\mW_Q^{lk} \\vf_{\\text{in}, i}^k \\\\ \\vk_{ij} &amp;= \\bigoplus_{l \\ge 0}\\sum_{k \\ge 0} \\mW_K^{lk}(\\vx_j-\\vx_i) \\vf^k_{\\text{in}, j} \\\\ \\alpha_{ij} &amp;= \\underset{j^\\prime \\in \\cN_i \\backslash i}{\\softmax} \\left( \\vq_i^\\top \\vk_{ij} \\right) \\\\ \\vf^{l}_{\\text{out}, i} &amp;= \\mW^{ll}_V \\vf^l_{\\text{in}, i} + \\sum_{k \\ge 0} \\sum_{j \\in \\cN_i \\backslash i} \\alpha_{ij} \\mW^{lk}_V (\\vx_j - \\vx_i) \\vf^k_{\\text{in}, j}. \\end{align}\\] <p>Similarly, the first term denotes self-interaction and the second term denotes attention. Equivariance is obvious as the attention weights are invariant (due to orthonormality of representations of SO(3), \\((\\mS_g\\vq)^\\top\\mS_g\\vk=\\vq^\\top\\vk\\)) and the value embeddings are equivariant w.r.t rototranslation.</p>"},{"location":"machine_learning/equivariant_gnns/#self-interaction_1","title":"Self-Interaction","text":"<p>Self-interaction is an elegant form of learnable skip connection. It is crucial since, in the SE(3)-Transformer, points do not attend to themselves. The authors propose two types of self-interaction layer, both of the form</p> \\[ \\vf^{\\text{out}, i, c^\\prime} = \\sum_c w^{ll}_{i, c^\\prime, c} \\vf^{l}_{\\text{in}, i, c} \\] <p>Linear \u2003 Same as TFN, weights per order are shared across all points. The self-interaction layer is followed by a norm-based non-linearity. Or, in math language, \\(w_{i, c^\\prime, c}^{ll} = w_{c^\\prime, c}^{ll}\\).</p> <p>Attentive \u2003 Weights are generated from an MLP. This is an invariant layer due to the invariance of inner products.</p> \\[ w_{i, c^\\prime, c}^{ll} = \\mathrm{MLP}\\, \\left(     \\bigoplus_{c, c^\\prime} \\vf^{l\\top}_{\\text{in}, i, c^\\prime} \\vf^l_{\\text{in}, i, c} \\right) \\] <ol> <li> <p>N. Thomas and T. Smidt et al. Tensor field networks: Rotation- and translation-equivariant neural networks for 3D point clouds. NeurIPS 2018. paper code \u21a9</p> </li> <li> <p>F. B. Fuchs, D. E. Worrall, V. Fisher, M. Welling. SE(3)-Transformers: 3D Roto-Translation Equivariant Attention Networks. NeurIPS 2020. paper \u21a9</p> </li> </ol>"},{"location":"machine_learning/graph_neural_networks/","title":"Graph Neural Networks","text":""},{"location":"machine_learning/graph_neural_networks/#preliminaries","title":"Preliminaries","text":""},{"location":"machine_learning/graph_neural_networks/#permutation-in-equi-varience-on-graphs","title":"Permutation In-/Equi-varience on Graphs","text":"<p>Let \\(\\mathbf{P}\\) be a permutation matrix.</p> <p>Invariance \u2003 Invariant function outputs the same value for all permutations of the node index. This is required by graph-level tasks.</p> \\[ f(\\mathbf{PX}, \\mathbf{PAP}^\\top) = f(\\mathbf{X}, \\mathbf{A}) \\] <p>Equivariance \u2003 Equivariant function outputs the permuted value for all permutations of the node index. This is required by node-/edge-level tasks.</p> \\[ f(\\mathbf{PX}, \\mathbf{PAP}^\\top) = \\mathbf{P}f(\\mathbf{X}, \\mathbf{A}) \\]"},{"location":"machine_learning/graph_neural_networks/#general-convolution","title":"General Convolution","text":""},{"location":"machine_learning/graph_neural_networks/#continuous-1-d-convolution","title":"Continuous 1-D Convolution","text":"\\[ \\DeclareMathOperator{\\mean}{\\mathrm{mean}} \\DeclareMathOperator{\\diag}{\\mathrm{diag}} \\DeclareMathOperator{\\relu}{\\mathrm{ReLU}} \\DeclareMathOperator{\\elu}{\\rm{E{\\small LU}}} \\DeclareMathOperator{\\leakyrelu}{\\mathrm{LeakyReLU}} \\DeclareMathOperator{\\batchnorm}{\\mathrm{BN}} \\DeclareMathOperator{\\softmax}{\\mathrm{softmax}} \\DeclareMathOperator{\\msg}{\\rm M{\\small SG}} \\DeclareMathOperator{\\agg}{\\rm A{\\small GG}} \\DeclareMathOperator{\\mlp}{\\rm{M{\\small LP}}} \\DeclareMathOperator{\\ltwonorm}{\\mathscr{l}_2\\mathrm{-norm}} \\def\\dd{\\mathrm{d}} (f * g)(t) := \\int_{-\\infty}^{\\infty} f(\\tau) g(t-\\tau) \\dd\\tau \\]"},{"location":"machine_learning/graph_neural_networks/#discrete-1-d-convolution","title":"Discrete 1-D Convolution","text":"\\[ (f * g)[n] := \\sum_{m=-\\infty}^{\\infty} f[m] g[n-m] \\] <p>This can be viewed as a multiplication with a circulant matrix.</p> \\[ \\begin{pmatrix} \\dots \\\\ h_{0} \\\\ h_{1} \\\\ h_{2} \\\\ \\dots \\end{pmatrix} =  \\begin{pmatrix}     \\dots &amp; \\dots &amp;  \\dots &amp;  \\dots &amp; \\dots \\\\     \\dots &amp; g_{0} &amp; g_{-1} &amp; g_{-2} &amp; \\dots \\\\     \\dots &amp; g_{1} &amp;  g_{0} &amp; g_{-1} &amp; \\dots \\\\     \\dots &amp; g_{2} &amp;  g_{1} &amp;  g_{0} &amp; \\dots \\\\     \\dots &amp; \\dots &amp;  \\dots &amp;  \\dots &amp; \\dots \\end{pmatrix} \\begin{pmatrix} \\dots \\\\ f_{0} \\\\ f_{1} \\\\ f_{2} \\\\ \\dots \\end{pmatrix} \\] <p>where \\(h = f * g\\). For finite-length \\(f\\) and \\(g\\), we could diagonize the circulant matrix</p> \\[ \\mathbf{h} = \\mathbf{\\Phi}\\diag(\\hat{g}_1, \\dots, \\hat{g}_n)\\mathbf{\\Phi}^\\top \\mathbf{f} = \\mathbf{\\Phi}(\\mathbf{\\Phi}^\\top \\mathbf{f})\\circ(\\mathbf{\\Phi}^\\top \\mathbf{g}). \\] <p>A more detailed tutorial on circulant matrix and its relation with Fourier transforms can be found here.</p>"},{"location":"machine_learning/graph_neural_networks/#discrete-2-d-convolution","title":"Discrete 2-D Convolution","text":"<p>$$ (f * g)[i, j] := \\sum_{a, b} f[a, b] g[i-a, j-b] $$ With \\((n_h, n_w)\\) input, \\((k_h, k_w)\\) kernel, \\((p_h, p_w)\\) padding and \\((s_h, s_w)\\) stride, the output shape will be</p> \\[\\lfloor(n_h-k_h+p_h+s_h)/s_h\\rfloor \\times \\lfloor(n_w-k_w+p_w+s_w)/s_w\\rfloor.\\]"},{"location":"machine_learning/graph_neural_networks/#spectral-graph-theory","title":"Spectral Graph Theory","text":"<p>We refer the readers to this elegant tutorial (part1, part2), which is a subset of Stanford's  CS 168: The Modern Algorithmic Toolbox  course.</p>"},{"location":"machine_learning/graph_neural_networks/#graph-convolution","title":"Graph Convolution","text":"<p>There are two approaches to defining graph convolution: spectral methods and spatial methods <sup>1</sup>. The former defines convolution through graph Fourier transform, and the latter defines convolution as aggregating messages coming from the neighborhood.</p>"},{"location":"machine_learning/graph_neural_networks/#spectral-graph-convolution","title":"Spectral Graph Convolution","text":"<p>Let \\(G = \\langle V=[n], E, \\mathbf{W} \\rangle\\) be an undirected weighted graph. The unnormalized graph Laplacian is \\(\\mathbf{L} = \\mathbf{D} - \\mathbf{W}\\), where the degree matrix \\(\\mathbf{D} = \\diag \\big( \\sum_{j \\ne i} w_{ij} \\big)\\).</p> <p>The Laplacian has an eigendecomposition \\(\\mathbf{L} = \\mathbf{\\Phi}\\mathbf{\\Lambda}\\mathbf{\\Phi}^\\top\\). Changing the eigenvalues in \\(\\mathbf{\\Lambda}\\) expresses any operation that commutes with \\(\\mathbf{L}\\). Given a graph signal \\(\\mathbf{f} = (f_1, \\dots, f_n)^\\top\\), its graph Fourier transform is given by \\(\\hat{\\mathbf{f}} = \\mathbf{\\Phi}^\\top \\mathbf{f}\\). The spectral convolution of two graph signals is defined as (transform \u2192 multiplication \u2192 reverse transform)</p> \\[ \\mathbf{f}*\\mathbf{g} = \\mathbf{\\Phi}\\diag(\\hat{g}_1, \\dots, \\hat{g}_n)\\mathbf{\\Phi}^\\top \\mathbf{f} = \\mathbf{\\Phi}(\\mathbf{\\Phi}^\\top \\mathbf{f})\\circ(\\mathbf{\\Phi}^\\top \\mathbf{g}). \\] <p>Why can't we define spectral graph convolution in the spatial domain as in here?</p> <p>Because there\u2019s no notion of space or direction inherent to a graph.</p> <p>To get anisotropy back, one can use natural edge features if applicable, or adopt mechanisms invariant by index permutation yet treat neighbors differently, e.g. node degrees (MoNet), edge gates (Gated GCN) and attention (GAT).</p> <p>Though theoretically elegant, these methods suffer from several drawbacks, namely</p> <ul> <li>Filters are basis-dependent \u2192 only applies to transductive setting</li> <li>\\(O(n)\\) parameters per layer</li> <li>\\(O(n^2)\\) computation of GFT and IGFT</li> <li>No guarantee of spatial localization of filters</li> </ul> <p>As shown above, directly learning the eigenvalues \\(\\diag(\\hat{g}_1, \\dots, \\hat{g}_n)\\) is typically inappropriate. We instead \"spatialize\" the spectral graph convolution by learning a function of the eigenvalues of the Laplacian. Observe that in the Euclidean setting, localization in space is equivalent to smoothness in the frequency domain</p> \\[ \\int_{-\\infty}^\\infty \\left|x\\right|^{2k} \\left|f(x)\\right|^2 \\dd x = \\int_{-\\infty}^\\infty \\left|\\frac{\\partial^k \\hat{f}(\\omega)}{\\partial \\omega^k}\\right|^2 \\dd \\omega. \\] <p>To encourage smooth localization of filters and reduce computational cost, we parameterize the filter \\(\\mathbf{g}\\) using a smooth spectral transfer function \\(\\tau(\\lambda)\\). In this way, application of the filter becomes</p> \\[ \\mathbf{f}*\\mathbf{g} = \\tau(\\mathbf{L})\\mathbf{f} = \\mathbf{\\Phi} \\tau(\\mathbf{\\Lambda}) \\mathbf{\\Phi}^\\top \\mathbf{f} \\] <p>where \\(\\tau(\\mathbf{\\Lambda}) = \\diag(\\tau(\\lambda_1), \\dots, \\tau(\\lambda_n))\\). If we parameterize \\(\\tau\\) as a \\(K\\)-th order polynomial, this expression is now \\(K\\)-localized since the \\(K\\)-th order polynomial of the Laplacian \\(\\mathbf{L}\\) depends only on the \\(K\\)-th order neighborhood of each node. It also enjoys \\(O(1)\\) parameters per layer and \\(O(NK)\\) computational complexity.</p>"},{"location":"machine_learning/graph_neural_networks/#spatial-graph-convolution","title":"Spatial Graph Convolution","text":"<p>Define neighborhood of node \\(i\\) as \\(\\mathcal{N}(i) := \\{j : (i,j) \\in E\\}\\), and the extended neighborhood of node \\(i\\) as \\(\\tilde{\\mathcal{N}}(i) := \\mathcal{N}(i) \\cup \\{i\\}\\). A message passing GNN can be generalized as:</p> \\[ \\begin{align} \\mathbf{m}^{l+1}_u &amp;= \\msg^{l} (\\mathbf{h}^{l}_u) \\\\ \\mathbf{h}^{l+1}_v &amp;= \\agg^{l} (\\{\\mathbf{m}_u^{l+1}: u \\in \\mathcal{N}(v) \\}, \\mathbf{h}^{l}_v ). \\end{align} \\] <p>In the MoNet paper, the author defined another general form for spatial graph convolution. For each edge \\((x, y) \\in E\\), they define a \\(d\\)-dimensional pseudo-coordinates \\(\\mathbf{u}(x, y)\\) similar to the \\(\\msg\\) function in the above definition. They also define a \\(\\mathbf{\\Theta}\\)-parameterized weighting kernel \\(w_{\\mathbf{\\Theta}} = (w_1(\\mathbf{u}), \\dots, w_K(\\mathbf{u}))\\), where \\(K\\) is the number of kernels. The patch operator can therefore be written as</p> \\[ \\mathcal{D}_k(x)f = \\sum_{y \\in \\mathcal{N}(x)} w_k\\big( \\mathbf{u}(x, y) \\big) f(y),\\quad k \\in [K]. \\] <p>A spatial generalization of the convolution on non-Euclidean domains is then given by</p> \\[ (f*g)(x) = \\sum_{k=1}^K g_k \\mathcal{D}_k(x)f \\] <p>Several CNN-type geometric deep learning methods on graphs and manifolds can be obtained as a particular setting of MoNet. \\(\\bar{\\mathbf{u}}_k\\), \\(\\bar{\\sigma}_\\rho\\), \\(\\bar{\\sigma}_\\theta\\) denote fixed parameters of the weight functions.</p> Method Pseudo-coordinates \\(\\mathbf{u}(x, y)\\) Weight function \\(w_k(\\mathbf{u})\\) CNN \\(\\mathbf{x}(y) - \\mathbf{x}(x)\\) \\(\\delta(\\mathbf{u}-\\bar{\\mathbf{u}}_k)\\) GCN \\(\\deg(x), \\deg(y)\\) \\(\\frac{1}{\\sqrt{u_1u_2}}\\) Geodesic CNN \\(\\rho(x,y), \\theta(x,y)\\) \\(\\exp \\bigg(-\\frac{1}{2}(\\mathbf{u}-\\bar{\\mathbf{u}}_k)^\\top \\Big(\\begin{smallmatrix} \\bar{\\sigma}_\\rho^2 &amp; \\\\ &amp; \\bar{\\sigma}_\\theta^2 \\end{smallmatrix}\\Big)^{-1} (\\mathbf{u}-\\bar{\\mathbf{u}}_k) \\bigg)\\) MoNet \\(\\tanh \\bigg(\\mathbf{A} \\Big(\\begin{smallmatrix} \\deg(x)^{-1/2} \\\\ \\deg(y)^{-1/2} \\end{smallmatrix}\\Big) +\\mathbf{b}\\bigg)\\) \\(\\exp \\bigg( -\\frac{1}{2} (\\mathbf{u}-\\mathbf{\\mu}_k)^\\top \\Sigma_k^{-1} (\\mathbf{u}-\\mathbf{\\mu}_k) \\bigg)\\)"},{"location":"machine_learning/graph_neural_networks/#relation-between-spectral-and-spatial-methods","title":"Relation between Spectral and Spatial Methods","text":"Notation ChebNet filter Spatial filter vector \\(\\mathbf{h} = \\sum_{k=0}^K \\alpha_k \\mathbf{L}^k \\mathbf{f}\\) \\(\\mathbf{h} = (\\mathcal{D}f)\\mathbf{g}\\) element \\(h_i = \\sum_{k=0}^K \\alpha_k (\\mathbf{L}^k \\mathbf{f})_i\\) \\(h_i = \\sum_{k=1}^K g_k (\\mathcal{D}f)_i\\) <p>ChebNet is a particular setting of spatial convolution with local weighting functions given by the powers of the Laplacian \\(w_k(x, y) = \\mathbf{L}^{k-1}_{x,y}\\).</p>"},{"location":"machine_learning/graph_neural_networks/#gnn-vs-random-walk-models","title":"GNN vs. Random Walk Models","text":"<p>Random walk objectives inherently capture local similarities From a representation perspective, random walk node embedding models emulate a convolutional GNN.</p> <p>Corollary 1: Random-walk objectives can fail to provide useful signal to GNNs.</p> <p>Corollary 2: At times, DeepWalk can be matched by an untrained conv-GNN.</p>"},{"location":"machine_learning/graph_neural_networks/#message-passing-gcns","title":"Message Passing GCNs","text":"<p>Message passing Neural Networks (MPNNs) can be divided into isotropic and anisotropic GCNs by whether the node update function \\(\\agg\\) treats every edge direction equally. The terminology of MPNNs and WL-GNNs and isotropic and anisotropic GNNs are from <sup>4</sup>.</p>"},{"location":"machine_learning/graph_neural_networks/#isotropic-gcns","title":"Isotropic GCNs","text":""},{"location":"machine_learning/graph_neural_networks/#gcn","title":"GCN","text":"<p>2017 ICLR - Semi-Supervised Classification with Graph Convolutional Networks <sup>5</sup></p> \\[ \\mathbf{h}^{l+1}_v = \\relu \\big( \\mathbf{U}^l \\mean \\{ \\mathbf{h}^l_u : u \\in \\mathcal{N}_v \\}\\big) \\] <p>In a normalized version, the \\(\\mean\\) operator becomes </p> \\[ \\mean \\{ \\mathbf{h}^l_u : u \\in \\mathcal{N}_v \\} = \\sum_{u \\in \\mathcal{N}_v} \\frac{\\tilde{w}_{ij}}{\\sqrt{\\tilde{d}_i}\\sqrt{\\tilde{d}_j}} \\mathbf{h}^l_u \\] <p>with \\(\\tilde{\\mathbf{W}} = \\mathbf{W} + \\mathbf{I}\\) and \\(\\tilde{\\mathbf{D}} = \\diag\\big( \\sum_{j} \\tilde{w}_{ij} \\big)\\).</p>"},{"location":"machine_learning/graph_neural_networks/#graphsage","title":"GraphSAGE","text":"<p>2017 NeurIPS - Inductive Representation Learning on Large Graphs <sup>6</sup></p> \\[ \\mathbf{h}^{l+1}_v = \\ltwonorm \\bigg( \\relu \\Big(     \\mathbf{U}^l \\big[         \\mathbf{h}^l_v \\Vert \\mean \\left\\{             \\mathbf{h}^l_u : u \\in \\mathcal{N}_v         \\right\\}     \\big] \\Big) \\bigg) \\]"},{"location":"machine_learning/graph_neural_networks/#anisotropic-gcns","title":"Anisotropic GCNs","text":""},{"location":"machine_learning/graph_neural_networks/#gat","title":"GAT","text":"<p>2018 ICLR - Graph Attention Networks <sup>7</sup></p> <p>\\(K\\)-head attention.</p> \\[ \\begin{align} \\alpha_{ij}^{k,l} &amp;= \\softmax_{j \\in \\tilde{\\mathcal{N}}(i)} \\bigg(     \\leakyrelu \\Big(          {\\mathbf{a}^{k,l}}^{\\top} [\\mathbf{U}^{k,l}\\mathbf{h}^l_i \\Vert \\mathbf{U}^{k,l}\\mathbf{h}^l_j]      \\Big)  \\bigg) \\\\ \\mathbf{h}^{l+1}_i &amp;= \\Vert_{k=1}^K \\bigg( \\elu \\Big(     \\sum_{j \\in \\tilde{\\mathcal{N}}(i)} \\alpha_{ij}^{k,l}\\mathbf{U}^{k,l}\\mathbf{h}^l_j  \\Big) \\bigg) \\end{align} \\]"},{"location":"machine_learning/graph_neural_networks/#monet","title":"MoNet","text":"<p>2017 CVPR - Geometric deep learning on graphs and manifolds using mixture model CNNs <sup>8</sup></p> \\[ \\begin{align} u_{ij}^l &amp;= \\tanh \\bigg(\\mathbf{A} \\begin{pmatrix} \\deg(x)^{-1/2} \\\\ \\deg(y)^{-1/2} \\end{pmatrix} +\\mathbf{b}\\bigg) \\\\ w_{ij}^{kl} &amp;= \\exp \\bigg( -\\frac{1}{2} (\\mathbf{u}-\\mathbf{\\mu}_k)^\\top \\Sigma_k^{-1} (\\mathbf{u}-\\mathbf{\\mu}_k) \\bigg) \\\\ \\mathbf{h}^{l+1}_i &amp;= \\relu \\Big( \\sum_{k=1}^K      \\sum_{j \\in \\mathcal{N}(i)} w_{ij}^{k,l}\\mathbf{U}^{k,l}\\mathbf{h}^l_j \\Big) \\end{align} \\]"},{"location":"machine_learning/graph_neural_networks/#gatedgcn","title":"GatedGCN","text":"<p>2018 ICLR - Residual Gated Graph ConvNets <sup>9</sup></p> \\[ \\begin{align} e_{ij}^{l+1} &amp;= \\softmax_{j \\in \\mathcal{N}(i)} \\bigg( \\hat{e}_{ij}^{l} + \\relu \\Big( \\batchnorm \\big(     \\mathbf{A}^l\\mathbf{h}_i^l + \\mathbf{B}^l\\mathbf{h}_j^l + \\mathbf{C}^l\\hat{e}_{ij}^{l} \\big) \\Big) \\bigg) \\\\ h_i^{l+1} &amp;= h_i^l + \\relu \\Big( \\batchnorm \\big(     \\mathbf{U}^l\\mathbf{h}_i^l +     \\sum_{j \\in \\mathcal{N}_i} e^{l+1}_{ij} \\odot \\mathbf{V}^l\\mathbf{h}_j^l \\big) \\Big)  \\\\ \\end{align} \\]"},{"location":"machine_learning/graph_neural_networks/#weisfeiler-lehman-gnns","title":"Weisfeiler-Lehman GNNs","text":"<p>This line of research <sup>2</sup> is based on the Weisfeiler-Lehman (WL) graph isomorphism test, which aims to develop provably expressive GNNs.</p>"},{"location":"machine_learning/graph_neural_networks/#gin","title":"GIN","text":"<p>2019 ICLR - How Powerful are Graph Neural Networks <sup>10</sup></p> <p>The GIN architecture is a provable 1-WL GNN based on the Weisfeiler-Lehman Isomorphism Test.</p> \\[ \\mathbf{h}_i^{l+1} = \\relu \\Big( \\mlp^l \\big( (1+\\epsilon) \\mathbf{h}_i^l + \\sum_{j \\in \\mathcal{N}_i} \\mathbf{h}_j^l \\big) \\Big) \\] <p>A small improvement that incorporates edge features has the following form:</p> \\[ \\mathbf{h}_i^{l+1} = \\relu \\bigg( \\mlp^l \\Big(     (1+\\epsilon) \\mathbf{h}_i^l + \\sum_{j \\in \\mathcal{N}_i} \\relu \\big( \\mathbf{h}_j^l + \\mathbf{e}_{ij}^l \\big) \\Big) \\bigg) \\] <p>GIN also proposed an injective graph-level readout</p> \\[ \\mathbf{h}_G = \\Vert_{l=0}^L \\sum_{v \\in V} \\mathbf{h}_v^l \\]"},{"location":"machine_learning/graph_neural_networks/#3wl-gnn","title":"3WL-GNN","text":"<p>2019 NeurIPS - Provably powerful graph networks <sup>11</sup></p> <p>3-WL GNNs uses rank-2 tensors (\\(n \\times n \\times d\\)) while being 3-WL provable. This 3-WL model improves the space/time complexities of \\(k\\)-GNN <sup>3</sup> from \\(O(n^3)\\)/\\(O(n^4)\\) to \\(O(n^2)\\)/\\(O(n^3)\\) respectively.</p> <p>We first introduce the \\(n \\times n \\times (1 + d_{\\mathrm{node}} + d_{\\mathrm{edge}})\\) input tensor</p> \\[\\mathbf{h}^0 = \\big[ A \\Vert \\mathbf{X}^{(\\mathrm{node})} \\Vert \\mathbf{X}^{(\\mathrm{edge})} \\big]\\] <p>where \\(\\mathbf{X}^{(\\mathrm{node})}_{i,i,:}\\) is the \\(i\\)-th node feature, and \\(\\mathbf{X}^{(\\mathrm{edge})}_{i,j,:}\\) is the \\((i,j)\\)-th edge feature. The model is defined as</p> \\[ \\mathbf{h}^{l+1} = \\big[     \\mlp_1(\\mathbf{h}^{l})\\mlp_2(\\mathbf{h}^{l})     \\Vert     \\mlp_3(\\mathbf{h}^{l}) \\big] \\] <p>and implemented (I don't know why, either) as</p> \\[ \\mathbf{h}^{l+1} = \\mlp_3\\Big( \\big[     \\mlp_1(\\mathbf{h}^{l})\\mlp_2(\\mathbf{h}^{l})     \\Vert     \\mathbf{h}^{l} \\big] \\Big) \\] <p>where the tensor multiplication is defined as an einsum of <code>ipk,pjk-&gt;ijk</code>, i.e., per-feature matrix multiplication.</p> <ol> <li> <p>TowardsDataScience blogpost - Graph Convolutional Networks for Geometric Deep Learning; NeurIPS 2017 Tutorial - Geometric Deep Learning slides video \u21a9</p> </li> <li> <p>Invariant Graph Networks, ICML 2019 Workshop - Learning and Reasoning with Graph-Structured Representations\u00a0\u21a9</p> </li> <li> <p>Graph Neural Networks and Graph Isomorphism, ICML 2019 Workshop - Learning and Reasoning with Graph-Structured Representations; For the \\(k\\)-GNN paper published on AAAI 2019, see Weisfeiler and Leman Go Neural: Higher-order Graph Neural Networks.\u00a0\u21a9</p> </li> <li> <p>Benchmarking Graph Neural Networks, ICML 2020 Workshop - Graph Representation Learning and Beyond\u00a0\u21a9</p> </li> <li> <p>Semi-Supervised Classification with Graph Convolutional Networks, ICLR 2017; Thomas Kipf's blog post explaining the basic ideas of GCN.\u00a0\u21a9</p> </li> <li> <p>Inductive Representation Learning on Large Graphs, NeurIPS 2017\u00a0\u21a9</p> </li> <li> <p>Graph Attention Networks, ICLR 2018; See this blog post by Petar Veli\u010dkovi\u0107 for more detailed explanations.\u00a0\u21a9</p> </li> <li> <p>Geometric deep learning on graphs and manifolds using mixture model CNNs, CVPR 2017\u00a0\u21a9</p> </li> <li> <p>Residual Gated Graph ConvNets, ICLR 2018\u00a0\u21a9</p> </li> <li> <p>How Powerful are Graph Neural Networks, ICLR 2019\u00a0\u21a9</p> </li> <li> <p>Provably powerful graph networks, NeurIPS 2019\u00a0\u21a9</p> </li> <li> <p>Theoretical Foundations of Graph Neural Networks, Computer Laboratory Wednesday Seminar, 17 February 2021 video, slides \u21a9</p> </li> </ol>"},{"location":"machine_learning/representation_learning_with_mutual_information_maximization/","title":"Representation Learning with Mutual Information Maximization","text":""},{"location":"machine_learning/representation_learning_with_mutual_information_maximization/#f-gan","title":"\\(f\\)-GAN","text":"<p>2016 NIPS - \\(f\\)-GAN: Training Generative Neural Samplers using Variational Divergence Minimization <sup>1</sup></p>"},{"location":"machine_learning/representation_learning_with_mutual_information_maximization/#f-divergence","title":"\\(f\\)-divergence","text":"<p>Suppose we want to train a generative model \\(Q\\) that generates data as realistic (close to the true data distribution \\(P\\)) as possible. In other words, we wish to minimize the \\(f\\)-divergence</p> \\[ \\DeclareMathOperator{\\Dcal}{\\mathcal{D}} \\DeclareMathOperator{\\Gcal}{\\mathcal{G}} \\DeclareMathOperator{\\Vcal}{\\mathcal{V}} \\DeclareMathOperator{\\Tcal}{\\mathcal{T}} \\DeclareMathOperator{\\Lcal}{\\mathcal{L}} \\DeclareMathOperator{\\lcal}{\\mathcal{l}} \\DeclareMathOperator{\\Xcal}{\\mathcal{X}} \\DeclareMathOperator{\\Zcal}{\\mathcal{Z}} \\DeclareMathOperator{\\E}{\\mathbb{E}} \\DeclareMathOperator{\\R}{\\mathbb{R}} \\DeclareMathOperator{\\dom}{\\mathrm{dom}} \\DeclareMathOperator{\\sign}{\\mathrm{sign}} \\DeclareMathOperator{\\KL}{\\Dcal_{\\mathrm{KL}}} \\DeclareMathOperator{\\JS}{\\Dcal_{\\mathrm{JS}}} \\def\\dd{\\mathrm{d}} \\def\\ee{\\mathrm{e}} \\Dcal_f(P \\Vert Q) = \\int_{\\Xcal} q(x) f\\left(\\frac{p(x)}{q(x)} \\right) \\dd x = \\E_{q(x)} f\\left(\\frac{p(x)}{q(x)} \\right) \\] <p>where \\(f:\\R^+ \\to \\R\\) is convex, lower-semicontinuous and satisfies \\(f(1) = 0\\).</p> <p>Below is a table of the \\(f\\)-divergence family.</p> Name \\(\\Dcal_f(P \\Vert Q)\\) \\(f(u)\\) \\(T^*(u)\\) Total variation \\(\\frac{1}{2} \\int \\vert p(x) - q(x) \\vert \\dd x\\) \\(\\frac{1}{2} \\vert u - 1 \\vert\\) \\(\\frac{1}{2} \\sign (u - 1)\\) Kullback-Leibler (KL) \\(\\int p(x)\\log \\frac{p(x)}{q(x)} \\dd x\\) \\(u \\log u\\) \\(1+\\log u\\) Reverse KL \\(\\int q(x)\\log \\frac{q(x)}{p(x)} \\dd x\\) \\(- \\log u\\) \\(-\\frac{1}{u}\\) Pearson \\(\\chi^2\\) \\(\\int \\frac{(q(x) - p(x))^{2}}{p(x)} \\dd x\\) \\((1 - u)^{2}\\) \\(2(u-1)\\) Neyman \\(\\chi^2\\) \\(\\int \\frac{(p(x) - q(x))^{2}}{q(x)} \\dd x\\) \\(\\frac{(1 - u)^{2}}{u}\\) \\(1-\\frac{1}{u^2}\\) Squared Hellinger \\(\\int \\left(\\sqrt{p(x)} - \\sqrt{q(x)}\\right)^{2} \\dd x\\) \\((\\sqrt{u} - 1)^{2}\\) \\((\\sqrt{u}-1)\\sqrt{\\frac{1}{u}}\\) Jeffrey \\(\\int (p(x) - q(x))\\log \\left(\\frac{p(x)}{q(x)}\\right) \\dd x\\) \\((u - 1)\\log u\\) \\(1+\\log u - \\frac{1}{u}\\) Jensen-Shannon \\(\\frac{1}{2}\\int p(x)\\log \\frac{2 p(x)}{p(x) + q(x)} + q(x)\\log \\frac{2 q(x)}{p(x) + q(x)} \\dd x\\) \\(-\\frac{u  + 1}{2}\\log \\frac{1  + u}{2} + \\frac{u}{2} \\log u\\) \\(\\frac{1}{2}\\log\\frac{2u}{u+1}\\) \\(\\alpha\\)-divergence \\(\\frac{1}{\\alpha (\\alpha - 1)}\\int p(x)\\left[ \\left(\\frac{q(x)}{p(x)}\\right)^\\alpha - 1 \\right] - \\alpha(q(x) - p(x)) \\dd x\\) \\(\\frac{1}{\\alpha (\\alpha - 1)}(u^\\alpha - 1 - \\alpha(u-1))\\) \\(\\frac{1}{\\alpha - 1}\\left( u^{\\alpha-1} - 1 \\right)\\) <p>Fenchel conjugate</p> <p>The Fenchel conjugate of function \\(f(x)\\) is defined as \\(f^*(x^*) = \\sup_{x \\in \\dom f} \\big\\{ \\langle x, x^* \\rangle - f(x) \\big\\}\\).</p> <p>We can easily verify that \\(f^*\\) is convex and lower-semicontinuous. When \\(f\\) is also convex and lower semi-continuous, \\(f^{**} = f\\).</p>"},{"location":"machine_learning/representation_learning_with_mutual_information_maximization/#variational-representation-of-the-f-divergence","title":"Variational representation of the \\(f\\)-divergence","text":"<p>We now derive the variational lower bound on \\(f\\)-divergence:</p> \\[ \\begin{align} \\Dcal_f(P \\Vert Q) &amp;= \\int_{\\Xcal} q(x) \\sup_{t \\in \\dom f^*}\\left\\{t\\frac{p(x)}{q(x)} -f^*(t) \\right\\} \\dd x\\\\ &amp;\\ge \\sup_{T \\in \\Tcal} \\Big\\{ \\int_{\\Xcal} p(x)T(x) \\dd x - \\int_{\\Xcal} q(x)f^*(T(x)) \\dd x \\Big\\}\\\\ &amp;= \\sup_{T \\in \\Tcal} \\Big\\{ \\E_{p(x)}T(x) - \\E_{q(x)}f^*(T(x)) \\Big\\} \\end{align} \\] <p>where \\(\\Tcal\\) is a class of functions \\(T: \\Xcal \\to \\R\\). It is straightforward to see that the optimal \\(T^*(x) = f^\\prime \\left( \\frac{p(x)}{q(x)} \\right)\\) (please do not confuse with conjugate function) by substituting the definition of \\(f^*\\) and let \\(t\\) be \\(\\frac{p(x)}{q(x)}\\)</p> \\[ \\Dcal_f(P \\Vert Q) \\ge \\E_{p(x)}T(x) - \\E_{q(x)} \\left[ \\frac{p(x)}{q(x)}T(x) - f\\left(\\frac{p(x)}{q(x)}\\right) \\right] = \\Dcal_f(P \\Vert Q) \\] <p>The critical value \\(f^\\prime(1)\\) can be interpreted as a classification threshold applied to \\(T(x)\\) to distinguish between true and generated samples.</p>"},{"location":"machine_learning/representation_learning_with_mutual_information_maximization/#f-gan-objective","title":"\\(f\\)-GAN Objective","text":"<p>We parameterize the generator \\(Q\\) with parameter \\(\\theta\\) and the discriminator \\(T\\) with parameter \\(\\omega\\). The \\(f\\)-GAN objective is then defined as</p> \\[ \\min_\\theta \\max_\\omega F(\\theta, \\omega) = \\E_{p(x)} T_\\omega(x) - \\E_{q_\\theta(x)} f^*(T_\\omega(x)). \\] <p>To account for \\(\\dom f^*\\) of various \\(f\\)-divergence, we further decompose \\(T_\\omega(x)\\) into \\(T_\\omega(x) = g_f\\big(V_\\omega(x) \\big)\\), where \\(V_\\omega: \\Xcal \\to \\R\\) is a neural network and \\(g_f : \\R \\to \\dom f^*\\) is an output activation function.</p> Name \\(g_f\\) \\(\\dom f^*\\) \\(f^*(t)\\) \\(f^\\prime(1)\\) Total variation \\(\\frac{1}{2}\\tanh(v)\\) \\(\\left[-\\frac{1}{2},\\frac{1}{2}\\right]\\) \\(t\\) \\(0\\) Kullback-Leibler (KL) \\(v\\) \\(\\R\\) \\(\\ee^{t-1}\\) \\(1\\) Reverse KL \\(-\\ee^v\\) \\(\\R_-\\) \\(-1 - \\log(-t)\\) \\(-1\\) Pearson \\(\\chi^2\\) \\(v\\) \\(\\R\\) \\(\\frac{1}{4}t^2 + t\\) \\(0\\) Neyman \\(\\chi^2\\) \\(1-\\ee^{v}\\) \\((-\\infty,1)\\) \\(2-2\\sqrt{1-t}\\) \\(0\\) Squared Hellinger \\(1-\\ee^{v}\\) \\((-\\infty,1)\\) \\(\\frac{t}{1-t}\\) \\(0\\) Jeffery \\(v\\) \\(\\R\\) \\(W(\\ee^{1-t})+\\frac{1}{W(\\ee^{1-t})}+t-2\\) \\(0\\) Jensen-Shannon \\(\\frac{\\log 2}{2}-\\frac{1}{2}\\log(1+\\ee^{-v})\\) \\(\\left(-\\infty,\\frac{\\log 2}{2}\\right)\\) \\(-\\frac{1}{2}\\log(2-\\ee^{2t})\\) \\(0\\) \\(\\alpha\\)-div. (\\(\\alpha \\in (0,1)\\)) \\(\\frac{1}{1-\\alpha} - \\log(1+\\ee^{-v})\\) \\((-\\infty, \\frac{1}{1-\\alpha})\\) \\(\\frac{1}{\\alpha}(t(\\alpha-1)+1)^{\\frac{\\alpha}{\\alpha-1}} - \\frac{1}{\\alpha}\\) \\(0\\) \\(\\alpha\\)-div. (\\(1&lt;\\alpha\\)) \\(v\\) \\(\\R\\) \\(\\frac{1}{\\alpha}(t(\\alpha-1)+1)^{\\frac{\\alpha}{\\alpha-1}} - \\frac{1}{\\alpha}\\) \\(0\\) <p>where \\(W\\) is the Lambert-\\(W\\) product log function.</p>"},{"location":"machine_learning/representation_learning_with_mutual_information_maximization/#mutual-information-neural-estimator-mine","title":"Mutual Information Neural Estimator (MINE)","text":"<p>2018 ICML - MINE: Mutual Information Neural Estimation <sup>2</sup></p> <p>MINE has two variants termed MINE and MINE-\\(f\\). The former uses the Donsker-Varadhan representation of the KL divergence, which results in a tighter estimator; the latter uses the \\(f\\)-divergence representation described above.</p>"},{"location":"machine_learning/representation_learning_with_mutual_information_maximization/#the-donsker-varadhan-representation-of-kl","title":"The Donsker-Varadhan representation of KL","text":"\\[ \\KL(P \\Vert Q) \\ge \\sup_{T \\in \\Tcal} \\left\\{     \\E_{p(x)} T(x) - \\log \\E_{q(x)} \\ee^{T(x)} \\right\\} \\] <p>Proof: Consider the Gibbs distribution \\(g(x) = \\frac{1}{Z} q(x) \\ee^{T(x)}\\) where \\(Z = \\E_{q(x)} \\ee^{T(x)}\\). Then</p> \\[ \\begin{align} \\Delta :&amp;= \\KL(P \\Vert Q) - \\E_{p(x)} T(x) - \\log \\E_{q(x)} \\ee^{T(x)} \\\\ &amp;= \\KL(p(x) \\Vert q(x)) - \\E_{p(x)} T(x) - \\log Z \\\\ &amp;= \\KL(p(x) \\Vert q(x)) - \\E_{p(x)} \\big( T(x) - \\log Z \\big) \\\\ &amp;= \\E_{p(x)}\\frac{p(x)}{q(x)} - \\E_{p(x)} \\frac{g(x)}{q(x)} \\\\ &amp;= \\KL(p(x) \\Vert g(x)) \\ge 0 \\end{align} \\] <p>where \\(\\Tcal\\) is a class of functions \\(T: \\Xcal \\to \\R\\) such that the two expectations are finite. The equality holds when \\(g(x) \\equiv p(x)\\), i.e. \\(T^*(x) = \\log \\frac{p(x)}{q(x)} + C\\).</p>"},{"location":"machine_learning/representation_learning_with_mutual_information_maximization/#the-f-divergence-representation-of-kl","title":"The \\(f\\)-divergence representation of KL","text":"<p>Adopting the variational lower bound for \\(f\\)-divergence, we have</p> \\[ \\KL(P \\Vert Q) \\ge \\sup_{T \\in \\Tcal} \\left\\{ \\E_{p(x)} T(x) - \\E_{q(x)} \\ee^{T(x) - 1} \\right\\} \\] <p>and the optimal \\(T^*(x) = 1+\\log \\frac{p(x)}{q(x)}\\).</p>"},{"location":"machine_learning/representation_learning_with_mutual_information_maximization/#estimating-mutual-information","title":"Estimating Mutual Information","text":"\\[ I(X;Z) = \\KL(p(x, z) \\Vert p(x)p(z)) \\ge \\sup_{\\theta \\in \\Theta} \\left\\{     \\E_{p(x,z)}T_\\theta (x,z) - \\log \\E_{p(x)p(z)}\\ee^{T_\\theta(x,z)} \\right\\} \\] <p>We estimate the expectations with empirical samples</p> \\[ \\hat{I}(X;Z)_n = \\sup_{\\theta \\in \\Theta} \\Vcal(\\theta) = \\sup_{\\theta \\in \\Theta} \\left\\{     \\E_{p^{(n)}(x,z)}T_\\theta (x,z) - \\log \\E_{p^{(n)}(x)\\hat{p}^{(n)}(z)}\\ee^{T_\\theta(x,z)} \\right\\} \\] <p>When using stochastic gradient descent (SGD), the gradient update of MINE</p> \\[ \\nabla_\\theta \\Vcal(\\theta) = \\E_B \\nabla_\\theta T(\\theta) - \\frac{\\E_B \\ee^{T_\\theta}\\nabla_\\theta T_\\theta}{\\E_B \\ee^{T_\\theta}} \\] <p>is a biased estimate of the full gradient update (Why?). This is corrected by an exponential moving average applied to the denominator. For MINE-\\(f\\), the SGD gradient is unbiased.</p>"},{"location":"machine_learning/representation_learning_with_mutual_information_maximization/#contrastive-predictive-coding-cpc-and-the-infonce-loss","title":"Contrastive Predictive Coding (CPC) and the InfoNCE Loss","text":"<p>2010 AISTATS - Noise-contrastive estimation: A new estimation principle for unnormalized statistical models <sup>3</sup></p> <p>2018 NeurIPS - Representation Learning with Contrastive Predictive Coding <sup>4</sup></p>"},{"location":"machine_learning/representation_learning_with_mutual_information_maximization/#noise-contrastive-estimation-nce","title":"Noise-Contrastive Estimation (NCE)","text":"<p>Suppose we have observed data \\(\\mathbf{x} \\sim p_d(\\cdot)\\) and we want to estimate a model from a family \\(\\{p_\\mathrm{m}(\\cdot;\\alpha)\\}_\\alpha\\) where \\(\\alpha\\) is the model parameter. The challenge is that often it is more convenient to define an unnormalized model \\(p_\\mathrm{m}^0\\) such that</p> \\[p_\\mathrm{m}(\\cdot;\\alpha) = \\frac{p_\\mathrm{m}^0(\\cdot;\\alpha)}{Z(\\alpha)} \\quad \\text{where} \\ Z(\\alpha) = \\int p_\\mathrm{m}^0(\\mathbf{u};\\alpha)\\dd \\mathbf{u}.\\] <p>The integral \\(Z(\\alpha)\\) is rarely analytically tractable, and if the data is highdimensional, numerical integration is difficult. We include the normalization constant \\(Z(\\alpha)\\) as an additional parameter \\(c \\approx -\\log Z(\\alpha)\\), so that</p> \\[\\log p_\\mathrm{m}(\\cdot; \\theta) = \\log p_\\mathrm{m}^0(\\cdot; \\alpha) + c \\quad \\text{where} \\ \\theta = {\\alpha, c}.\\] <p>Performing Maxmimum Likelihood Estimation (MLE) on this objective is not feasible as \\(c\\) would be pushed to infinity. Instead we learn to discriminate between the data \\(\\mathbf{x}\\) and some artificially generated noise \\(\\mathbf{y} \\sim p_{\\mathrm{n}}\\). With \\(T\\) positive (data) and \\(T\\) negative (noise) examples, we aim to correctly classify each of them, and thus define the NCE objective as</p> \\[ \\begin{align} J_T(\\theta) &amp;= \\frac{1}{2T} \\sum_{t=1}^T \\Big[ \\log h(\\mathbf{x}_t; \\theta) + \\log \\big( 1 - h(\\mathbf{y}_t; \\theta) \\big) \\Big] \\\\ &amp;= \\frac{1}{2T} \\sum_{t=1}^T \\left[     \\log \\frac{p_{\\mathrm{m}}(\\mathbf{x}_t; \\theta)}{p_{\\mathrm{m}}(\\mathbf{x}_t; \\theta) + p_{\\mathrm{n}}(\\mathbf{x}_t)} +     \\log \\frac{p_{\\mathrm{n}}(\\mathbf{y}_t)}{p_{\\mathrm{m}}(\\mathbf{y}_t; \\theta) + p_{\\mathrm{n}}(\\mathbf{y}_t)} \\right] \\end{align} \\] <p>where \\(h(\\mathbf{u}; \\theta) = \\sigma\\big(\\log p_{\\mathrm{m}}(\\mathbf{u}; \\theta) - \\log p_{\\mathrm{n}}(\\mathbf{u}) \\big)\\).</p> <p>This blog post (in Chinese) shows by gradient calculation that when the number of negative samples approches infinity, the NCE gradient equals to the MLE gradient.</p>"},{"location":"machine_learning/representation_learning_with_mutual_information_maximization/#contrastive-predictive-coding","title":"Contrastive Predictive Coding","text":"<p>Let \\(\\{x_t\\}\\) be a sequence of observations, \\(z_t = g_{\\mathrm{enc}}(x_t)\\) be the encoded latent representation at time step \\(t\\), and \\(c_t = g_{\\mathrm{ar}}(z_{\\le t})\\) be the summarized context (global, ar for auto-regressive) latent representation at time step \\(t\\). Given a set \\(X = \\{x_1, \\cdots, x_N\\}\\) of \\(N\\) random samples containing one positive sample from \\(p(x_{t+k}|c_t)\\) and \\(N - 1\\) negative samples from the 'proposal' distribution \\(p(x_{t+k})\\), we wish to preserve the mutual information between the \\(k\\)-step-later input \\(x_{t+k}\\) and the current context \\(c_t\\), by trying to identify the positive sample among all the samples:</p> \\[ p(d=i|X, c_t) = \\frac{p(x_i|c_t)\\prod_{l \\ne i}p(x_l)}{\\sum_{j=1}^N p(x_j|c_t)\\prod_{l \\ne j}p(x_l)} = \\frac{\\frac{p(x_i|c_t)}{p(x_i)}}{\\sum_{j=1}^N \\frac{p(x_j|c_t)}{p(x_j)}} = \\frac{f_i(x_i, c_t)}{\\sum_{j=1}^N f_j(x_j, c_t)} \\] <p>where \\(f_{k}(x_{t+k}, c_t) = C \\frac{p(x_{t+k}|c_t)}{p(x_{t+k})}\\) and \\(C\\) is an arbitrary constant. Note that \\(f\\) is unnormalized and can be parameterized by a simple log-bilinear model</p> \\[ f_{k}(x_{t+k}, c_t) = \\exp \\big( z_{t+k}^T W_k c_t \\big). \\] <p>To maximize our contrastive predictive capabilities, we minimize the following InfoNCE loss:</p> \\[ \\Lcal^{\\mathrm{(InfoNCE)}} = -\\E_X \\left[ \\log \\frac{f_{k}(x_{t+k}, c_t)}{\\sum_{x_j \\in X} f_{k}(x_{j}, c_t)} \\right] \\]"},{"location":"machine_learning/representation_learning_with_mutual_information_maximization/#relation-with-mutual-information","title":"Relation with Mutual Information","text":"\\[ I(x_{t+k}; c_t) \\ge \\log N - \\Lcal^{\\mathrm{(InfoNCE)}} \\] <p>Proof:</p> \\[ \\begin{align} \\Lcal^{\\mathrm{(InfoNCE)}} &amp;= \\E_X \\log \\Big(     1 + \\frac{p(x_{t+k})}{p(x_{t+k}|c_t)} \\sum_{j \\ne t + k} \\frac{p(x_j|c_t)}{p(x_j)} \\Big) \\\\ &amp;\\approx \\E_X \\log \\Big(     1 + \\frac{p(x_{t+k})}{p(x_{t+k}|c_t)} (N-1) \\E_{x_j \\in X_\\mathrm{neg}} \\frac{p(x_j|c_t)}{p(x_j)} \\Big) \\\\ &amp;\\approx \\E_X \\log \\Big(     1 + \\frac{p(x_{t+k})}{p(x_{t+k}|c_t)} (N-1) \\Big) \\\\ &amp;\\ge \\E_X \\log \\Big(     N \\frac{p(x_{t+k})}{p(x_{t+k}|c_t)} \\Big) \\\\ &amp;= -I(x_{t+k}; c_t) + \\log N \\end{align} \\] <p>Note that the approximation is more accurate as the number of negative samples increases.</p>"},{"location":"machine_learning/representation_learning_with_mutual_information_maximization/#relation-with-mine","title":"Relation with MINE","text":"<p>Let \\(F(x, c) = \\log f(x, c)\\), then</p> \\[ \\begin{align} \\Lcal^{\\mathrm{(InfoNCE)}} &amp;= -\\E_X \\left[ \\frac{f_{k}(x_{t+k}, c_t)}{\\sum_{x_j \\in X} f_{k}(x_{j}, c_t)} \\right] \\\\ &amp;= \\E_X F(x_{t+k}, c_t) - \\E_X \\log \\big(\\ee^{F(x_{t+k}, c_t)} \\sum_{x_j \\in X_{\\mathrm{neg}}} \\ee^{F(x_j, c_t)} \\big) \\\\ &amp;\\le \\E_X F(x_{t+k}, c_t) - \\E_{c_t} \\log \\sum_{x_j \\in X_{\\mathrm{neg}}} \\ee^{F(x_j, c_t)} \\\\ &amp;= \\E_X F(x_{t+k}, c_t) - \\E_{c_t} \\Big[ \\log \\frac{1}{N-1} \\sum_{x_j \\in X_{\\mathrm{neg}}} \\ee^{F(x_j, c_t)} + \\log(N - 1) \\Big] \\end{align} \\] <p>which is equivalent to the MINE estimator:</p> \\[ \\hat{I}(X;Z)_n = \\sup_{\\theta \\in \\Theta} \\Vcal(\\theta) = \\sup_{\\theta \\in \\Theta} \\left\\{     \\E_{p^{(n)}(x,z)}T_\\theta (x,z) - \\log \\E_{p^{(n)}(x)\\hat{p}^{(n)}(z)}\\ee^{T_\\theta(x,z)} \\right\\} \\]"},{"location":"machine_learning/representation_learning_with_mutual_information_maximization/#deep-infomax-dim","title":"Deep InfoMax (DIM)","text":"<p>2019 ICLR - Learning deep representations by mutual information estimation and maximization <sup>5</sup></p> <p>Deep InfoMax is a principled framework for training a continuous and (almost everywhere) differentiable encoder \\(E_\\psi: \\Xcal \\to \\Zcal\\) to maximize mutual information between its input and output, with neural network parameters \\(\\psi \\in \\Psi\\).</p> <p>Assume that we are given a set of training examples on an input space, \\(\\mathbf{X} := \\{x^{(i)} \\in \\Xcal\\}_{i=1}^N\\), with empirical probability distribution \\(P\\). We define \\(U_{\\psi,P}\\) as the marginal distribution of \\(z=E_\\psi(x)\\) where \\(x\\) is sampled from \\(P\\), i.e., \\(u(z=E_\\psi(x)) = \\big( \\nabla_x E_\\psi(x) \\big)^{-1} p(x)\\).</p> <p>We assert our encoder should be trained according to the following criteria:</p> <ul> <li>Local and global mutual information maximization</li> <li>Statistical constraints (prior in the latent space \\(v(z)\\)).</li> </ul> \\[ \\min_{\\psi} - I(X; Z) + \\lambda \\KL(v(z)\\Vert u(z)) \\] <p>As a preliminary, we introduce the local feature encoder \\(C_\\psi\\), the global feature encoder \\(E_\\psi = f_\\psi \\circ C_\\psi\\) and the discriminator \\(T_{\\psi, \\omega} = D_\\omega \\circ g \\circ (C_\\psi, E_\\psi)\\), where \\(D_\\omega\\) is a neural classifier, and \\(g\\) is a function that combines the local and global features.</p> <p>The overall DIM objective consists of three parts, global MI, local MI and statistical constraints.</p> \\[ \\begin{align} \\max_{\\omega_1, \\omega_2, \\psi} &amp;\\Big(     \\alpha \\hat{I}_{\\omega_1, \\psi} \\big( X; E_\\psi(X) \\big) + \\frac{\\beta}{M^2}\\sum_{i=1}^{M^2} \\hat{I}_{\\omega_2, \\psi} \\big (X^{(i)}; E_\\psi(X) \\big) \\Big) + \\\\ &amp;\\min_\\psi\\max_\\phi \\gamma \\hat{D}_\\phi(V \\Vert U_{\\psi, P}) \\end{align} \\] <p>In the following sections, we first introduce how to enfore statistical constraints \\(\\hat{D}_\\phi\\) and local MI maximization, then discuss objectives for general MI maximization \\(\\hat{I}\\).</p>"},{"location":"machine_learning/representation_learning_with_mutual_information_maximization/#statistical-constraints","title":"Statistical Constraints","text":"<p>Why use adversarial objectives for KL regularization?</p> <p>Here we could also use VAE-style prior regularization \\(\\min \\KL \\big(q(z|x) \\Vert p(z) \\big)\\), but this assumes for every data point \\(x\\), its latent \\(q(z|x)\\) is close to \\(p(z)\\). This will encourage \\(q(z)\\) to pick the modes of \\(p(z)\\), rather than the whole distribution of \\(p(z)\\). See the Adversarial AutoEncoders paper for more details.</p> <p>DIM imposes statistical constraints onto learned representations by implicitly training the encoder so that the push-forward distribution, \\(U_{\\psi, P}\\), matches a prior \\(V\\). Following variational representation of the Jensen-Shannon divergence, we optimize this objective by</p> \\[ \\min_\\psi \\max_\\phi \\JS(V \\Vert U_{\\psi, P}) = \\E_{v(z)} \\log D_\\phi (z) + \\E_{p(x)} \\log (1 - D_\\phi (E_\\psi (x))) \\] <p>Note that the discriminator \\(D_\\phi\\) operates in the latent space rather than the input space.</p>"},{"location":"machine_learning/representation_learning_with_mutual_information_maximization/#local-mi-maximization","title":"Local MI Maximization","text":"<p>Maximizing the mutual information b/t encoder input and output may not be meaningful enough. We propose to maximize the average MI between the high-level representation and local patches of the image. Because the same global representation is encouraged to have high MI with all the patches, this favours encoding aspects of the data that are shared across patches.</p> <p></p> <p>First we encode the input to a feature map, \\(C_\\psi(x) = \\{ C_\\psi^{(i)} \\}_{i=1}^{M\\times M}\\) that reflects useful structure in the data (e.g., spatial locality). Next, we summarize this local feature map into a global feature, \\(E_\\psi(x) = f_\\psi \\circ C_\\psi(x)\\). We then define our MI estimator on global/local pairs, maximizing the average estimated MI:</p> \\[ \\max_{\\omega, \\psi} \\frac{1}{M^2} \\sum_{i=1}^{M^2} \\hat{I}_{\\omega, \\psi}\\big( C_\\psi^{(i)}(X); E_\\psi(X) \\big). \\]"},{"location":"machine_learning/representation_learning_with_mutual_information_maximization/#mi-maximization-objectives","title":"MI Maximization Objectives","text":""},{"location":"machine_learning/representation_learning_with_mutual_information_maximization/#the-donsker-varadhan-objective","title":"The Donsker-Varadhan Objective","text":"<p>This lower-bound to the MI is based on the Donsker-Varadhan representation of the KL-divergence. It is the tightest possible bound on KL divergence, but it is less stable and requires many negative samples.</p> \\[ I(X; Z) \\ge \\hat{I}^{\\text{(DV)}}_{\\psi, \\omega} (X; Z) = \\E_{p(x, z)} T_{\\psi, \\omega} (x, z) - \\log \\E_{p(x)p(z)} (\\ee^{T_{\\psi, \\omega} (x, z)}) \\]"},{"location":"machine_learning/representation_learning_with_mutual_information_maximization/#the-jensen-shannon-objective","title":"The Jensen-Shannon Objective","text":"<p>Since we do not concern the precise value of mutual information, and rather primarily interested in its maximization, we could instead optimize on the Jensen-Shannon divergence. This objective is stable to optimize and requires few negative sample, but it is a looser bound to the true mutual information.</p> <p>Following \\(f\\)-GAN formulation, with output activation \\(g_f = -\\log(1+\\ee^{-v})\\) and conjugate function \\(f^*(t) = -\\log(1-\\ee^t)\\), we define the following objective:</p> \\[ \\begin{align} &amp;\\hat{I}^{\\text{(JS)}}_{\\psi, \\omega} (X; E_\\psi(X)) \\\\ =&amp; \\widehat{\\JS}(p(x, E_\\psi(x)) \\Vert p(x)p(E_\\psi(x))) \\\\ =&amp; \\E_{p(x)} \\Big[ \\widetilde{T}_{\\psi, \\omega} (x, E_\\psi(x)) - \\E_{p(x^\\prime)} f^*(\\widetilde{T}_{\\psi, \\omega} (x^\\prime, E_\\psi(x))) \\Big] \\\\ =&amp; \\E_{p(x)} \\Big[ -\\log \\big(1 + \\ee^{-T_{\\psi, \\omega}(x, E_\\psi(x))} \\big) - \\E_{p(x^\\prime)} \\log \\big(1 + \\ee^{T_{\\psi, \\omega}(x^\\prime, E_\\psi(x))} \\big) \\Big] \\\\ =&amp; \\E_{p(x)} \\Big[ \\log \\sigma \\big( T_{\\psi, \\omega}(x, E_\\psi(x)) \\big) + \\E_{p(x^\\prime)} \\log \\Big( 1 - \\sigma \\big( T_{\\psi, \\omega}(x^\\prime, E_\\psi(x)) \\big) \\Big) \\Big] \\end{align} \\] <p>where \\(\\widetilde{T} = g_f \\circ T\\) is the discriminator output after activation \\(g_f\\). In section A.1 of the DMI paper <sup>5</sup>, the authors show theoretically and empirically that \\(\\max_{\\psi, \\omega} \\JS(p(x, z) \\Vert p(x)p(z))\\) is indeed a good maximizer of \\(I(X; Z)\\).</p>"},{"location":"machine_learning/representation_learning_with_mutual_information_maximization/#the-infonce-objective","title":"The InfoNCE Objective","text":"<p>This objective uses noise-contrastive estimation to bound mutual information. It obtains strong results, but requires many negative samples.</p> \\[ \\begin{align} \\hat{I}^{\\text{(InfoNCE)}}_{\\psi, \\omega} (X; E_\\psi(X)) &amp;= \\E_{p(x, z)} \\bigg[     T_{\\psi, \\omega}(x, E_\\psi(x)) - \\E_{p(x^\\prime)} \\Big[         \\log \\sum_{x^\\prime} \\ee^{T_{\\psi, \\omega}(x^\\prime, E_\\psi(x)) }     \\Big] \\bigg] \\end{align} \\]"},{"location":"machine_learning/representation_learning_with_mutual_information_maximization/#deep-graph-infomax","title":"Deep Graph Infomax","text":"<p>2019 ICLR - Deep Graph Infomax <sup>6</sup></p> <p>Deep Graph Infomax (DGI) is a general approach for learning node representations within graph-structured data in an unsupervised manner. It relies on maximizing mutual information between patch representations and corresponding high-level summaries of graphs.</p> <p>We first introduce</p> <ul> <li>The encoder \\(E: \\R^{N \\times F^{\\mathrm{in}}} \\times \\R^{N \\times N} \\to \\R^{N \\times F}\\) such that \\(E(\\mathbf{X}, \\mathbf{A}) = \\mathbf{H} = (\\mathbf{h}_1, \\dots, \\mathbf{h}_N)^\\top\\) produces node embeddings (or patch representations) that summarize a patch of the graph centered around node \\(i\\).</li> <li>The readout function \\(R: \\R^{N \\times F} \\to \\R^F\\) which summarizes the obtained patch representations into a graph-level representation \\(\\mathbf{s} = R(E(\\mathbf{X}, \\mathbf{A}))\\). It is implemented as a sigmoid after a mean \\(R(\\mathbf{H}) = \\sigma\\left( \\frac{1}{N} \\sum_{i=1}^N \\mathbf{h}_i \\right)\\).</li> <li>The discriminator \\(D: \\R^F \\times \\R^F \\to \\R\\) such that \\(D(\\mathbf{h}_i, \\mathbf{s})\\) represents the logit scores assigned to this patch-summary pair (should be higher for patches contained within the summary). It is implemented as a bilinear function \\(D(\\mathbf{h}_i, \\mathbf{s}) = \\mathbf{h}_i \\mathbf{W} \\mathbf{s}\\).</li> <li>Negative samples are generated by pairing the summary vector \\(\\mathbf{s}\\) of a graph with patch representations \\(\\widetilde{\\mathbf{h}}_j\\) from another graph \\((\\widetilde{\\mathbf{X}}, \\widetilde{\\mathbf{A}})\\). This alternative graph is obtained as other elements of a training set in a multi-graph setting, or by an explicit corruption function \\((\\widetilde{\\mathbf{X}}, \\widetilde{\\mathbf{A}}) = C(\\mathbf{X}, \\mathbf{A})\\) which permutes row-wise the node feature matrix \\(\\mathbf{X}\\).</li> </ul> <p>Next we introduce the DGI objective for one training graph \\(\\Gcal = (\\mathbf{X}, \\mathbf{A})\\), based on the Jensen-Shannon objective for Deep InfoMax</p> \\[ \\begin{align} \\max \\Lcal = \\frac{1}{N + M} \\bigg( &amp;     \\sum_{i=1}^N \\E_{x_i \\sim V(\\Gcal)} \\log \\sigma \\big( D(\\mathbf{h}_i, \\mathbf{s}) \\big) \\\\ + &amp;      \\sum_{j=1}^M \\E_{\\widetilde{x}_j \\sim V(\\widetilde{\\Gcal})} \\log \\Big( 1 - \\sigma \\big(D(\\widetilde{\\mathbf{h}}_j, \\mathbf{s}) \\big) \\Big) \\bigg) \\end{align} \\]"},{"location":"machine_learning/representation_learning_with_mutual_information_maximization/#infograph","title":"InfoGraph","text":"<p>2020 ICLR - InfoGraph: Unsupervised and Semi-supervised Graph-Level Representation Learning via Mutual Information Maximization <sup>7</sup></p> <p>InfoGraph studies learning the representations of whole graphs (rather than nodes as in DGI) in both unsupervised and semi-supervised scenarios. Its unsupervised version is similar to DGI except for</p> <ul> <li>Batch-wise generation of negative samples rather than random-sampling- or corruption-based negative samples.</li> <li>GIN methodologies for better graph-level representation learning.</li> </ul> <p>In semi-supervised setting, directly adding a supervised loss would likely result in negative transfer. The authors alleviate this problem by separating the parameters of the supervised encoder \\(\\varphi\\) and those of the unsupervised encoder \\(\\phi\\), and adding a student-teacher loss which encourage mutual information maximization between the two encoders at all levels. The overall loss is:</p> \\[ \\begin{align} \\Lcal = &amp; \\sum_{G \\in \\Gcal_{\\mathrm{l}}} \\lcal (\\widetilde{y}_\\phi(G), y_G) + \\\\ &amp; \\sum_{G \\in \\Gcal_{\\mathrm{l}} \\cup \\Gcal_{\\mathrm{u}}} \\frac{1}{|V|} \\sum_{u \\in V} \\widehat{I}(h^u_\\varphi(G), H_\\varphi(G)) - \\\\ &amp; \\lambda \\sum_{G \\in \\Gcal_{\\mathrm{l}} \\cup \\Gcal_{\\mathrm{u}}} \\frac{1}{|V|} \\sum_{k=1}^K \\widehat{I}(H^{(k)}_\\phi(G), H^{(k)}_\\varphi(G)) \\end{align} \\] <p>In practice, to reduce the computational overhead, at each training step, we enforce mutual-information maximization on a randomly chosen layer of the encoder.</p> <ol> <li> <p>NeurIPS 2016 - \\(f\\)-GAN: Training Generative Neural Samplers using Variational Divergence Minimization; A blog post explaining the paper in Chinese.\u00a0\u21a9</p> </li> <li> <p>ICML 2018 - MINE: Mutual Information Neural Estimation \u21a9</p> </li> <li> <p>AISTATS 2010 - Noise-contrastive estimation: A new estimation principle for unnormalized statistical models \u21a9</p> </li> <li> <p>NeurIPS 2018 - Representation Learning with Contrastive Predictive Coding \u21a9</p> </li> <li> <p>ICLR 2019 - Learning deep representations by mutual information estimation and maximization (slides, video); A blog post explaining the paper in Chinese.\u00a0\u21a9\u21a9</p> </li> <li> <p>ICLR 2019 - Deep Graph Infomax. For relations with previous unsupervised graph representation learning methods, see the IPAM tutorial Unsupervised Learning with Graph Neural Networks by Thomas Kipf and also Daza's Master Thesis A Modular Framework for Unsupervised Graph Representation Learning.\u00a0\u21a9</p> </li> <li> <p>ICLR 2020 - InfoGraph: Unsupervised and Semi-supervised Graph-Level Representation Learning via Mutual Information Maximization \u21a9</p> </li> </ol>"},{"location":"python-tutorial/","title":"Why Python?","text":"<pre><code>print(123)\nclass dummy(object):\n    def __init__(self, shit):\n        self.shit = shit\n</code></pre>"},{"location":"python-tutorial/#your-python-playground","title":"Your Python Playground","text":"<pre>\n        <code>print('123')</code>\n    </pre> <p>Run</p> Output:"},{"location":"single_cell/review/","title":"Review","text":""},{"location":"single_cell/review/#recurring-themes-of-single-cell-data-science","title":"Recurring themes of single-cell data science","text":""},{"location":"single_cell/review/#varying-levels-of-resolution","title":"Varying levels of resolution","text":"<p>When drawing maps of cell types and states, it is important that sc-seq methods</p> <ul> <li>have a structure that recapitulates both tissue development and tissue organization</li> <li>account for continuous cell states in addition to discrete cell types (i.e., reflecting cell state trajectories within cell types and smooth transitions between cell types, as observed in tissue generation)</li> <li>allow for choosing the level of resolution flexibly; and (iv)</li> <li>include biological and functional annotation wherever available and helpful in the intended functional context.</li> </ul>"},{"location":"single_cell/review/#quantifying-uncertainty-of-measurements-and-analysis-results","title":"Quantifying uncertainty of measurements and analysis results","text":"<p>The amount of material sampled from single cells is considerably less than that used in bulk experiments. The increase in resolution due to sc-seq also means a reduction of the stability of the supporting signals. This in turn implies that data becomes substantially more uncertain and tasks so far considered routine, such as single nucleotide variation (SNV) calling in bulk sequencing, require considerable methodological care with sc-seq data.</p>"},{"location":"single_cell/review/#scaling-to-higher-dimensionalities","title":"Scaling to higher dimensionalities","text":""}]}